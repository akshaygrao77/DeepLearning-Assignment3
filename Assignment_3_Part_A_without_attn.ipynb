{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import all necessary libraries"
      ],
      "metadata": {
        "id": "eCUhCbkRn2VT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v8grmhdOGcQW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.callbacks import History\n",
        "import math\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import csv "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse"
      ],
      "metadata": {
        "id": "8mx6XHIOXAwM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset using CURL"
      ],
      "metadata": {
        "id": "G1lknpXhnyHT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGA8GCP1GY0r",
        "outputId": "d70e1240-b375-4524-f896-855ba59e03bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1915M  100 1915M    0     0   168M      0  0:00:11  0:00:11 --:--:--  171M\n",
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ]
        }
      ],
      "source": [
        "!curl https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar --output daksh.tar\n",
        "!tar -xvf  'daksh.tar' "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login to wandb"
      ],
      "metadata": {
        "id": "uPcgDtyIoBCt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qck0iG7IyN60",
        "outputId": "98ee96e6-8d89-41a8-cacb-19faeb005f4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.16)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.11)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makshaygrao\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!wandb login\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "SFfsHST6z4y4",
        "outputId": "d7382011-72be-4176-ddf7-df978de2dc49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makshaygrao\u001b[0m (\u001b[33mcs21s002-ee21s113-dlassignment-1\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220505_143119-19wbaxlw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/runs/19wbaxlw\" target=\"_blank\">star-droid-212</a></strong> to <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/runs/19wbaxlw?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f718e6ed510>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# wandb.init(project=\"DeepLearningAssignment-3\", entity='cs21s002-ee21s113-dlassignment-1')\n",
        "wandb.init(project=\"DL-Assignment3\", entity='cs21s002-ee21s113-dlassignment-1')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-process dataset"
      ],
      "metadata": {
        "id": "SgmmMZQuoIGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model_prefix = \"Enc_\"\n",
        "decoder_model_prefix = \"Dec_\""
      ],
      "metadata": {
        "id": "RWMRI6z0yak3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mzR1_lcKgW5O"
      },
      "outputs": [],
      "source": [
        "def obtain_input_target_data_from_path(path,tokenizer_obj):\n",
        "  input_texts = []\n",
        "  target_texts = []\n",
        "  \n",
        "  df = pd.read_csv(path,sep=\"\\t\",names=[\"1\", \"2\",\"3\"]).astype(str)\n",
        "  if tokenizer_obj is None:\n",
        "    # Shuffle rows in random order with a fixed seed(for reproducability)\n",
        "    df=df.sample(frac=1,random_state=1)\n",
        "  # Add all the  input and target texts with start sequence and end sequence added to target \n",
        "  for index, row in df.iterrows():\n",
        "      input_text=row['2']\n",
        "      target_text= row['1']\n",
        "      # Skip empty lines/words\n",
        "      if target_text =='</s>' or input_text=='</s>':\n",
        "        continue\n",
        "      \n",
        "      target_text = \"\\t\" + target_text + \"\\n\"\n",
        "      input_texts.append(input_text)\n",
        "      target_texts.append(target_text)\n",
        "  \n",
        "  return input_texts, target_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1pov4thdppKt"
      },
      "outputs": [],
      "source": [
        "def convert_text_to_sequences(tokenizer_obj,inp_texts):\n",
        "  if tokenizer_obj is None:\n",
        "    tokenizer_obj = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "    tokenizer_obj.fit_on_texts(inp_texts)\n",
        "  ret_tensor = tokenizer_obj.texts_to_sequences(inp_texts)\n",
        "  ret_tensor = tf.keras.preprocessing.sequence.pad_sequences(ret_tensor,padding='post')\n",
        "\n",
        "  return ret_tensor,tokenizer_obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "s-fHBjgNHJNY"
      },
      "outputs": [],
      "source": [
        "# This method converts a dataset(from path) to input and target sequences\n",
        "def pre_process_data(path,input_tokenizer=None,target_tokenizer=None,input_length=None,target_length=None):\n",
        "  \n",
        "  input_texts, target_texts = obtain_input_target_data_from_path(path,input_tokenizer)\n",
        "  \n",
        "  input_tensor,input_tokenizer = convert_text_to_sequences(input_tokenizer,input_texts)\n",
        "  \n",
        "  target_tensor,target_tokenizer = convert_text_to_sequences(target_tokenizer,target_texts)\n",
        "  \n",
        "  # Above functions return padded version wrt longest sequence in the given list of sequence\n",
        "  # The below function, pads more zeros wrt input_length and target_length\n",
        "  if input_length is not None and target_length is not None:\n",
        "      input_tensor=tf.concat([input_tensor,tf.zeros((input_tensor.shape[0],input_length-input_tensor.shape[1]))],axis=1)\n",
        "      target_tensor=tf.concat([target_tensor,tf.zeros((target_tensor.shape[0],target_length-target_tensor.shape[1]))],axis=1)\n",
        "  return input_texts,input_tensor,input_tokenizer,target_texts,target_tensor,target_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KL3-IxpstPlS"
      },
      "outputs": [],
      "source": [
        "transliteration_target_language = 'kn'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FWLwIxUwISEb"
      },
      "outputs": [],
      "source": [
        "train_input_texts,train_input_tensor,input_tokenizer,train_target_texts,train_target_tensor,target_tokenizer = pre_process_data(\"/content/dakshina_dataset_v1.0/\"+transliteration_target_language+\"/lexicons/\"+transliteration_target_language+\".translit.sampled.train.tsv\")\n",
        "# Only training dataset is used to fit the tokenizer on text. Other datasets just use this vocab for pre-processing\n",
        "# The length for padding is also set from training datasets\n",
        "val_input_texts,val_input_tensor,val_input_tokenizer,val_target_texts,val_target_tensor,val_target_tokenizer = pre_process_data(\"/content/dakshina_dataset_v1.0/\"+transliteration_target_language+\"/lexicons/\"+transliteration_target_language+\".translit.sampled.dev.tsv\",input_tokenizer,target_tokenizer,train_input_tensor.shape[1],train_target_tensor.shape[1])\n",
        "test_input_texts,test_input_tensor,test_input_tokenizer,test_target_texts,test_target_tensor,test_target_tokenizer = pre_process_data(\"/content/dakshina_dataset_v1.0/\"+transliteration_target_language+\"/lexicons/\"+transliteration_target_language+\".translit.sampled.test.tsv\",input_tokenizer,target_tokenizer,train_input_tensor.shape[1],train_target_tensor.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pi_oWsEotJq8"
      },
      "outputs": [],
      "source": [
        "num_encoder_tokens = len(input_tokenizer.word_index)+1\n",
        "num_decoder_tokens = len(target_tokenizer.word_index)+1\n",
        "max_encoder_seq_length =  train_input_tensor.shape[1]\n",
        "max_decoder_seq_length = train_target_tensor.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASxKQYeZ7knF",
        "outputId": "036e7558-4cb6-42e3-9505-e5c10d8a0947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27\n",
            "64\n",
            "26\n",
            "26\n"
          ]
        }
      ],
      "source": [
        "print(num_encoder_tokens)\n",
        "print(num_decoder_tokens)\n",
        "print(max_encoder_seq_length)\n",
        "print(max_decoder_seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions"
      ],
      "metadata": {
        "id": "zezy7OPwoRVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_encoder_decoder_layers_from_model(model):\n",
        "  decoder_layers = 0\n",
        "  encoder_layers = 0\n",
        "  for each_layer in model.layers:\n",
        "    layer_name = each_layer.name\n",
        "    if(decoder_model_prefix+\"cell\" in layer_name):\n",
        "      decoder_layers += 1\n",
        "    elif(encoder_model_prefix+\"cell\" in layer_name):\n",
        "      encoder_layers += 1\n",
        "  return encoder_layers,decoder_layers"
      ],
      "metadata": {
        "id": "zLyeKv586RC4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_latent_dim_from_model(model):\n",
        "  return model.get_layer(str(encoder_model_prefix)+\"cell_0\").output[0].shape[2]"
      ],
      "metadata": {
        "id": "sePAFz1Lr_ky"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rnntype_from_model(model):\n",
        "  if isinstance(model.get_layer(encoder_model_prefix+\"cell_0\"), keras.layers.LSTM):\n",
        "    return 'LSTM'\n",
        "  elif isinstance(model.get_layer(encoder_model_prefix+\"cell_0\"), keras.layers.GRU):\n",
        "    return \"GRU\"\n",
        "  elif isinstance(model.get_layer(encoder_model_prefix+\"cell_0\"), keras.layers.RNN):\n",
        "    return \"RNN\""
      ],
      "metadata": {
        "id": "cXzE_MFawowh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(code,lr):\n",
        "  if(code==\"SGD\"):\n",
        "    return keras.optimizers.SGD(lr)\n",
        "  elif(code == \"RMSprop\"):\n",
        "    return keras.optimizers.RMSprop(lr)\n",
        "  elif(code == \"Adam\"):\n",
        "    return keras.optimizers.Adam(lr)\n",
        "  elif(code == \"Nadam\"):\n",
        "    return keras.optimizers.Nadam(lr)\n",
        "  else:\n",
        "    return keras.optimizers.Adam(lr)"
      ],
      "metadata": {
        "id": "Mq6YOZgj47Av"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Y0gUSgh26w03"
      },
      "outputs": [],
      "source": [
        "index_to_char_target = dict((target_tokenizer.word_index[key], key) for key in target_tokenizer.word_index.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for constructing seq-seq model"
      ],
      "metadata": {
        "id": "gSXGniWdosl9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mF1p5Vjl6gGZ"
      },
      "outputs": [],
      "source": [
        "def build_layered_RNN_model(rnn_type,embedding_in_dim,embedding_out_dim,layers,dropout,inp_length,model_out_dim,prefix,initial_state = None):\n",
        "   #input layer ; takes in tokenize input\n",
        "  model_inputs = keras.Input(shape=( inp_length),name=prefix+\"inp\")\n",
        "  #embedding layer\n",
        "  embed = keras.layers.Embedding(embedding_in_dim, embedding_out_dim,name=prefix+\"embed\")(model_inputs)\n",
        "  \n",
        "  last_layer_model = None\n",
        "  if rnn_type == 'LSTM':\n",
        "    #adding everything except the last LSTM layer, because in last layer return state=True\n",
        "    for i in range(layers):\n",
        "      layered_model = keras.layers.LSTM(model_out_dim, return_sequences=True,return_state=True,dropout=dropout,name=prefix+\"cell_\"+str(i))\n",
        "      if i==0:\n",
        "        inp_layer = embed\n",
        "      else:\n",
        "        inp_layer = last_layer_model\n",
        "      \n",
        "      model_layer_out,state_h, state_c = layered_model(inp_layer,initial_state)\n",
        "      \n",
        "      last_layer_model = model_layer_out\n",
        "    \n",
        "    model_states = [state_h, state_c]\n",
        "    \n",
        "  elif rnn_type=='GRU':\n",
        "    #adding everything except the last GRU layer, because in last layer return state=True    \n",
        "    for i in range(layers):\n",
        "      layered_model = keras.layers.GRU(model_out_dim, return_sequences=True,return_state=True,dropout=dropout,name=prefix+\"cell_\"+str(i))\n",
        "      if i==0:\n",
        "        inp_layer = embed\n",
        "      else:\n",
        "        inp_layer = last_layer_model\n",
        "      \n",
        "      model_layer_out,state = layered_model(inp_layer,initial_state)\n",
        "      \n",
        "      last_layer_model = model_layer_out\n",
        "\n",
        "    model_states = [state]\n",
        "  elif rnn_type=='RNN':\n",
        "    #adding everything except the last RNN layer, because in last layer return state=True\n",
        "    for i in range(layers):      \n",
        "      layered_model = keras.layers.SimpleRNN(model_out_dim, return_sequences=True,return_state=True,dropout=dropout,name=prefix+\"cell_\"+str(i))\n",
        "      if i==0:\n",
        "        inp_layer = embed\n",
        "      else:\n",
        "        inp_layer = last_layer_model\n",
        "        \n",
        "      model_layer_out,state = layered_model(inp_layer,initial_state)\n",
        "      \n",
        "      last_layer_model = model_layer_out\n",
        "\n",
        "    model_states = [state]\n",
        "    \n",
        "  return model_states,last_layer_model,model_inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Br2JpV-7xET_"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Build the model\n",
        "def build_model(rnn_type,embedding_dim,encoder_layers,decoder_layers,dropout,latent_dim):\n",
        "  \n",
        "  encoder_states,encoder_outputs,encoder_inputs = build_layered_RNN_model(rnn_type=rnn_type,embedding_in_dim = num_encoder_tokens,embedding_out_dim = embedding_dim,layers = encoder_layers,dropout = dropout,inp_length = max_encoder_seq_length,model_out_dim = latent_dim,prefix=encoder_model_prefix)\n",
        "\n",
        "  _,decoder_outputs,decoder_inputs = build_layered_RNN_model(rnn_type=rnn_type,embedding_in_dim = num_decoder_tokens,embedding_out_dim = embedding_dim,layers = decoder_layers,dropout = dropout,inp_length = max_decoder_seq_length,model_out_dim = latent_dim,prefix=decoder_model_prefix,initial_state = encoder_states)\n",
        "  \n",
        "  decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\",name='final')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "  model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for constructing inference model"
      ],
      "metadata": {
        "id": "vTi5-tj7oye0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "iUuV-IQ8ltes"
      },
      "outputs": [],
      "source": [
        "def get_inference_encoder_model(model):\n",
        "  encoder_layers,_ = get_encoder_decoder_layers_from_model(model)\n",
        "  encoder_inputs = model.input[0]  \n",
        "  if isinstance(model.get_layer(encoder_model_prefix+\"cell_0\"), keras.layers.LSTM):\n",
        "    encoder_outputs, state_h_enc, state_c_enc = model.get_layer(encoder_model_prefix+\"cell_\"+str(encoder_layers - 1)).output  \n",
        "    encoder_states = [state_h_enc, state_c_enc]\n",
        "  elif (isinstance(model.get_layer(encoder_model_prefix+\"cell_0\"), keras.layers.GRU) or isinstance(model.get_layer(encoder_model_prefix+\"cell_0\"), keras.layers.RNN)):\n",
        "    encoder_outputs, state = model.get_layer(encoder_model_prefix+\"cell_\"+str(encoder_layers - 1)).output  \n",
        "    encoder_states = [state]\n",
        "\n",
        "  encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "  return encoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WhC8eYW4mPv-"
      },
      "outputs": [],
      "source": [
        "def get_inference_decoder_model(model):\n",
        "  latent_dim = get_latent_dim_from_model(model)\n",
        "  _,decoder_layers = get_encoder_decoder_layers_from_model(model)\n",
        "\n",
        "  # Decoder during inference takes just one character(i.e vector rep of a character). This is either from previous timestep or start of sequence(\"\\t\")\n",
        "  decoder_inputs =  keras.Input(shape=( 1))\n",
        "  # Contains input to each decoder layer\n",
        "  decoder_states_inputs=[]\n",
        "  # Contains state output from each decoder layer\n",
        "  decoder_states=[]\n",
        "  previous_decoder_output = None\n",
        "\n",
        "  emdedded_rep_of_decoder_input = model.get_layer(decoder_model_prefix+\"embed\")(decoder_inputs)\n",
        "  \n",
        "  if isinstance(model.get_layer(decoder_model_prefix+\"cell_0\"), keras.layers.LSTM):\n",
        "    for i in range(decoder_layers):\n",
        "      #every layer must have an input through which we can supply it's hidden state\n",
        "      decoder_state_input_h = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "      decoder_state_input_c = keras.Input(shape=(latent_dim,),name='inp4_'+str(i))\n",
        "      init_state = [decoder_state_input_h, decoder_state_input_c]\n",
        "      decoder_lstm = model.get_layer(decoder_model_prefix+\"cell_\"+str(i))\n",
        "      if i==0:\n",
        "        decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(emdedded_rep_of_decoder_input, initial_state=init_state)\n",
        "      else:\n",
        "        decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(previous_decoder_output, initial_state=init_state )\n",
        "      \n",
        "      previous_decoder_output = decoder_outputs\n",
        "      decoder_states_inputs.append (decoder_state_input_h)\n",
        "      decoder_states_inputs.append (decoder_state_input_c)\n",
        "      decoder_states.append (state_h_dec)\n",
        "      decoder_states.append (state_c_dec)\n",
        "  elif isinstance(model.get_layer(decoder_model_prefix+\"cell_0\"), keras.layers.GRU):\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_state_input = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "      init_state = [decoder_state_input]\n",
        "      decoder_gru = model.get_layer(decoder_model_prefix+\"cell_\"+str(i))\n",
        "      if i==0:\n",
        "        decoder_outputs, state = decoder_gru(emdedded_rep_of_decoder_input, initial_state=init_state)\n",
        "      else:\n",
        "        decoder_outputs, state = decoder_gru(previous_decoder_output, initial_state=init_state )\n",
        "      \n",
        "      previous_decoder_output = decoder_outputs\n",
        "      decoder_states_inputs.append (decoder_state_input)\n",
        "      decoder_states.append (state)\n",
        "  elif isinstance(model.get_layer(decoder_model_prefix+\"cell_0\"), keras.layers.RNN):\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_state_input = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "      init_state = [decoder_state_input]\n",
        "      decoder_rnn = model.get_layer(decoder_model_prefix+\"cell_\"+str(i))\n",
        "      if i==0:\n",
        "        decoder_outputs, state = decoder_rnn(emdedded_rep_of_decoder_input, initial_state=init_state)\n",
        "      else:\n",
        "        decoder_outputs, state = decoder_rnn(previous_decoder_output, initial_state=init_state )\n",
        "      \n",
        "      previous_decoder_output = decoder_outputs\n",
        "      decoder_states_inputs.append (decoder_state_input)\n",
        "      decoder_states.append (state)      \n",
        "  decoder_dense = model.get_layer('final')\n",
        "  decoder_outputs = decoder_dense(previous_decoder_output)\n",
        "  decoder_model = keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "  return decoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NDJJuJM8koKB"
      },
      "outputs": [],
      "source": [
        "def build_inference_model(model):\n",
        "    encoder_model = get_inference_encoder_model(model)\n",
        "    \n",
        "    decoder_model = get_inference_decoder_model(model)\n",
        "\n",
        "    return encoder_model,decoder_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for running samples on inference model(with and without beam search)"
      ],
      "metadata": {
        "id": "FpLlZooEo8U_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XshCgw_96g5y"
      },
      "outputs": [],
      "source": [
        "class BeamRecordKeeping:\n",
        "  def __init__(self,decoder_input_state_values,prev_char_index,joint_probability,accumulated_previous_chars):\n",
        "    self.decoder_input_state_values = decoder_input_state_values.copy()\n",
        "    self.prev_char_index = np.copy(prev_char_index)\n",
        "    self.joint_probability = joint_probability\n",
        "    self.accumulated_previous_chars = accumulated_previous_chars\n",
        "\n",
        "  def __str__(self):\n",
        "    return \"decoder_input_state_values: \"+str(self.decoder_input_state_values) + \"\\n prev_char_index: \"+str(self.prev_char_index)+\"\\n joint_probability: \"+str(self.joint_probability)+\" accumulated_previous_chars: \"+str(self.accumulated_previous_chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "510BPMuu6eS7"
      },
      "outputs": [],
      "source": [
        "def get_sampled_char(sampled_token_index):\n",
        "  if sampled_token_index == 0:\n",
        "    sampled_char='\\n'\n",
        "  else:\n",
        "    sampled_char = index_to_char_target[sampled_token_index]\n",
        "  return sampled_char\n",
        "\n",
        "def get_predicted_word_from_beam(list_of_beam_objs):\n",
        "  current_highest_score = list_of_beam_objs[0].joint_probability\n",
        "  predicted_word = list_of_beam_objs[0].accumulated_previous_chars\n",
        "  for each_obj in list_of_beam_objs:\n",
        "    if(each_obj.joint_probability > current_highest_score):\n",
        "      predicted_word = each_obj.accumulated_previous_chars\n",
        "  return predicted_word"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This function is for cases where beam_width=1"
      ],
      "metadata": {
        "id": "hOla8vEXpW2e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Qojx7JC35S5y"
      },
      "outputs": [],
      "source": [
        "def decode_batch_of_sequences(rnn_type,input_seq,encoder_model,decoder_model,batch_size,encoder_layers,decoder_layers):\n",
        "    # Get encoder output\n",
        "    encoder_output_state_values = encoder_model.predict(input_seq)\n",
        "    if rnn_type=='GRU' or 'RNN':\n",
        "      decoder_input_state_values=[encoder_output_state_values]\n",
        "    \n",
        "    # This is needed because encoder state is fed to all decoder layers\n",
        "    decoder_input_state_values = decoder_input_state_values * decoder_layers\n",
        "    \n",
        "    # This is contain previously predicted character's index for every words in batch.\n",
        "    prev_char_index = np.zeros((batch_size, 1))\n",
        "    # We start with \\t for every word in batch\n",
        "    prev_char_index[:, 0] = target_tokenizer.word_index['\\t']\n",
        "    \n",
        "    predicted_words = [ \"\" for i in range(batch_size)]\n",
        "    done=[False for i in range(batch_size)]\n",
        "    for i in range(max_decoder_seq_length):\n",
        "        decoder_out = decoder_model.predict(tuple([prev_char_index] + decoder_input_state_values))\n",
        "        # Decoder output has both output of all timesteps followed by hidden states\n",
        "        output_probability = decoder_out[0]\n",
        "        # Decoder state input is previous layer state output\n",
        "        decoder_input_state_values = decoder_out[1:]\n",
        "        for j in range(batch_size):\n",
        "          if done[j]:\n",
        "            continue          \n",
        "          sampled_token_index = np.argmax(output_probability[j, -1, :])\n",
        "          if sampled_token_index == 0:\n",
        "            sampled_char='\\n'\n",
        "          else:\n",
        "            sampled_char = index_to_char_target[sampled_token_index]\n",
        "          if sampled_char == '\\n':\n",
        "            done[j]=True\n",
        "            continue            \n",
        "          predicted_words[j] += sampled_char\n",
        "          #update the previously predicted characters        \n",
        "          prev_char_index[j,0]=target_tokenizer.word_index[sampled_char]\n",
        "    return predicted_words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This function is called when beam_width>1 during running inference model"
      ],
      "metadata": {
        "id": "uNkF8x_npNY4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Mwfd_Mx3WQmP"
      },
      "outputs": [],
      "source": [
        "def decode_batch_of_sequences_for_bigger_beam_width(rnn_type,input_seq,encoder_model,decoder_model,batch_size,encoder_layers,decoder_layers,beam_search_width):\n",
        "    print(\"batch_size\"+str(batch_size))\n",
        "    print(\"input_seq:\"+str(input_seq.shape))\n",
        "    next_list_of_beam_record_objects = []\n",
        "    predicted_words = [ \"\" for i in range(batch_size)]\n",
        "    list_of_beam_record_objects = []\n",
        "    for j in range(batch_size):\n",
        "      next_list_of_beam_record_objects = []\n",
        "      list_of_beam_record_objects = []\n",
        "      if(j % 100 == 0):\n",
        "        print(\"**********Batch number*************:\"+str(j))\n",
        "      current_seq = input_seq[j]\n",
        "      current_seq = tf.expand_dims(current_seq, 0)\n",
        "      # Get encoder output\n",
        "      decoder_input_state_values = encoder_model.predict(current_seq)\n",
        "      if rnn_type=='GRU' or 'RNN':\n",
        "        decoder_input_state_values=[decoder_input_state_values]\n",
        "        decoder_input_state_values = decoder_input_state_values * decoder_layers\n",
        "      else:\n",
        "        decoder_input_state_values = decoder_input_state_values[0] * decoder_layers\n",
        "      \n",
        "      prev_char_index = np.zeros((1, 1))\n",
        "      # We start with \\t for every word in batch\n",
        "      prev_char_index[:, 0] = target_tokenizer.word_index['\\t']\n",
        "      done  = False\n",
        "      for _ in range(beam_search_width):\n",
        "        current_beam_search_obj = BeamRecordKeeping(decoder_input_state_values,prev_char_index,0,\"\")\n",
        "        list_of_beam_record_objects.append(current_beam_search_obj)\n",
        "\n",
        "      for i in range(max_decoder_seq_length):\n",
        "        if(done):\n",
        "          break\n",
        "\n",
        "        if(i != 0):\n",
        "          list_of_beam_record_objects = next_list_of_beam_record_objects\n",
        "        next_list_of_beam_record_objects = []\n",
        "        for beam_index in range(beam_search_width):\n",
        "          # print(\"prev_char_index\"+str(list_of_beam_record_objects[beam_index].prev_char_index.shape))\n",
        "          # print(\"decoder_input_state_values\"+str(len(list_of_beam_record_objects[beam_index].decoder_input_state_values)))\n",
        "          # print(\"decoder_input_state_values\"+str(list_of_beam_record_objects[beam_index].decoder_input_state_values[0].shape))\n",
        "\n",
        "          decoder_out = decoder_model.predict(tuple([list_of_beam_record_objects[beam_index].prev_char_index] + list_of_beam_record_objects[beam_index].decoder_input_state_values))\n",
        "          # Decoder output has both output of all timesteps followed by hidden states\n",
        "          output_probability = decoder_out[0]\n",
        "          # Decoder state input is previous layer state output\n",
        "          decoder_input_state_values = decoder_out[1:]\n",
        "          sampled_token_index = np.argsort(output_probability[0][-1, :])[-beam_search_width:]\n",
        "          sampled_probability_values = output_probability[0][-1, :][sampled_token_index]\n",
        "\n",
        "          for each_candidate in range(1,len(sampled_probability_values)+1):\n",
        "            new_joint_probability = list_of_beam_record_objects[beam_index].joint_probability + math.log(sampled_probability_values[-each_candidate])\n",
        "            if(len(next_list_of_beam_record_objects) < beam_search_width):\n",
        "              sampled_char = get_sampled_char(sampled_token_index[-each_candidate])\n",
        "              if sampled_char == '\\n':\n",
        "                done = True\n",
        "                break\n",
        "              accumulated_previous_chars = list_of_beam_record_objects[beam_index].accumulated_previous_chars + sampled_char\n",
        "              prev_char_index[:, 0]=target_tokenizer.word_index[sampled_char]\n",
        "              next_beam_record_keeping_obj = BeamRecordKeeping(decoder_input_state_values,prev_char_index,new_joint_probability,accumulated_previous_chars)\n",
        "              next_list_of_beam_record_objects.append(next_beam_record_keeping_obj)\n",
        "            else:\n",
        "              replace_indx = -1\n",
        "              for (current_indx,each_obj) in enumerate(next_list_of_beam_record_objects):\n",
        "                if(each_obj.joint_probability < new_joint_probability):\n",
        "                  replace_indx = current_indx\n",
        "                  break\n",
        "              if(replace_indx != -1):\n",
        "                sampled_char = get_sampled_char(sampled_token_index[-each_candidate])\n",
        "                if sampled_char == '\\n':\n",
        "                  done = True\n",
        "                  break\n",
        "                accumulated_previous_chars = list_of_beam_record_objects[beam_index].accumulated_previous_chars + sampled_char\n",
        "                prev_char_index[:, 0]=target_tokenizer.word_index[sampled_char]\n",
        "                next_beam_record_keeping_obj = BeamRecordKeeping(decoder_input_state_values,prev_char_index,new_joint_probability,accumulated_previous_chars)\n",
        "                next_list_of_beam_record_objects[replace_indx] = next_beam_record_keeping_obj\n",
        "          \n",
        "          if(done or i == max_decoder_seq_length-1):\n",
        "            if( len(next_list_of_beam_record_objects) == 0):\n",
        "              predicted_words[j] = get_predicted_word_from_beam(list_of_beam_record_objects)\n",
        "            else:\n",
        "              predicted_words[j] = get_predicted_word_from_beam(next_list_of_beam_record_objects)\n",
        "            break\n",
        "        \n",
        "    return predicted_words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for obtaining accuracies on inference model"
      ],
      "metadata": {
        "id": "36MKoOcnpbs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_row_to_csv(fields,rows,file_path):\n",
        "  # writing to csv file \n",
        "  with open(file_path, 'w',encoding='utf-8-sig') as csvfile: \n",
        "    # creating a csv writer object \n",
        "    csvwriter = csv.writer(csvfile) \n",
        "        \n",
        "    # writing the fields \n",
        "    csvwriter.writerow(fields) \n",
        "        \n",
        "    # writing the data rows \n",
        "    csvwriter.writerows(rows)"
      ],
      "metadata": {
        "id": "jxWA4RcoFIIM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "kbTznqNEx4sR"
      },
      "outputs": [],
      "source": [
        "def test_accuracy(model,encoder_model,decoder_model,beam_search_width=1):\n",
        "  rnn_type = get_rnntype_from_model(model)\n",
        "  encoder_layers,decoder_layers = get_encoder_decoder_layers_from_model(model)\n",
        "\n",
        "  overall_rows = []\n",
        "  sucess_rows = []\n",
        "  failure_rows = []\n",
        "  fields = ['Original', 'Actual_target', 'Predicted_target']\n",
        "\n",
        "  success=0\n",
        "  success_char = 0\n",
        "  total_chars = 0\n",
        "  #Get all the predicted words\n",
        "  if(beam_search_width == 0 or beam_search_width == 1):\n",
        "    pred=decode_batch_of_sequences(rnn_type,test_input_tensor,encoder_model,decoder_model,test_input_tensor.shape[0],encoder_layers,decoder_layers)\n",
        "  else:\n",
        "    pred=decode_batch_of_sequences_for_bigger_beam_width(rnn_type,test_input_tensor,encoder_model,decoder_model,test_input_tensor.shape[0],encoder_layers,decoder_layers,beam_search_width)\n",
        "  for seq_index in range(test_input_tensor.shape[0]):\n",
        "      predicted_word = pred[seq_index]\n",
        "      target_word=test_target_texts[seq_index][1:-1]\n",
        "      for (indx,each_ele) in enumerate(target_word):\n",
        "        total_chars += 1\n",
        "        if(indx < len(predicted_word)):\n",
        "          if(target_word[indx] == predicted_word[indx]):\n",
        "            success_char += 1\n",
        "      \n",
        "      each_row = [str(test_input_texts[seq_index]),str(target_word),str(predicted_word)]\n",
        "      overall_rows.append(each_row)\n",
        "      \n",
        "      #test the word one by one and write to files\n",
        "      if target_word == predicted_word:\n",
        "        sucess_rows.append(each_row)\n",
        "        success+=1\n",
        "        f = open(\"success.txt\", \"a\")\n",
        "        f.write(test_input_texts[seq_index]+' '+target_word+' '+predicted_word+'\\n')\n",
        "        f.close()\n",
        "      else:\n",
        "        failure_rows.append(each_row)\n",
        "        f = open(\"failure.txt\", \"a\")\n",
        "        f.write(test_input_texts[seq_index]+' '+target_word+' '+predicted_word+'\\n')\n",
        "        f.close()\n",
        "  \n",
        "  write_row_to_csv(fields,sucess_rows,\"success.csv\")\n",
        "  write_row_to_csv(fields,failure_rows,\"failure.csv\")\n",
        "  write_row_to_csv(fields,overall_rows,\"overall_rows.csv\")\n",
        "  \n",
        "  print(\"success:\"+str(success))\n",
        "  print(\"success_char:\"+str(success_char))\n",
        "  return float(success)/float(test_input_tensor.shape[0]),float(success_char)/float(total_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "jcWesBWKm2hn"
      },
      "outputs": [],
      "source": [
        "def batch_validate(model,encoder_model,decoder_model,beam_search_width=1):\n",
        "  rnn_type = get_rnntype_from_model(model)\n",
        "  encoder_layers,decoder_layers = get_encoder_decoder_layers_from_model(model)\n",
        "\n",
        "  success = 0\n",
        "  success_char = 0\n",
        "  total_chars = 0\n",
        "  #get all the predicted words\n",
        "  if(beam_search_width == 0 or beam_search_width == 1):\n",
        "    pred=decode_batch_of_sequences(rnn_type,val_input_tensor,encoder_model,decoder_model,val_input_tensor.shape[0],encoder_layers,decoder_layers)\n",
        "  else:\n",
        "    pred=decode_batch_of_sequences_for_bigger_beam_width(rnn_type,val_input_tensor,encoder_model,decoder_model,val_input_tensor.shape[0],encoder_layers,decoder_layers,beam_search_width)\n",
        "\n",
        "  for seq_index in range(val_input_tensor.shape[0]):\n",
        "    predicted_word = pred[seq_index]\n",
        "    target_word = val_target_texts[seq_index][1:-1]\n",
        "    #test the words one by one\n",
        "    if predicted_word == target_word:\n",
        "      # print(\"pred:\"+str(pred[seq_index]))\n",
        "      # print(\"Target: \"+str(val_target_texts[seq_index][1:-1]))\n",
        "      success+=1\n",
        "      \n",
        "    for (indx,each_ele) in enumerate(target_word):\n",
        "      total_chars += 1\n",
        "      if(indx < len(predicted_word)):\n",
        "        if(target_word[indx] == predicted_word[indx]):\n",
        "          # print(\"pred:\"+str(pred[seq_index]))\n",
        "          # print(\"Target: \"+str(target_word))\n",
        "          success_char += 1\n",
        "  \n",
        "  print(\"success:\"+str(success))\n",
        "  print(\"success_char:\"+str(success_char))\n",
        "  # print(\"val_input_tensor.shape[0]:\"+str(val_input_tensor.shape[0]))\n",
        "  return float(success)/float(val_input_tensor.shape[0]),float(success_char)/float(total_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to help and plot confusion matrix"
      ],
      "metadata": {
        "id": "chWR9OEUp1Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def obtain_original_target_pred_list_from_file(file_path):\n",
        "  file_as_list = []\n",
        "  source_lang_list = []\n",
        "  pred_target_lang_list = []\n",
        "  actual_target_lang_list = []\n",
        "  with open(file_path, \"r\") as file:\n",
        "    file_as_list = file.readlines()\n",
        "  \n",
        "  source_lang_list = [0]*len(file_as_list)\n",
        "  pred_target_lang_list = [0]*len(file_as_list)\n",
        "  actual_target_lang_list = [0]*len(file_as_list)\n",
        "  \n",
        "  for ind in range(len(file_as_list)):\n",
        "    current_line_as_str = ''.join(file_as_list[ind])\n",
        "    split_words = current_line_as_str.split(\" \")\n",
        "    source_lang_list[ind] = split_words[0]\n",
        "    actual_target_lang_list[ind] = split_words[1]\n",
        "    pred_target_lang_list[ind] = split_words[2].replace(\"\\n\",'')\n",
        "\n",
        "  return source_lang_list,actual_target_lang_list,pred_target_lang_list"
      ],
      "metadata": {
        "id": "Rmrx-9OFycVW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_character_wise_list(str_list):\n",
        "  char_list = []\n",
        "  for each_str in str_list:\n",
        "    temp = []\n",
        "    temp[:0] = each_str\n",
        "    char_list = char_list + temp\n",
        "  \n",
        "  return char_list"
      ],
      "metadata": {
        "id": "PcQyeFCdCZ2z"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_k_frequent_token_from_tokenizer(tokenizer,k):\n",
        "  list_of_tuples=[]\n",
        "  for i, (word, count) in enumerate(tokenizer.word_counts.items()):\n",
        "    list_of_tuples.append((word,count))\n",
        "\n",
        "  sorted_count_tuples = sorted(list_of_tuples, key = lambda x: x[1])\n",
        "  top_k_count_tuples = sorted_count_tuples[-k:] \n",
        "  bottom_k_count_tuples = sorted_count_tuples[:k]\n",
        "  \n",
        "  strt_idx = (len(sorted_count_tuples) // 2) - (k // 2)\n",
        "  end_idx = (len(sorted_count_tuples) // 2) + (k // 2)\n",
        "  \n",
        "  # slicing extracting middle elements\n",
        "  middle_k_count_tuples = sorted_count_tuples[strt_idx: end_idx + 1]\n",
        "\n",
        "  top_token_list = []\n",
        "  for each_token in top_k_count_tuples:\n",
        "    top_token_list.append(each_token[0])\n",
        "  \n",
        "  \n",
        "  bottom_token_list = []\n",
        "  for each_token in bottom_k_count_tuples:\n",
        "    bottom_token_list.append(each_token[0])\n",
        "\n",
        "  middle_token_list = []\n",
        "  for each_token in middle_k_count_tuples:\n",
        "    middle_token_list.append(each_token[0])\n",
        "\n",
        "  return top_token_list,bottom_token_list,middle_token_list"
      ],
      "metadata": {
        "id": "eCzOMcIGTxxg"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_label_to_indx_dict(label_list):\n",
        "  label_to_indx = dict()\n",
        "  for (indx,label) in enumerate(label_list):\n",
        "    label_to_indx[label] = indx\n",
        "  return label_to_indx"
      ],
      "metadata": {
        "id": "t2vbL6T0xiJF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retain_character_in_list(inp_list,retain_list):\n",
        "  ret_list = []\n",
        "  for each_ele  in inp_list:\n",
        "    if(each_ele in retain_list):\n",
        "      ret_list.append(each_ele)\n",
        "  \n",
        "  return ret_list  "
      ],
      "metadata": {
        "id": "zEUYNp-NYnaH"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def equalise_str_by_padding(str1,str2,padding_char):\n",
        "  ret_str1 = str1\n",
        "  ret_str2 = str2\n",
        "  length1 = len(convert_to_character_wise_list([str1]))\n",
        "  length2 = len(convert_to_character_wise_list([str2]))\n",
        "  if(length2 > length1):\n",
        "    ret_str1 = str1.ljust(length2,padding_char)\n",
        "  elif(length1 > length2):\n",
        "    ret_str2 = str2.ljust(length1,padding_char)\n",
        "\n",
        "  return ret_str1,ret_str2    "
      ],
      "metadata": {
        "id": "QZa3kHDejgV6"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_character_sequence_to_indexed_sequence(inp_list,label_to_indx):\n",
        "  ret_list = [0] * len(inp_list)\n",
        "  for (indx,each_ele) in enumerate(inp_list):\n",
        "    ret_list[indx] = label_to_indx[each_ele]\n",
        "  return ret_list"
      ],
      "metadata": {
        "id": "HP-Le70ssYCq"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def obtained_characterwise_padded_rep_of_actual_pred(actual_target_lang_list,pred_target_lang_list,padding_char):\n",
        "  ret_actual_target_lang_list = []\n",
        "  ret_pred_target_lang_list = []\n",
        "  for (indx,each_actual_word) in enumerate(actual_target_lang_list):\n",
        "    each_pred_word = pred_target_lang_list[indx]\n",
        "    each_pred_word,each_actual_word = equalise_str_by_padding(each_pred_word,each_actual_word,padding_char)\n",
        "    ret_actual_target_lang_list = ret_actual_target_lang_list + convert_to_character_wise_list(each_actual_word)\n",
        "    ret_pred_target_lang_list = ret_pred_target_lang_list + convert_to_character_wise_list(each_pred_word)\n",
        "  \n",
        "  return ret_actual_target_lang_list,ret_pred_target_lang_list"
      ],
      "metadata": {
        "id": "cJ9xoN4lofK1"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retain_character_in_list_from_pred_actual(actual_list,pred_list,retain_list):\n",
        "  ret_pred_list = []\n",
        "  ret_actual_list = []\n",
        "  for (indx,each_pred_char) in enumerate(pred_list):\n",
        "    each_actual_char = actual_list[indx]\n",
        "    if((each_actual_char in retain_list) and (each_pred_char in retain_list)):\n",
        "      ret_pred_list.append(each_pred_char)\n",
        "      ret_actual_list.append(each_actual_char)\n",
        "  \n",
        "  return ret_actual_list,ret_pred_list"
      ],
      "metadata": {
        "id": "E_vu6AskXohG"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix_subcases(characterwise_actual_target_lang_list,characterwise_pred_target_lang_list,retain_list,target_key_list,mode,number_of_symbols,use_wandb=False):\n",
        "  filtered_characterwise_actual_target_lang_list,filtered_characterwise_pred_target_lang_list = retain_character_in_list_from_pred_actual(characterwise_actual_target_lang_list,characterwise_pred_target_lang_list,retain_list)\n",
        "  label_list = retain_character_in_list(target_key_list,retain_list)\n",
        "\n",
        "  dict_label = construct_label_to_indx_dict(label_list)\n",
        "  indxed_actual_target_lang_list = convert_character_sequence_to_indexed_sequence(filtered_characterwise_actual_target_lang_list,dict_label)\n",
        "  indxed_pred_target_lang_list = convert_character_sequence_to_indexed_sequence(filtered_characterwise_pred_target_lang_list,dict_label)\n",
        "\n",
        "  cnf_matrix = confusion_matrix(filtered_characterwise_actual_target_lang_list, filtered_characterwise_pred_target_lang_list, labels=label_list)\n",
        "  print(cnf_matrix)\n",
        "  print(\"Labels:\"+str(label_list))\n",
        "\n",
        "  if(use_wandb == True):\n",
        "    if(mode == \"top\"):\n",
        "      wandb.log({\"conf_mat_top\" : wandb.plot.confusion_matrix(\n",
        "                        probs=None,\n",
        "                        y_true=indxed_actual_target_lang_list,\n",
        "                        preds=indxed_pred_target_lang_list,\n",
        "                        class_names=label_list,\n",
        "                        title=\"Top-\"+str(len(retain_list))+\" frequent symbols confusion matrix\"\n",
        "                        )})\n",
        "    elif(mode == \"bot\"):\n",
        "      wandb.log({\"conf_mat_bottom\" : wandb.plot.confusion_matrix(\n",
        "                        probs=None,\n",
        "                        y_true=indxed_actual_target_lang_list,\n",
        "                        preds=indxed_pred_target_lang_list,\n",
        "                        class_names=label_list,\n",
        "                        title=\"Least-\"+str(len(retain_list))+\" frequent symbols confusion matrix\"\n",
        "                        )})\n",
        "    elif(mode == \"mid\"):\n",
        "      wandb.log({\"conf_mat_middle\" : wandb.plot.confusion_matrix(\n",
        "                        probs=None,\n",
        "                        y_true=indxed_actual_target_lang_list,\n",
        "                        preds=indxed_pred_target_lang_list,\n",
        "                        class_names=label_list,\n",
        "                        title=\"Middle-\"+str(len(retain_list))+\" frequent symbols confusion matrix\"\n",
        "                        )})"
      ],
      "metadata": {
        "id": "bOcvSPTIfny4"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix_from_success_failure_files(number_of_symbols = 11,success_file_path=\"success.txt\",failure_file_path=\"failure.txt\",use_wandb=False):\n",
        "  target_key_list = list(target_tokenizer.word_index.keys())\n",
        "  target_key_list.remove('\\t')\n",
        "\n",
        "  char_to_index = dict((target_key_list[indx], indx) for indx in range(len(target_key_list)))\n",
        "\n",
        "  _,success_actual_target_lang_list,success_pred_target_lang_list = obtain_original_target_pred_list_from_file(success_file_path)\n",
        "  _,failure_actual_target_lang_list,failure_pred_target_lang_list = obtain_original_target_pred_list_from_file(failure_file_path)\n",
        "\n",
        "  actual_target_lang_list = failure_actual_target_lang_list + success_actual_target_lang_list\n",
        "  pred_target_lang_list = failure_pred_target_lang_list + success_pred_target_lang_list\n",
        "\n",
        "  characterwise_actual_target_lang_list,characterwise_pred_target_lang_list = obtained_characterwise_padded_rep_of_actual_pred(actual_target_lang_list,pred_target_lang_list,\"\\n\")\n",
        "\n",
        "  retain_top_list,retain_bottom_list,retain_middle_list = get_top_k_frequent_token_from_tokenizer(target_tokenizer,number_of_symbols)\n",
        "  if '\\t' in retain_top_list:\n",
        "    retain_top_list.remove('\\t')\n",
        "  if '\\t' in retain_bottom_list:\n",
        "    retain_bottom_list.remove('\\t')\n",
        "  if '\\t' in retain_middle_list:\n",
        "    retain_middle_list.remove('\\t')\n",
        "\n",
        "  print(\"Top-k symbols confusion matrix\")\n",
        "  plot_confusion_matrix_subcases(characterwise_actual_target_lang_list,characterwise_pred_target_lang_list,retain_top_list,target_key_list,mode='top',number_of_symbols=number_of_symbols,use_wandb=use_wandb)\n",
        "  print(\"Bottom-k symbols confusion matrix\")\n",
        "  plot_confusion_matrix_subcases(characterwise_actual_target_lang_list,characterwise_pred_target_lang_list,retain_bottom_list,target_key_list,number_of_symbols=number_of_symbols,use_wandb=use_wandb,mode='bot')\n",
        "  print(\"Mid-k symbols confusion matrix\")\n",
        "  plot_confusion_matrix_subcases(characterwise_actual_target_lang_list,characterwise_pred_target_lang_list,retain_middle_list,target_key_list,number_of_symbols=number_of_symbols,use_wandb=use_wandb,mode='mid')\n",
        "  "
      ],
      "metadata": {
        "id": "Iwa2u46MxByP"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code to train model and return inference and root model "
      ],
      "metadata": {
        "id": "6zIFQx1pptUS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "P_psbbJxVblX"
      },
      "outputs": [],
      "source": [
        "def run_custom_model(encoder_layers,decoder_layers,epochs,lr,latent_dim,rnn_type,embedding_dim,dropout,bs,optimizer,model_save_path,encoder_save_path,decoder_save_path,save=False,use_wandb=False):\n",
        "    if(save == True):\n",
        "      from google.colab import drive\n",
        "      drive.mount('/content/drive')\n",
        "      # Create a MirroredStrategy.\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        strategy = tf.distribute.MirroredStrategy()\n",
        "    else:  # use default strategy\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
        "        # Open a strategy scope and create the model\n",
        "    with strategy.scope():\n",
        "      model = build_model(rnn_type,embedding_dim,encoder_layers,decoder_layers,dropout,latent_dim)\n",
        "\n",
        "    plot_model(model, to_file=str(model_save_path)+'.png', show_shapes=True, show_dtype=True,show_layer_names=True)\n",
        "\n",
        "    model.summary()\n",
        "    \n",
        "    optimizerObj = get_optimizer(optimizer,lr)\n",
        "    \n",
        "    model.compile(optimizer=optimizerObj, loss=keras.losses.SparseCategoricalCrossentropy(reduction='none'), metrics=[\"accuracy\"])\n",
        "    if(use_wandb == False):\n",
        "      hist=model.fit([train_input_tensor, train_target_tensor],tf.concat([train_target_tensor[:,1:],tf.zeros((train_target_tensor[:,:].shape[0],1))], axis=1),batch_size=bs,epochs=epochs,shuffle=True)\n",
        "    else:\n",
        "      hist=model.fit([train_input_tensor, train_target_tensor],tf.concat([train_target_tensor[:,1:],tf.zeros((train_target_tensor[:,:].shape[0],1))], axis=1),batch_size=bs,epochs=epochs,shuffle=True,callbacks=[WandbCallback(), history])\n",
        "\n",
        "    encoder_inference_model,decoder_inference_model=build_inference_model(model)\n",
        "    plot_model(encoder_inference_model, to_file=str(encoder_save_path)+'.png', show_shapes=True)\n",
        "    plot_model(decoder_inference_model, to_file=str(decoder_save_path)+'.png', show_shapes=True)\n",
        "\n",
        "    if(save == True):\n",
        "      model.save('drive/MyDrive/Colab Notebooks/'+str(model_save_path)+'.h5')\n",
        "      encoder_inference_model.save('drive/MyDrive/Colab Notebooks/'+str(encoder_save_path)+\".h5\")\n",
        "      decoder_inference_model.save('drive/MyDrive/Colab Notebooks/'+str(decoder_save_path)+\".h5\")\n",
        "      plot_model(encoder_inference_model, to_file='drive/MyDrive/Colab Notebooks/'+str(encoder_save_path)+\".png\", show_shapes=True)\n",
        "      plot_model(decoder_inference_model, to_file='drive/MyDrive/Colab Notebooks/'+str(decoder_save_path)+'.png', show_shapes=True)\n",
        "      plot_model(model, to_file='drive/MyDrive/Colab Notebooks/'+str(model_save_path)+'.png', show_shapes=True, show_dtype=True,show_layer_names=True)\n",
        "\n",
        "\n",
        "    model.save(str(model_save_path)+'.h5')\n",
        "    encoder_inference_model.save(str(encoder_save_path)+'.h5')\n",
        "    decoder_inference_model.save(str(decoder_save_path)+'.h5')\n",
        "\n",
        "    return model,encoder_inference_model,decoder_inference_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code used to obtain accuracy either by loading models from path(in GDrive or local) or by passing model objects"
      ],
      "metadata": {
        "id": "XGAcBNw6qGNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_run_accuracy(is_test=False,model_path=None,encoder_inf_path=None,decoder_inf_path=None,from_gdrive=False,beam_width = 1,model=None,encoder_inference_model=None,decoder_inference_model=None):\n",
        "  if(from_gdrive == True):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "  \n",
        "  if(model is None):\n",
        "    print(\"Load model from path\")\n",
        "    model = keras.models.load_model(model_path)\n",
        "  \n",
        "  if(encoder_inf_path is None or decoder_inf_path is None):\n",
        "    if(decoder_inference_model is None or encoder_inference_model is None):\n",
        "      encoder_inference_model,decoder_inference_model=build_inference_model(model)\n",
        "  else:\n",
        "    encoder_inference_model= keras.models.load_model(encoder_inf_path)\n",
        "    decoder_inference_model= keras.models.load_model(decoder_inf_path)\n",
        "  \n",
        "  if(is_test == True):\n",
        "    word_val_acc,char_val_acc=test_accuracy(model,encoder_inference_model,decoder_inference_model,beam_width)\n",
        "  else:\n",
        "    word_val_acc,char_val_acc=batch_validate(model,encoder_inference_model,decoder_inference_model,beam_width)\n",
        "\n",
        "  return word_val_acc,char_val_acc"
      ],
      "metadata": {
        "id": "MYpd1tvBn59I"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code to run the best model observed and return inference and root models\n",
        "Later, load_model_run_accuracy needs to be called to obtain accuracies"
      ],
      "metadata": {
        "id": "0OAWGT3BqPyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_best_model(save=False):\n",
        "  encoder_layers = 3\n",
        "  decoder_layers = 3\n",
        "  epochs = 20\n",
        "  lr = 0.0001\n",
        "  latent_dim = 1024\n",
        "  rnn_type = 'GRU'\n",
        "  embedding_dim = 512\n",
        "  dropout = 0.4\n",
        "  bs = 64\n",
        "  optimizer = \"Adam\"\n",
        "  model,encoder_inference_model,decoder_inference_model = run_custom_model(encoder_layers,decoder_layers,epochs,lr,latent_dim,rnn_type,embedding_dim,dropout,bs,optimizer,\"best_model_assignment_3\",\"best_encoder_inference_model_assignment_3\",\"best_decoder_inference_model_assignment_3\",save)\n",
        "  return model,encoder_inference_model,decoder_inference_model"
      ],
      "metadata": {
        "id": "EpOpF5Rv1FFG"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code related to hyper-parameter tuning"
      ],
      "metadata": {
        "id": "HaatzB6vqIaA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "VX5pjUqzDVr-"
      },
      "outputs": [],
      "source": [
        "default_config = {\n",
        "        \"rnn_type\": \"LSTM\",\n",
        "        \"dropout\": 0.5,\n",
        "        \"encoder_layers\":3,\n",
        "        \"decoder_layers\":3,\n",
        "        \"latent_dim\": 64,\n",
        "        \"epochs\": 1,\n",
        "        \"lr\": 0.0001,\n",
        "        \"embedding_out_dim\": 64,\n",
        "        \"beam_search\":1,\n",
        "        \"batch_size\":64,\n",
        "        \"optimizer\": \"Adam\"\n",
        "    }\n",
        "\n",
        "#Keras callback    \n",
        "history = History()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "PvOMovPZyGqj"
      },
      "outputs": [],
      "source": [
        "def HP_tuning_run():\n",
        "    # Create a MirroredStrategy.\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        strategy = tf.distribute.MirroredStrategy()\n",
        "    else:  # use default strategy\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
        "\n",
        "    # wandb.init(config=default_config, magic=True,project=\"DeepLearningAssignment-3\", entity='cs21s002-ee21s113-dlassignment-1')\n",
        "    wandb.init(config=default_config, magic=True,project=\"DL-Assignment3\", entity='cs21s002-ee21s113-dlassignment-1')\n",
        "    # wandb.init(config=default_config, magic=True,project=\"DeepLearningAssignment-3\", entity='akshaygrao')\n",
        "    config = wandb.config\n",
        "    print(\"Config: \"+str(config))\n",
        "    run_name = str(config).replace(\"{\", \"\").replace(\"}\",\"\").replace(\":\",\"-\")\n",
        "    wandb.run.name = run_name\n",
        "    \n",
        "    model,encoder_inference_model,decoder_inference_model = run_custom_model(encoder_layers=config.encoder_layers,decoder_layers=config.decoder_layers,epochs=config.epochs,lr=config.lr,latent_dim=config.latent_dim,rnn_type=config.rnn_type,embedding_dim=config.embedding_out_dim,dropout=config.dropout,bs=config.batch_size,optimizer=config.optimizer,model_save_path=f'{run_name.replace(\",\",\"-\")}_model',encoder_save_path=f'{run_name.replace(\",\",\"-\")}_encoder',decoder_save_path=f'{run_name.replace(\",\",\"-\")}_decoder',save=False,use_wandb=True)\n",
        "\n",
        "    word_val_acc,char_val_acc = load_model_run_accuracy(model=model,encoder_inference_model=encoder_inference_model,decoder_inference_model=decoder_inference_model,beam_width=config.beam_search)\n",
        "    print(\"word_val_acc\"+str(word_val_acc))\n",
        "    print(\"char_val_acc\"+str(char_val_acc))\n",
        "    wandb.log({\"word_val_acc\":round(word_val_acc,5)})\n",
        "    wandb.log({\"char_val_acc\":round(char_val_acc,5)})\n",
        "    wandb.log({\"language\":transliteration_target_language})\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "w5KOeeV13mvj"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    \"name\": \"Assignment-3-final-batch-optimizer\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\":{\n",
        "      \"goal\": \"maximize\",\n",
        "      \"name\": \"word_val_acc\"\n",
        "    },\n",
        "    \"project\": 'DL-Assignment3',\n",
        "    \"parameters\": {\n",
        "        \"rnn_type\": {\n",
        "            \"values\": [\"LSTM\",\"GRU\",\"RNN\"]\n",
        "        },\n",
        "        \"dropout\": {\n",
        "            \"values\": [0.2,0.4]\n",
        "        },\n",
        "        \"encoder_layers\": {\n",
        "            \"values\": [3]\n",
        "        },\n",
        "        \"decoder_layers\": {\n",
        "            \"values\": [3]\n",
        "        },\n",
        "        \"latent_dim\": {\n",
        "            \"values\": [512,1024,2048]\n",
        "        },\n",
        "        \"epochs\": {\n",
        "            \"values\": [20]\n",
        "        },\n",
        "        \"lr\": {\n",
        "            \"values\": [0.0001]\n",
        "        },\n",
        "        \"embedding_out_dim\": {\n",
        "            \"values\":[128,256]\n",
        "        },\n",
        "        \"beam_search\":{\n",
        "            \"values\":[1]\n",
        "        },\n",
        "        \"batch_size\":{\n",
        "            \"values\":[64,128,256]\n",
        "        },\n",
        "        \"optimizer\":{\n",
        "            \"values\":[\"Nadam\",\"SGD\",\"RMSprop\"]\n",
        "        }\n",
        "        \n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "fag3IotW5Qu2"
      },
      "outputs": [],
      "source": [
        "# sweep_id = wandb.sweep(sweep_config,  project='DeepLearningAssignment-3', entity='akshaygrao')\n",
        "# sweep_id = wandb.sweep(sweep_config,  project='DeepLearningAssignment-3', entity='cs21s002-ee21s113-dlassignment-1')\n",
        "# sweep_id = wandb.sweep(sweep_config,  project='DL-Assignment3', entity='cs21s002-ee21s113-dlassignment-1')\n",
        "sweep_id=\"1fwq5qge\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itgfsYWC5oCn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8ce059013ef1496084f47129ab2a44d4",
            "9514eae985c2427c8fa6d5ad8185a577",
            "0113ab6223e14383b60ad82a6c562661",
            "b993f329f60e4ee18af076d9e44a88ee",
            "2d44dad5fb474689a085d4ad6b4619b7",
            "90f053840c964fe7a9780f53df2d633c",
            "0eb32e0706ce4a0f884046e82a611ed7",
            "f7cebc53ec7748e5b6abb9a2f6cc633f",
            "37bc8ae3b20c436c8d92d84b5a9987e7",
            "2504ea5c58d44b4b806119c58481846c",
            "bc21b4e3525b43cf935e38d58d35f294",
            "85ae35621f5944c799dcf85590212e0b",
            "cb0a8ebce8504584a9d3c0464a8289bb",
            "48f55f2809f144168df64d1a016dd54d",
            "020c03729142430c89f58ea60ab40b88",
            "2476c2cc652d4c8aaeea1d6cd97afccb"
          ]
        },
        "outputId": "deaf0c06-7c8f-458c-9218-2e7adc7f8956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wk3abcim with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_out_dim: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatent_dim: 1024\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Nadam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_type: RNN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Number of devices: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.15"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220503_164258-wk3abcim</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/runs/wk3abcim\" target=\"_blank\">driven-sweep-11</a></strong> to <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/sweeps/1fwq5qge\" target=\"_blank\">https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/sweeps/1fwq5qge</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config: {'batch_size': 128, 'beam_search': 1, 'decoder_layers': 3, 'dropout': 0.2, 'embedding_out_dim': 256, 'encoder_layers': 3, 'epochs': 20, 'latent_dim': 1024, 'lr': 0.0001, 'optimizer': 'Nadam', 'rnn_type': 'RNN'}\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Number of devices: 1\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Enc_inp (InputLayer)           [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " Enc_embed (Embedding)          (None, 26, 256)      6912        ['Enc_inp[0][0]']                \n",
            "                                                                                                  \n",
            " Enc_cell_0 (SimpleRNN)         [(None, 26, 1024),   1311744     ['Enc_embed[0][0]']              \n",
            "                                 (None, 1024)]                                                    \n",
            "                                                                                                  \n",
            " Dec_inp (InputLayer)           [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " Enc_cell_1 (SimpleRNN)         [(None, 26, 1024),   2098176     ['Enc_cell_0[0][0]']             \n",
            "                                 (None, 1024)]                                                    \n",
            "                                                                                                  \n",
            " Dec_embed (Embedding)          (None, 26, 256)      16384       ['Dec_inp[0][0]']                \n",
            "                                                                                                  \n",
            " Enc_cell_2 (SimpleRNN)         [(None, 26, 1024),   2098176     ['Enc_cell_1[0][0]']             \n",
            "                                 (None, 1024)]                                                    \n",
            "                                                                                                  \n",
            " Dec_cell_0 (SimpleRNN)         [(None, 26, 1024),   1311744     ['Dec_embed[0][0]',              \n",
            "                                 (None, 1024)]                    'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " Dec_cell_1 (SimpleRNN)         [(None, 26, 1024),   2098176     ['Dec_cell_0[0][0]',             \n",
            "                                 (None, 1024)]                    'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " Dec_cell_2 (SimpleRNN)         [(None, 26, 1024),   2098176     ['Dec_cell_1[0][0]',             \n",
            "                                 (None, 1024)]                    'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " final (Dense)                  (None, 26, 64)       65600       ['Dec_cell_2[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 11,105,088\n",
            "Trainable params: 11,105,088\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "396/396 [==============================] - 159s 379ms/step - loss: 0.9926 - accuracy: 0.7461 - _timestamp: 1651596345.0000 - _runtime: 166.0000\n",
            "Epoch 2/20\n",
            "396/396 [==============================] - 151s 382ms/step - loss: 0.4605 - accuracy: 0.8671 - _timestamp: 1651596496.0000 - _runtime: 317.0000\n",
            "Epoch 3/20\n",
            "396/396 [==============================] - 151s 380ms/step - loss: 0.3124 - accuracy: 0.9079 - _timestamp: 1651596647.0000 - _runtime: 468.0000\n",
            "Epoch 4/20\n",
            "396/396 [==============================] - 149s 377ms/step - loss: 0.2361 - accuracy: 0.9300 - _timestamp: 1651596796.0000 - _runtime: 617.0000\n",
            "Epoch 5/20\n",
            "396/396 [==============================] - 150s 379ms/step - loss: 0.1882 - accuracy: 0.9438 - _timestamp: 1651596946.0000 - _runtime: 767.0000\n",
            "Epoch 6/20\n",
            "396/396 [==============================] - 151s 380ms/step - loss: 0.1551 - accuracy: 0.9534 - _timestamp: 1651597097.0000 - _runtime: 918.0000\n",
            "Epoch 7/20\n",
            "396/396 [==============================] - 150s 378ms/step - loss: 0.1318 - accuracy: 0.9602 - _timestamp: 1651597247.0000 - _runtime: 1068.0000\n",
            "Epoch 8/20\n",
            "396/396 [==============================] - 149s 377ms/step - loss: 0.1138 - accuracy: 0.9655 - _timestamp: 1651597396.0000 - _runtime: 1217.0000\n",
            "Epoch 9/20\n",
            "396/396 [==============================] - 149s 377ms/step - loss: 0.0997 - accuracy: 0.9699 - _timestamp: 1651597545.0000 - _runtime: 1366.0000\n",
            "Epoch 10/20\n",
            "396/396 [==============================] - 151s 382ms/step - loss: 0.0890 - accuracy: 0.9729 - _timestamp: 1651597696.0000 - _runtime: 1517.0000\n",
            "Epoch 11/20\n",
            "396/396 [==============================] - 151s 382ms/step - loss: 0.0793 - accuracy: 0.9757 - _timestamp: 1651597848.0000 - _runtime: 1669.0000\n",
            "Epoch 12/20\n",
            "396/396 [==============================] - 151s 380ms/step - loss: 0.0715 - accuracy: 0.9781 - _timestamp: 1651597998.0000 - _runtime: 1819.0000\n",
            "Epoch 13/20\n",
            "396/396 [==============================] - 150s 380ms/step - loss: 0.0649 - accuracy: 0.9800 - _timestamp: 1651598149.0000 - _runtime: 1970.0000\n",
            "Epoch 14/20\n",
            "396/396 [==============================] - 151s 382ms/step - loss: 0.0598 - accuracy: 0.9816 - _timestamp: 1651598300.0000 - _runtime: 2121.0000\n",
            "Epoch 15/20\n",
            "396/396 [==============================] - 152s 384ms/step - loss: 0.0543 - accuracy: 0.9831 - _timestamp: 1651598452.0000 - _runtime: 2273.0000\n",
            "Epoch 16/20\n",
            "396/396 [==============================] - 151s 380ms/step - loss: 0.0506 - accuracy: 0.9842 - _timestamp: 1651598602.0000 - _runtime: 2423.0000\n",
            "Epoch 17/20\n",
            "396/396 [==============================] - 151s 381ms/step - loss: 0.0470 - accuracy: 0.9853 - _timestamp: 1651598753.0000 - _runtime: 2574.0000\n",
            "Epoch 18/20\n",
            "396/396 [==============================] - 150s 379ms/step - loss: 0.0431 - accuracy: 0.9864 - _timestamp: 1651598903.0000 - _runtime: 2724.0000\n",
            "Epoch 19/20\n",
            "396/396 [==============================] - 151s 380ms/step - loss: 0.0409 - accuracy: 0.9871 - _timestamp: 1651599054.0000 - _runtime: 2875.0000\n",
            "Epoch 20/20\n",
            "396/396 [==============================] - 151s 381ms/step - loss: 0.0378 - accuracy: 0.9881 - _timestamp: 1651599205.0000 - _runtime: 3026.0000\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "success:1965\n",
            "success_char:30528\n",
            "word_val_acc0.3908891983290233\n",
            "char_val_acc0.751624975379161\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.920 MB of 0.920 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ce059013ef1496084f47129ab2a44d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▅▆▆▇▇▇▇▇███████████</td></tr><tr><td>char_val_acc</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>word_val_acc</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.9881</td></tr><tr><td>char_val_acc</td><td>0.75162</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>language</td><td>kn</td></tr><tr><td>loss</td><td>0.03776</td></tr><tr><td>word_val_acc</td><td>0.39089</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">driven-sweep-11</strong>: <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/runs/wk3abcim\" target=\"_blank\">https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/runs/wk3abcim</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220503_164258-wk3abcim/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: t2lyky2c with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_out_dim: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatent_dim: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: RMSprop\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_type: RNN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Number of devices: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "wandb version 0.12.16 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.15"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220503_173413-t2lyky2c</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/runs/t2lyky2c\" target=\"_blank\">helpful-sweep-12</a></strong> to <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/sweeps/1fwq5qge\" target=\"_blank\">https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/sweeps/1fwq5qge</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config: {'batch_size': 64, 'beam_search': 1, 'decoder_layers': 3, 'dropout': 0.4, 'embedding_out_dim': 256, 'encoder_layers': 3, 'epochs': 20, 'latent_dim': 512, 'lr': 0.0001, 'optimizer': 'RMSprop', 'rnn_type': 'RNN'}\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Number of devices: 1\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Enc_inp (InputLayer)           [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " Enc_embed (Embedding)          (None, 26, 256)      6912        ['Enc_inp[0][0]']                \n",
            "                                                                                                  \n",
            " Enc_cell_0 (SimpleRNN)         [(None, 26, 512),    393728      ['Enc_embed[0][0]']              \n",
            "                                 (None, 512)]                                                     \n",
            "                                                                                                  \n",
            " Dec_inp (InputLayer)           [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " Enc_cell_1 (SimpleRNN)         [(None, 26, 512),    524800      ['Enc_cell_0[0][0]']             \n",
            "                                 (None, 512)]                                                     \n",
            "                                                                                                  \n",
            " Dec_embed (Embedding)          (None, 26, 256)      16384       ['Dec_inp[0][0]']                \n",
            "                                                                                                  \n",
            " Enc_cell_2 (SimpleRNN)         [(None, 26, 512),    524800      ['Enc_cell_1[0][0]']             \n",
            "                                 (None, 512)]                                                     \n",
            "                                                                                                  \n",
            " Dec_cell_0 (SimpleRNN)         [(None, 26, 512),    393728      ['Dec_embed[0][0]',              \n",
            "                                 (None, 512)]                     'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " Dec_cell_1 (SimpleRNN)         [(None, 26, 512),    524800      ['Dec_cell_0[0][0]',             \n",
            "                                 (None, 512)]                     'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " Dec_cell_2 (SimpleRNN)         [(None, 26, 512),    524800      ['Dec_cell_1[0][0]',             \n",
            "                                 (None, 512)]                     'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " final (Dense)                  (None, 26, 64)       32832       ['Dec_cell_2[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,942,784\n",
            "Trainable params: 2,942,784\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "791/791 [==============================] - 166s 202ms/step - loss: 0.9658 - accuracy: 0.7427 - _timestamp: 1651599426.0000 - _runtime: 173.0000\n",
            "Epoch 2/20\n",
            "791/791 [==============================] - 159s 202ms/step - loss: 0.5570 - accuracy: 0.8415 - _timestamp: 1651599586.0000 - _runtime: 333.0000\n",
            "Epoch 3/20\n",
            "791/791 [==============================] - 158s 199ms/step - loss: 0.4307 - accuracy: 0.8742 - _timestamp: 1651599743.0000 - _runtime: 490.0000\n",
            "Epoch 4/20\n",
            "791/791 [==============================] - 158s 200ms/step - loss: 0.3590 - accuracy: 0.8936 - _timestamp: 1651599901.0000 - _runtime: 648.0000\n",
            "Epoch 5/20\n",
            "791/791 [==============================] - 156s 197ms/step - loss: 0.3128 - accuracy: 0.9064 - _timestamp: 1651600057.0000 - _runtime: 804.0000\n",
            "Epoch 6/20\n",
            "791/791 [==============================] - 156s 197ms/step - loss: 0.2800 - accuracy: 0.9156 - _timestamp: 1651600213.0000 - _runtime: 960.0000\n",
            "Epoch 7/20\n",
            "791/791 [==============================] - 156s 198ms/step - loss: 0.2545 - accuracy: 0.9229 - _timestamp: 1651600369.0000 - _runtime: 1116.0000\n",
            "Epoch 8/20\n",
            "791/791 [==============================] - 156s 198ms/step - loss: 0.2338 - accuracy: 0.9290 - _timestamp: 1651600526.0000 - _runtime: 1273.0000\n",
            "Epoch 9/20\n",
            "791/791 [==============================] - 156s 197ms/step - loss: 0.2170 - accuracy: 0.9337 - _timestamp: 1651600681.0000 - _runtime: 1428.0000\n",
            "Epoch 10/20\n",
            "791/791 [==============================] - 156s 197ms/step - loss: 0.2031 - accuracy: 0.9379 - _timestamp: 1651600837.0000 - _runtime: 1584.0000\n",
            "Epoch 11/20\n",
            "791/791 [==============================] - 157s 199ms/step - loss: 0.1912 - accuracy: 0.9414 - _timestamp: 1651600995.0000 - _runtime: 1742.0000\n",
            "Epoch 12/20\n",
            "791/791 [==============================] - 158s 200ms/step - loss: 0.1807 - accuracy: 0.9444 - _timestamp: 1651601153.0000 - _runtime: 1900.0000\n",
            "Epoch 13/20\n",
            "791/791 [==============================] - 158s 200ms/step - loss: 0.1717 - accuracy: 0.9470 - _timestamp: 1651601311.0000 - _runtime: 2058.0000\n",
            "Epoch 14/20\n",
            "791/791 [==============================] - 158s 200ms/step - loss: 0.1633 - accuracy: 0.9497 - _timestamp: 1651601469.0000 - _runtime: 2216.0000\n",
            "Epoch 15/20\n",
            "791/791 [==============================] - 158s 199ms/step - loss: 0.1567 - accuracy: 0.9515 - _timestamp: 1651601627.0000 - _runtime: 2374.0000\n",
            "Epoch 16/20\n",
            "791/791 [==============================] - 157s 198ms/step - loss: 0.1499 - accuracy: 0.9535 - _timestamp: 1651601783.0000 - _runtime: 2530.0000\n",
            "Epoch 17/20\n",
            "791/791 [==============================] - 158s 200ms/step - loss: 0.1447 - accuracy: 0.9550 - _timestamp: 1651601941.0000 - _runtime: 2688.0000\n",
            "Epoch 18/20\n",
            "791/791 [==============================] - 156s 198ms/step - loss: 0.1390 - accuracy: 0.9570 - _timestamp: 1651602098.0000 - _runtime: 2845.0000\n",
            "Epoch 19/20\n",
            "791/791 [==============================] - 159s 201ms/step - loss: 0.1342 - accuracy: 0.9582 - _timestamp: 1651602257.0000 - _runtime: 3004.0000\n",
            "Epoch 20/20\n",
            "791/791 [==============================] - 156s 198ms/step - loss: 0.1294 - accuracy: 0.9597 - _timestamp: 1651602413.0000 - _runtime: 3160.0000\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "success:1786\n",
            "success_char:29353\n",
            "word_val_acc0.355281480007957\n",
            "char_val_acc0.7226954894622809\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.705 MB of 0.705 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37bc8ae3b20c436c8d92d84b5a9987e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▄▅▆▆▇▇▇▇▇▇█████████</td></tr><tr><td>char_val_acc</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>word_val_acc</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.95968</td></tr><tr><td>char_val_acc</td><td>0.7227</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>language</td><td>kn</td></tr><tr><td>loss</td><td>0.12938</td></tr><tr><td>word_val_acc</td><td>0.35528</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">helpful-sweep-12</strong>: <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/runs/t2lyky2c\" target=\"_blank\">https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/runs/t2lyky2c</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220503_173413-t2lyky2c/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1vr3ap6t with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_out_dim: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatent_dim: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: RMSprop\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_type: RNN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Number of devices: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "wandb version 0.12.16 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.15"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220503_182726-1vr3ap6t</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/runs/1vr3ap6t\" target=\"_blank\">worthy-sweep-13</a></strong> to <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/sweeps/1fwq5qge\" target=\"_blank\">https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/sweeps/1fwq5qge</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config: {'batch_size': 128, 'beam_search': 1, 'decoder_layers': 3, 'dropout': 0.4, 'embedding_out_dim': 256, 'encoder_layers': 3, 'epochs': 20, 'latent_dim': 512, 'lr': 0.0001, 'optimizer': 'RMSprop', 'rnn_type': 'RNN'}\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Number of devices: 1\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Enc_inp (InputLayer)           [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " Enc_embed (Embedding)          (None, 26, 256)      6912        ['Enc_inp[0][0]']                \n",
            "                                                                                                  \n",
            " Enc_cell_0 (SimpleRNN)         [(None, 26, 512),    393728      ['Enc_embed[0][0]']              \n",
            "                                 (None, 512)]                                                     \n",
            "                                                                                                  \n",
            " Dec_inp (InputLayer)           [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " Enc_cell_1 (SimpleRNN)         [(None, 26, 512),    524800      ['Enc_cell_0[0][0]']             \n",
            "                                 (None, 512)]                                                     \n",
            "                                                                                                  \n",
            " Dec_embed (Embedding)          (None, 26, 256)      16384       ['Dec_inp[0][0]']                \n",
            "                                                                                                  \n",
            " Enc_cell_2 (SimpleRNN)         [(None, 26, 512),    524800      ['Enc_cell_1[0][0]']             \n",
            "                                 (None, 512)]                                                     \n",
            "                                                                                                  \n",
            " Dec_cell_0 (SimpleRNN)         [(None, 26, 512),    393728      ['Dec_embed[0][0]',              \n",
            "                                 (None, 512)]                     'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " Dec_cell_1 (SimpleRNN)         [(None, 26, 512),    524800      ['Dec_cell_0[0][0]',             \n",
            "                                 (None, 512)]                     'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " Dec_cell_2 (SimpleRNN)         [(None, 26, 512),    524800      ['Dec_cell_1[0][0]',             \n",
            "                                 (None, 512)]                     'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " final (Dense)                  (None, 26, 64)       32832       ['Dec_cell_2[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,942,784\n",
            "Trainable params: 2,942,784\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "396/396 [==============================] - 127s 299ms/step - loss: 1.0973 - accuracy: 0.7139 - _timestamp: 1651602580.0000 - _runtime: 134.0000\n",
            "Epoch 2/20\n",
            "396/396 [==============================] - 117s 295ms/step - loss: 0.6704 - accuracy: 0.8124 - _timestamp: 1651602697.0000 - _runtime: 251.0000\n",
            "Epoch 3/20\n",
            "396/396 [==============================] - 117s 296ms/step - loss: 0.5178 - accuracy: 0.8509 - _timestamp: 1651602814.0000 - _runtime: 368.0000\n",
            "Epoch 4/20\n",
            "396/396 [==============================] - 115s 291ms/step - loss: 0.4355 - accuracy: 0.8728 - _timestamp: 1651602929.0000 - _runtime: 483.0000\n",
            "Epoch 5/20\n",
            "396/396 [==============================] - 117s 295ms/step - loss: 0.3807 - accuracy: 0.8877 - _timestamp: 1651603046.0000 - _runtime: 600.0000\n",
            "Epoch 6/20\n",
            "396/396 [==============================] - 118s 298ms/step - loss: 0.3411 - accuracy: 0.8986 - _timestamp: 1651603164.0000 - _runtime: 718.0000\n",
            "Epoch 7/20\n",
            "396/396 [==============================] - 117s 297ms/step - loss: 0.3085 - accuracy: 0.9075 - _timestamp: 1651603281.0000 - _runtime: 835.0000\n",
            "Epoch 8/20\n",
            "255/396 [==================>...........] - ETA: 42s - loss: 0.2873 - accuracy: 0.9135"
          ]
        }
      ],
      "source": [
        "# wandb.agent(sweep_id, function=HP_tuning_run, project='DeepLearningAssignment-3', entity='akshaygrao')\n",
        "# wandb.agent(sweep_id, function=HP_tuning_run, project='DeepLearningAssignment-3', entity='cs21s002-ee21s113-dlassignment-1')\n",
        "wandb.agent(sweep_id, function=HP_tuning_run, project='DL-Assignment3', entity='cs21s002-ee21s113-dlassignment-1')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run this code in colab to get test accuracy for best model"
      ],
      "metadata": {
        "id": "LgC5dYaPqdgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to get model from gdrive(change path) and obtain accuracy or test or validation set\n",
        "# load_model_run_accuracy(is_test=True,from_gdrive=True,model_path=\"drive/MyDrive/Colab Notebooks/best_model_assignment_3.h5\",encoder_inf_path=\"drive/MyDrive/Colab Notebooks/best_encoder_inference_model_assignment_3.h5\",decoder_inference_model=\"drive/MyDrive/Colab Notebooks/best_decoder_inference_model_assignment_3.h5\")\n"
      ],
      "metadata": {
        "id": "GzTzZ5iCwGIK"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this by passing success and failure files to get confusion matrix\n",
        "# plot_confusion_matrix_from_success_failure_files(number_of_symbols = 11,success_file_path=\"success.txt\",failure_file_path=\"failure.txt\",use_wandb=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZUVAE0cloCO",
        "outputId": "4c5b6c36-7fa3-4068-dbce-19f0c19742e0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-k symbols confusion matrix\n",
            "[[   0  150  165   47   52  124   22   78   47   12]\n",
            " [ 138 3106  123   61   85   23   64   20  131   18]\n",
            " [ 154   82 2602   10   30    5   49   32   27   16]\n",
            " [  89   68   25 1960   99    3   43   45   40   54]\n",
            " [  23  114   60  109 1910   31   37    9    4   17]\n",
            " [ 150   22    9    7   22 1559    7   14   37   13]\n",
            " [  35   49   23   47   18   12 1282   30    2   21]\n",
            " [  63   11   26   76   18   16    6 1377   15   10]\n",
            " [  38  129   25   51    7   46    9    8 1422   16]\n",
            " [   4   11    6   51   33   11    4    6   22 1523]]\n",
            "Labels:['\\n', '್', 'ಿ', 'ಾ', 'ರ', 'ು', 'ತ', 'ದ', 'ನ', 'ವ']\n",
            "Bottom-k symbols confusion matrix\n",
            "[[ 6  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 10  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  3  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  8  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  2  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  1]]\n",
            "Labels:['ಞ', 'ಈ', 'ಐ', 'ಛ', 'ಢ', 'ಓ', 'ಔ', 'ಊ', 'ಃ', 'ಋ', 'ಝ']\n",
            "Mid-k symbols confusion matrix\n",
            "[[258   0   0   0   0   0   3   0   0  11   1]\n",
            " [  0 189   0   0  22   0   0   0   4   0   0]\n",
            " [  0   0 137   0   0   1   0   0   1   0   0]\n",
            " [  0   0   0 149   0   0   0   0   0   4   0]\n",
            " [  0  21   1   0 162   0   0   0  42   0   0]\n",
            " [  0   0   0   3   0 300   0   0   0   0   0]\n",
            " [  3   0   0   0   1   0 223   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0 222   0   0   0]\n",
            " [  0   5   0   0  17   0   0   0 190   0   0]\n",
            " [ 30   0   0   0   0   0   3   0   0 132   0]\n",
            " [  0   0   3   0   0   0   0   0   0   0 142]]\n",
            "Labels:['ಶ', 'ೂ', 'ೀ', 'ಣ', 'ೋ', 'ಜ', 'ಚ', 'ಅ', 'ೊ', 'ಷ', 'ಧ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model,encoder_inference_model,decoder_inference_model = run_best_model(True)\n",
        "word_val_acc,char_val_acc = load_model_run_accuracy(model=model,encoder_inference_model=encoder_inference_model,decoder_inference_model=decoder_inference_model,is_test=True)\n",
        "print(\"Test word_val_acc\"+str(word_val_acc))\n",
        "print(\"Test char_val_acc\"+str(char_val_acc))"
      ],
      "metadata": {
        "id": "EyVtENGJ76B0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77093b49-91d4-4a3d-efb6-6a8963b202e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Number of devices: 1\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Enc_inp (InputLayer)           [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " Enc_embed (Embedding)          (None, 26, 512)      13824       ['Enc_inp[0][0]']                \n",
            "                                                                                                  \n",
            " Enc_cell_0 (GRU)               [(None, 26, 1024),   4724736     ['Enc_embed[0][0]']              \n",
            "                                 (None, 1024)]                                                    \n",
            "                                                                                                  \n",
            " Dec_inp (InputLayer)           [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " Enc_cell_1 (GRU)               [(None, 26, 1024),   6297600     ['Enc_cell_0[0][0]']             \n",
            "                                 (None, 1024)]                                                    \n",
            "                                                                                                  \n",
            " Dec_embed (Embedding)          (None, 26, 512)      32768       ['Dec_inp[0][0]']                \n",
            "                                                                                                  \n",
            " Enc_cell_2 (GRU)               [(None, 26, 1024),   6297600     ['Enc_cell_1[0][0]']             \n",
            "                                 (None, 1024)]                                                    \n",
            "                                                                                                  \n",
            " Dec_cell_0 (GRU)               [(None, 26, 1024),   4724736     ['Dec_embed[0][0]',              \n",
            "                                 (None, 1024)]                    'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " Dec_cell_1 (GRU)               [(None, 26, 1024),   6297600     ['Dec_cell_0[0][0]',             \n",
            "                                 (None, 1024)]                    'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " Dec_cell_2 (GRU)               [(None, 26, 1024),   6297600     ['Dec_cell_1[0][0]',             \n",
            "                                 (None, 1024)]                    'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " final (Dense)                  (None, 26, 64)       65600       ['Dec_cell_2[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 34,752,064\n",
            "Trainable params: 34,752,064\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "Epoch 1/20\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "791/791 [==============================] - 221s 262ms/step - loss: 1.0752 - accuracy: 0.7148\n",
            "Epoch 2/20\n",
            "791/791 [==============================] - 209s 264ms/step - loss: 0.8129 - accuracy: 0.7685\n",
            "Epoch 3/20\n",
            "791/791 [==============================] - 209s 264ms/step - loss: 0.6499 - accuracy: 0.8127\n",
            "Epoch 4/20\n",
            "791/791 [==============================] - 209s 264ms/step - loss: 0.4866 - accuracy: 0.8590\n",
            "Epoch 5/20\n",
            "791/791 [==============================] - 209s 264ms/step - loss: 0.3540 - accuracy: 0.8963\n",
            "Epoch 6/20\n",
            "791/791 [==============================] - 208s 263ms/step - loss: 0.2587 - accuracy: 0.9241\n",
            "Epoch 7/20\n",
            "791/791 [==============================] - 208s 263ms/step - loss: 0.1937 - accuracy: 0.9435\n",
            "Epoch 8/20\n",
            "791/791 [==============================] - 208s 263ms/step - loss: 0.1497 - accuracy: 0.9564\n",
            "Epoch 9/20\n",
            "791/791 [==============================] - 209s 264ms/step - loss: 0.1185 - accuracy: 0.9655\n",
            "Epoch 10/20\n",
            "791/791 [==============================] - 209s 264ms/step - loss: 0.0956 - accuracy: 0.9723\n",
            "Epoch 11/20\n",
            "791/791 [==============================] - 209s 264ms/step - loss: 0.0784 - accuracy: 0.9773\n",
            "Epoch 12/20\n",
            "791/791 [==============================] - 209s 264ms/step - loss: 0.0652 - accuracy: 0.9813\n",
            "Epoch 13/20\n",
            "791/791 [==============================] - 208s 263ms/step - loss: 0.0548 - accuracy: 0.9843\n",
            "Epoch 14/20\n",
            "791/791 [==============================] - 208s 264ms/step - loss: 0.0461 - accuracy: 0.9869\n",
            "Epoch 15/20\n",
            "791/791 [==============================] - 208s 263ms/step - loss: 0.0394 - accuracy: 0.9888\n",
            "Epoch 16/20\n",
            "791/791 [==============================] - 209s 264ms/step - loss: 0.0339 - accuracy: 0.9904\n",
            "Epoch 17/20\n",
            "791/791 [==============================] - 209s 264ms/step - loss: 0.0292 - accuracy: 0.9917\n",
            "Epoch 18/20\n",
            "791/791 [==============================] - 208s 264ms/step - loss: 0.0253 - accuracy: 0.9929\n",
            "Epoch 19/20\n",
            "791/791 [==============================] - 208s 263ms/step - loss: 0.0226 - accuracy: 0.9935\n",
            "Epoch 20/20\n",
            "791/791 [==============================] - 208s 263ms/step - loss: 0.0199 - accuracy: 0.9944\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "success:2237\n",
            "success_char:31484\n",
            "Test word_val_acc0.44323360412126017\n",
            "Test char_val_acc0.7593275932759328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_val_acc,char_val_acc = load_model_run_accuracy(model=model,encoder_inference_model=encoder_inference_model,decoder_inference_model=decoder_inference_model,is_test=False)\n",
        "print(\"Validation word_val_acc\"+str(word_val_acc))\n",
        "print(\"Validation char_val_acc\"+str(char_val_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnRAsh3CyjVJ",
        "outputId": "0408bb55-089e-4489-d870-8c475b2c79a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "success:2276\n",
            "success_char:31069\n",
            "Validation word_val_acc0.45275512233936743\n",
            "Validation char_val_acc0.7649448493204648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Command line argument handlers"
      ],
      "metadata": {
        "id": "mpsSeuYIqkCQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hBu8EDUmm-f"
      },
      "outputs": [],
      "source": [
        "def init_argparse() -> argparse.ArgumentParser:\n",
        "    parser = argparse.ArgumentParser()\n",
        "    \n",
        "    subparsers = parser.add_subparsers(dest='command')\n",
        "    run_model_parser = subparsers.add_parser('run_model')\n",
        "    run_model_parser.add_argument(\"--encoder_layers\", action=\"store\",dest='encoder_layers', help=\"Specify number of layers in encoder(default:3)\",default=3,required=False)\n",
        "    run_model_parser.add_argument(\"--decoder_layers\", action=\"store\",dest='decoder_layers', help=\"Specify number of layers in decoder(default:3)\",default=3,required=False)\n",
        "    run_model_parser.add_argument(\"--epochs\", action=\"store\",dest='epochs', help=\"Specify number of epochs(Default:15)\",default=15,required=False)\n",
        "    run_model_parser.add_argument(\"--lr\", action=\"store\",dest='lr', help=\"Specify learning rate(Default:0.0001)\",default=0.0001,required=False)\n",
        "    run_model_parser.add_argument(\"--latent_dim\", action=\"store\",dest='latent_dim', help=\"Specify latent dimensions(Default:1024)\",default=1024,required=False)\n",
        "    run_model_parser.add_argument(\"--rnn_type\", action=\"store\",dest='rnn_type', help=\"Specify cell type of RNN('RNN','GRU','LSTM'))(Default:GRU)\",default='GRU',required=False)\n",
        "    run_model_parser.add_argument(\"--embedding_dim\", action=\"store\",dest='embedding_dim', help=\"Specify dimension of embedding layer output(Default:256)\",default=256,required=False)\n",
        "    run_model_parser.add_argument(\"--dropout\", action=\"store\",dest='dropout', help=\"Specify dropout in input (applies at all layers of encoder and decoder)(Default:0.4)\",default=0.4,required=False)\n",
        "    run_model_parser.add_argument(\"--bs\", action=\"store\",dest='bs', help=\"Specify batch size(Default:64)\",default=64,required=False)\n",
        "    run_model_parser.add_argument(\"--optimizer\", action=\"store\",dest='optimizer', help=\"Specify optimizer algorithm('Adam','Nadam','SGD','RMSprop')(Default:'Adam')\",default='Adam',required=False)\n",
        "    run_model_parser.add_argument(\"--model_save_path\", action=\"store\",dest='model_save_path', help=\"Specify path to save model into(Default:'model')\",default='model',required=False)\n",
        "    run_model_parser.add_argument(\"--encoder_save_path\", action=\"store\",dest='encoder_save_path', help=\"Specify path to save encoder model into(Default:'encoder_inference_model')\",default='encoder_inference_model',required=False)\n",
        "    run_model_parser.add_argument(\"--decoder_save_path\", action=\"store\",dest='decoder_save_path', help=\"Specify path to save decoder model into(Default:'decoder_inference_model')\",default='decoder_inference_model',required=False)\n",
        "    run_model_parser.add_argument(\"--save\", action=\"store_true\",dest='save', help=\"Save model to google drive(default false)\",required=False)\n",
        "\n",
        "    run_best_model_parser = subparsers.add_parser('run_best_model')\n",
        "    run_best_model_parser.add_argument(\"--save\", action=\"store_true\",dest='save', help=\"Save model to google drive(default false)\",required=False)\n",
        "\n",
        "    last_accuracy_parser = subparsers.add_parser('last_model_run_accuracy')\n",
        "    last_accuracy_parser.add_argument(\"--is_test\", action=\"store_true\",dest='is_test', help=\"Specify if you need test accuracy(or validation accuracy). If passed assumes True otherwise returns validation accuracy\",required=False)\n",
        "    last_accuracy_parser.add_argument(\"--beam_width\", action=\"store\",dest='beam_width', help=\"Specify beam width(Default:1)\",default=1,required=False)\n",
        "\n",
        "    accuracy_parser = subparsers.add_parser('load_model_run_accuracy')\n",
        "    accuracy_parser.add_argument(\"--model_path\", action=\"store\",dest='model_path', help=\"Specify model path\",required=True)\n",
        "    accuracy_parser.add_argument(\"--encoder_inf_path\", action=\"store\",dest='encoder_inf_path', help=\"Specify encoder model path\",required=True)\n",
        "    accuracy_parser.add_argument(\"--decoder_inf_path\", action=\"store\",dest='decoder_inf_path', help=\"Specify decoder model path\",required=True)\n",
        "    accuracy_parser.add_argument(\"--from_gdrive\", action=\"store_true\",dest='from_gdrive', help=\"Retrieve path from google drive(If passed assumes True)\",required=False)\n",
        "    accuracy_parser.add_argument(\"--beam_width\", action=\"store\",dest='beam_width', help=\"Specify beam width(Default:1)\",default=1,required=False)\n",
        "    accuracy_parser.add_argument(\"--is_test\", action=\"store_true\",dest='is_test', help=\"Specify if you need test accuracy(or validation accuracy). If passed assumes True otherwise returns validation accuracy\",required=False)\n",
        "\n",
        "    return parser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  parser = init_argparse()\n",
        "  args = parser.parse_args(['--help'])\n",
        "\n",
        "  model = None\n",
        "  encoder_inference_model = None\n",
        "  decoder_inference_model = None\n",
        "  \n",
        "  if(args.command == 'run_model'):\n",
        "    encoder_layers = args.encoder_layers \n",
        "    decoder_layers = args.decoder_layers \n",
        "    epochs = args.epochs \n",
        "    lr = args.lr \n",
        "    latent_dim = args.latent_dim \n",
        "    rnn_type = args.rnn_type \n",
        "    embedding_dim = args.embedding_dim \n",
        "    dropout = args.dropout \n",
        "    bs = args.bs \n",
        "    optimizer = args.optimizer \n",
        "    model_save_path = args.model_save_path \n",
        "    encoder_save_path = args.encoder_save_path \n",
        "    decoder_save_path = args.decoder_save_path \n",
        "    save = args.save\n",
        "\n",
        "    model,encoder_inference_model,decoder_inference_model = run_custom_model(encoder_layers,decoder_layers,epochs,lr,latent_dim,rnn_type,embedding_dim,dropout,bs,beam_width,optimizer,model_save_path,encoder_save_path,decoder_save_path,save=save)\n",
        "  \n",
        "  elif(args.command == 'run_best_model'):\n",
        "    print(\"Running best model----\")\n",
        "    model,encoder_inference_model,decoder_inference_model = run_best_model(save=args.save)\n",
        "  elif(args.command == 'last_model_run_accuracy'):\n",
        "    if(model is None or encoder_inference_model is None or decoder_inference_model is None):\n",
        "      print(\"Producing accuracy of last run model\")\n",
        "      is_test = args.is_test\n",
        "      beam_width = args.beam_width\n",
        "      if(is_test):\n",
        "        word_val_acc,char_val_acc = load_model_run_accuracy(model=model,encoder_inference_model=encoder_inference_model,decoder_inference_model=decoder_inference_model,beam_width=beam_width,is_test=is_test)\n",
        "        print(\"Test word_val_acc\"+str(word_val_acc))\n",
        "        print(\"Test char_val_acc\"+str(char_val_acc))\n",
        "      else:\n",
        "        word_val_acc,char_val_acc = load_model_run_accuracy(model=model,encoder_inference_model=encoder_inference_model,decoder_inference_model=decoder_inference_model,beam_width=beam_width,is_test=is_test)\n",
        "        print(\"Validation word_val_acc\"+str(word_val_acc))\n",
        "        print(\"Validation char_val_acc\"+str(char_val_acc))\n",
        "    else:\n",
        "      print(\"Invalid command!!! Run a model before running accuracy\")\n",
        "  elif(args.command == 'load_model_run_accuracy'):\n",
        "    print(\"Producing accuracy by running model loaded from path\")\n",
        "    is_test = args.is_test\n",
        "    beam_width = args.beam_width\n",
        "    model_path = args.model_path\n",
        "    encoder_inf_path = args.encoder_inf_path\n",
        "    decoder_inf_path = args.decoder_inf_path\n",
        "    from_gdrive = args.from_gdrive\n",
        "    \n",
        "    if(is_test):\n",
        "      word_val_acc,char_val_acc = load_model_run_accuracy(model_path=model_path,encoder_inf_path=encoder_inf_path,decoder_inf_path=decoder_inf_path,beam_width=beam_width,is_test=is_test,from_gdrive=from_gdrive)\n",
        "      print(\"Test word_val_acc\"+str(word_val_acc))\n",
        "      print(\"Test char_val_acc\"+str(char_val_acc))\n",
        "    else:\n",
        "      word_val_acc,char_val_acc = load_model_run_accuracy(model_path=model_path,encoder_inf_path=encoder_inf_path,decoder_inf_path=decoder_inf_path,beam_width=beam_width,is_test=is_test,from_gdrive=from_gdrive)\n",
        "      print(\"Validation word_val_acc\"+str(word_val_acc))\n",
        "      print(\"Validation char_val_acc\"+str(char_val_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "QkmYTo7hZ17s",
        "outputId": "d973a595-dc02-4083-b797-787f931478a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: ipykernel_launcher.py [-h]\n",
            "                             {run_model,run_best_model,last_model_run_accuracy,load_model_run_accuracy}\n",
            "                             ...\n",
            "\n",
            "positional arguments:\n",
            "  {run_model,run_best_model,last_model_run_accuracy,load_model_run_accuracy}\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xxycHaDOlpgp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment_3_Part_A_without_attn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8ce059013ef1496084f47129ab2a44d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9514eae985c2427c8fa6d5ad8185a577",
              "IPY_MODEL_0113ab6223e14383b60ad82a6c562661"
            ],
            "layout": "IPY_MODEL_b993f329f60e4ee18af076d9e44a88ee"
          }
        },
        "9514eae985c2427c8fa6d5ad8185a577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d44dad5fb474689a085d4ad6b4619b7",
            "placeholder": "​",
            "style": "IPY_MODEL_90f053840c964fe7a9780f53df2d633c",
            "value": "0.940 MB of 0.940 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "0113ab6223e14383b60ad82a6c562661": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0eb32e0706ce4a0f884046e82a611ed7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7cebc53ec7748e5b6abb9a2f6cc633f",
            "value": 1
          }
        },
        "b993f329f60e4ee18af076d9e44a88ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d44dad5fb474689a085d4ad6b4619b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90f053840c964fe7a9780f53df2d633c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0eb32e0706ce4a0f884046e82a611ed7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7cebc53ec7748e5b6abb9a2f6cc633f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37bc8ae3b20c436c8d92d84b5a9987e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2504ea5c58d44b4b806119c58481846c",
              "IPY_MODEL_bc21b4e3525b43cf935e38d58d35f294"
            ],
            "layout": "IPY_MODEL_85ae35621f5944c799dcf85590212e0b"
          }
        },
        "2504ea5c58d44b4b806119c58481846c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb0a8ebce8504584a9d3c0464a8289bb",
            "placeholder": "​",
            "style": "IPY_MODEL_48f55f2809f144168df64d1a016dd54d",
            "value": "0.726 MB of 0.726 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "bc21b4e3525b43cf935e38d58d35f294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_020c03729142430c89f58ea60ab40b88",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2476c2cc652d4c8aaeea1d6cd97afccb",
            "value": 1
          }
        },
        "85ae35621f5944c799dcf85590212e0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb0a8ebce8504584a9d3c0464a8289bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48f55f2809f144168df64d1a016dd54d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "020c03729142430c89f58ea60ab40b88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2476c2cc652d4c8aaeea1d6cd97afccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}