{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3 -part-B-with attn_and_Question6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshaygrao77/DeepLearning-Assignment3/blob/main/Assignment_3_part_B_with_attn_and_Question6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2vkvje2XA3r"
      },
      "source": [
        "# CS6910 Assignment 3 -part-B-with attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0qCvgqBxzLo",
        "outputId": "8442d833-2ab8-412e-8be9-970629f68eae"
      },
      "source": [
        "!pip install wandb\n",
        "!pip install wordcloud\n",
        "!pip install colour"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.16)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.11)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from wordcloud) (1.21.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud) (7.1.2)\n",
            "Requirement already satisfied: colour in /usr/local/lib/python3.7/dist-packages (0.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VffbVoa3r-Lq",
        "outputId": "ab6f3f10-dbf1-4063-c640-7a6be15b807c"
      },
      "source": [
        "## Installing font for kannada for matplotlib ##\n",
        "!apt-get install -y fonts-lohit-deva\n",
        "!fc-list : family \n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libnvidia-common-460 nsight-compute-2020.2.0\n",
            "Use 'apt autoremove' to remove them.\n",
            "The following NEW packages will be installed:\n",
            "  fonts-lohit-deva\n",
            "0 upgraded, 1 newly installed, 0 to remove and 42 not upgraded.\n",
            "Need to get 78.2 kB of archives.\n",
            "After this operation, 196 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-lohit-deva all 2.95.4-2 [78.2 kB]\n",
            "Fetched 78.2 kB in 1s (136 kB/s)\n",
            "Selecting previously unselected package fonts-lohit-deva.\n",
            "(Reading database ... 155202 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-lohit-deva_2.95.4-2_all.deb ...\n",
            "Unpacking fonts-lohit-deva (2.95.4-2) ...\n",
            "Setting up fonts-lohit-deva (2.95.4-2) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Liberation Sans Narrow\n",
            "Liberation Mono\n",
            "Liberation Serif\n",
            "Humor Sans\n",
            "Liberation Sans\n",
            "Lohit Devanagari\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6xB0KDuy95e"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import wandb\n",
        "import re, string\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.font_manager import FontProperties\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from collections import Counter\n",
        "from colour import Color\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()\n",
        "config_defaults = {\"embedding_dim\": 64, \n",
        "                       \"enc_dec_layers\": 1,\n",
        "                       \"layer_type\": \"lstm\",\n",
        "                       \"units\": 128,\n",
        "                       \"dropout\": 0,\n",
        "                       \"attention\": False,\n",
        "                       \"beam_width\": 3,\n",
        "                       \"teacher_forcing_ratio\": 1.0\n",
        "                       }\n",
        "wandb.init(config=config_defaults, project='Assignment3-partB_attn', entity='cs21s002-ee21s113-dlassignment-1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "shdPhW00cLKY",
        "outputId": "c4569524-5ea5-4884-ec17-b42860fa265c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmanu_data_analyst\u001b[0m (\u001b[33mcs21s002-ee21s113-dlassignment-1\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220504_142008-r4o6n1kd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/Assignment3-partB_attn/runs/r4o6n1kd\" target=\"_blank\">rogue-carrier-24</a></strong> to <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/Assignment3-partB_attn\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/Assignment3-partB_attn/runs/r4o6n1kd?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f276e72afd0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVMyzAmMzE1a",
        "outputId": "a423c028-1244-4bfe-b44b-df9005623296"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t1O6AgVjPYm"
      },
      "source": [
        "# Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w96THw2KjYFJ"
      },
      "source": [
        "## Download the dataset ##\n",
        "import requests\n",
        "import tarfile\n",
        "\n",
        "def download_data(save_path):\n",
        "\n",
        "    data_url = r\"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\"\n",
        "\n",
        "    r = requests.get(data_url, allow_redirects=True)\n",
        "    tar_path = \"data_assignment3.tar\"\n",
        "\n",
        "    if r.status_code == 200:\n",
        "        with open(tar_path, 'wb') as f:\n",
        "            f.write(r.content)\n",
        "\n",
        "    tar_file = tarfile.open(tar_path)\n",
        "    tar_file.extractall(save_path)\n",
        "    tar_file.close()\n",
        "\n",
        "\n",
        "# downloading and extracting the data to drive \n",
        "# uncomment the line below if downloading data for the 1st time\n",
        "#download_data(\"/content/drive/MyDrive/DakshinaDataset\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlbXxZXZjpEq"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC_zHEatTLsT"
      },
      "source": [
        "# Files with English to Devanagari (Hindi) translation word by word \n",
        "# Punctutations have already been cleaned from this file \n",
        "\n",
        "def get_data_files(language):\n",
        "    \"\"\" Function fo read data \n",
        "    \"\"\"\n",
        "\n",
        "    ## REPLACE THIS PATH UPTO dakshina_dataset_v1.0 with your own dataset path ##\n",
        "    template = \"/content/drive/MyDrive/DakshinaDataset/dakshina_dataset_v1.0/{}/lexicons/{}.translit.sampled.{}.tsv\"\n",
        "\n",
        "    train_tsv = template.format(language, language, \"train\")\n",
        "    val_tsv = template.format(language, language, \"dev\")\n",
        "    test_tsv = template.format(language, language, \"test\")\n",
        "\n",
        "    return train_tsv, val_tsv, test_tsv\n",
        "\n",
        "## Utility functions for preprocessing data ##\n",
        "\n",
        "def add_start_end_tokens(df, cols, sos=\"\\t\", eos=\"\\n\"):\n",
        "    \"\"\" Adds EOS and SOS tokens to data \n",
        "    \"\"\"\n",
        "    def add_tokens(s):  \n",
        "        # \\t = starting token\n",
        "        # \\n = ending token\n",
        "        return sos + str(s) + eos\n",
        "\n",
        "    for col in cols:\n",
        "        df[col] = df[col].apply(add_tokens) \n",
        "    \n",
        "def tokenize(lang, tokenizer=None):\n",
        "    \"\"\" Uses tf.keras tokenizer to tokenize the data/words into characters\n",
        "    \"\"\"\n",
        "\n",
        "    if tokenizer is None:\n",
        "        tokenizer = Tokenizer(char_level=True)\n",
        "        tokenizer.fit_on_texts(lang)\n",
        "\n",
        "        lang_tensor = tokenizer.texts_to_sequences(lang)\n",
        "        lang_tensor = tf.keras.preprocessing.sequence.pad_sequences(lang_tensor,\n",
        "                                                            padding='post')\n",
        "\n",
        "    else: \n",
        "        lang_tensor = tokenizer.texts_to_sequences(lang)\n",
        "        lang_tensor = tf.keras.preprocessing.sequence.pad_sequences(lang_tensor,\n",
        "                                                            padding='post')\n",
        "\n",
        "    return lang_tensor, tokenizer\n",
        "\n",
        "def preprocess_data(fpath, input_lang_tokenizer=None, targ_lang_tokenizer=None):\n",
        "    \"\"\" Reads, tokenizes and adds SOS/EOS tokens to data based on above functions\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_csv(fpath, sep=\"\\t\", header=None)\n",
        "\n",
        "    # adding start and end tokens to know when to stop predicting \n",
        "    add_start_end_tokens(df, [0,1])\n",
        "    \n",
        "    input_lang_tensor, input_tokenizer = tokenize(df[1].astype(str).tolist(), \n",
        "                                                    tokenizer=input_lang_tokenizer)\n",
        "    \n",
        "    targ_lang_tensor, targ_tokenizer = tokenize(df[0].astype(str).tolist(),\n",
        "                                                    tokenizer=targ_lang_tokenizer) \n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_lang_tensor, targ_lang_tensor))\n",
        "    dataset = dataset.shuffle(len(dataset))\n",
        "    \n",
        "    return dataset, input_tokenizer, targ_tokenizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjGJ1sFE-eon"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLLikr1QN4kE"
      },
      "source": [
        "## Utility functions ##\n",
        "def get_layer(name, units, dropout, return_state=False, return_sequences=False):\n",
        "\n",
        "    if name==\"rnn\":\n",
        "        return layers.SimpleRNN(units=units, dropout=dropout, \n",
        "                                return_state=return_state,\n",
        "                                return_sequences=return_sequences)\n",
        "\n",
        "    if name==\"gru\":\n",
        "        return layers.GRU(units=units, dropout=dropout, \n",
        "                          return_state=return_state,\n",
        "                          return_sequences=return_sequences)\n",
        "\n",
        "    if name==\"lstm\":\n",
        "        return layers.LSTM(units=units, dropout=dropout, \n",
        "                           return_state=return_state,\n",
        "                           return_sequences=return_sequences)\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, enc_state, enc_out):\n",
        "    \n",
        "    enc_state = tf.concat(enc_state, 1)\n",
        "    enc_state = tf.expand_dims(enc_state, 1)\n",
        "\n",
        "    score = self.V(tf.nn.tanh(self.W1(enc_state) + self.W2(enc_out)))\n",
        "\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    context_vector = attention_weights * enc_out\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, layer_type, n_layers, units, encoder_vocab_size, embedding_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layer_type = layer_type\n",
        "        self.n_layers = n_layers\n",
        "        self.units = units\n",
        "        self.dropout = dropout\n",
        "        self.embedding = tf.keras.layers.Embedding(encoder_vocab_size, embedding_dim)\n",
        "        self.create_rnn_layers()\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        x = self.rnn_layers[0](x, initial_state=hidden)\n",
        "\n",
        "        for layer in self.rnn_layers[1:]:\n",
        "            x = layer(x)\n",
        "\n",
        "        output, state = x[0], x[1:]\n",
        "\n",
        "        return output, state\n",
        "    \n",
        "    def create_rnn_layers(self):\n",
        "        self.rnn_layers = []\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "            self.rnn_layers.append(get_layer(self.layer_type, self.units, self.dropout,\n",
        "                                                return_sequences=True,\n",
        "                                                return_state=True))\n",
        "\n",
        "\n",
        "    def initialize_hidden_state(self, batch_size):\n",
        "\n",
        "        if self.layer_type != \"lstm\":\n",
        "            return [tf.zeros((batch_size, self.units))]\n",
        "        else:\n",
        "            return [tf.zeros((batch_size, self.units))]*2\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, layer_type, n_layers, units, decoder_vocab_size, embedding_dim, dropout, attention=False):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.layer_type = layer_type\n",
        "        self.n_layers = n_layers\n",
        "        self.units = units\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "        self.embedding_layer = layers.Embedding(input_dim=decoder_vocab_size, \n",
        "                                                output_dim=embedding_dim)\n",
        "        \n",
        "        self.dense = layers.Dense(decoder_vocab_size, activation=\"softmax\")\n",
        "        self.flatten = layers.Flatten()\n",
        "        if self.attention:\n",
        "            self.attention_layer = BahdanauAttention(self.units)\n",
        "        self.create_rnn_layers()\n",
        "\n",
        "    def call(self, x, hidden, enc_out=None):\n",
        "        \n",
        "        x = self.embedding_layer(x)\n",
        "\n",
        "        if self.attention:\n",
        "            context_vector, attention_weights = self.attention_layer(hidden, enc_out)\n",
        "            x = tf.concat([tf.expand_dims(context_vector, 1), x], -1)\n",
        "        else:\n",
        "            attention_weights = None\n",
        "\n",
        "        x = self.rnn_layers[0](x, initial_state=hidden)\n",
        "\n",
        "        for layer in self.rnn_layers[1:]:\n",
        "            x = layer(x)\n",
        "\n",
        "        output, state = x[0], x[1:]\n",
        "\n",
        "        output = self.dense(self.flatten(output))\n",
        "        \n",
        "        return output, state, attention_weights\n",
        "\n",
        "    def create_rnn_layers(self):\n",
        "        self.rnn_layers = []    \n",
        "\n",
        "        for i in range(self.n_layers - 1):\n",
        "            self.rnn_layers.append(get_layer(self.layer_type, self.units, self.dropout,\n",
        "                                                return_sequences=True,\n",
        "                                                return_state=True))\n",
        "        \n",
        "        self.rnn_layers.append(get_layer(self.layer_type, self.units, self.dropout,\n",
        "                                            return_sequences=False,\n",
        "                                            return_state=True))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lks31eDH0KKc"
      },
      "source": [
        "class BeamSearch():\n",
        "    def __init__(self, model, k):\n",
        "        self.k = k \n",
        "        self.model = model\n",
        "        self.acc = tf.keras.metrics.Accuracy()\n",
        "\n",
        "    def sample_beam_search(self, probs):\n",
        "\n",
        "        m, n = probs.shape\n",
        "        output_sequences = [[[], 0.0]]\n",
        "\n",
        "        for row in probs:\n",
        "            beams = []\n",
        "\n",
        "            for tup in output_sequences:\n",
        "                seq, score = tup\n",
        "                for j in range(n):\n",
        "                    new_beam = [seq + [j], score - tf.math.log(row[j])]\n",
        "                    beams.append(new_beam)\n",
        "\n",
        "            output_sequences = sorted(beams, key=lambda x: x[1])[:self.k]\n",
        "\n",
        "        tensors, scores = list(zip(*output_sequences))\n",
        "        tensors = list(map(lambda x: tf.expand_dims(tf.constant(x),0), tensors))\n",
        "\n",
        "        return tf.concat(tensors, 0), scores\n",
        "\n",
        "    def beam_accuracy(self, input, target):\n",
        "        accs = []\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.acc.reset_states()\n",
        "            self.acc.update_state(target, input[i, :])  \n",
        "            accs.append(self.acc.result())\n",
        "\n",
        "        return max(accs)\n",
        "    \n",
        "    def step(self, input, target, enc_state):\n",
        "\n",
        "        batch_acc = 0\n",
        "        sequences = []\n",
        "\n",
        "        enc_out, enc_state = self.model.encoder(input, enc_state)\n",
        "\n",
        "        dec_state = enc_state\n",
        "        dec_input = tf.expand_dims([self.model.targ_tokenizer.word_index[\"\\t\"]]*self.model.batch_size ,1)\n",
        "\n",
        "        for t in range(1, target.shape[1]):\n",
        "\n",
        "            preds, dec_state, _ = self.model.decoder(dec_input, dec_state, enc_out)\n",
        "\n",
        "            sequences.append(preds)\n",
        "            preds = tf.argmax(preds, 1)\n",
        "            dec_input = tf.expand_dims(preds, 1)\n",
        "\n",
        "        sequences = tf.concat(list(map(lambda x: tf.expand_dims(x, 1), sequences)), axis=1)\n",
        "\n",
        "        for i in range(target.shape[0]):\n",
        "\n",
        "            possibilities, scores = self.sample_beam_search(sequences[i, :, :])\n",
        "            batch_acc += self.beam_accuracy(possibilities, target[i, 1:])\n",
        "\n",
        "        batch_acc = batch_acc / target.shape[0]\n",
        "\n",
        "        return 0, batch_acc\n",
        "\n",
        "    def evaluate(self, test_dataset, batch_size=None, upto=5, use_wandb=True):\n",
        "        \n",
        "        if batch_size is not None:\n",
        "            self.model.batch_size = batch_size\n",
        "            test_dataset = test_dataset.batch(batch_size)\n",
        "        else:\n",
        "            self.model.batch_size = 1\n",
        "\n",
        "        test_acc = 0\n",
        "        enc_state = self.model.encoder.initialize_hidden_state(self.model.batch_size)\n",
        "\n",
        "        for batch, (input, target) in enumerate(test_dataset.take(upto)):\n",
        "           \n",
        "           _, acc = self.step(input, target, enc_state)\n",
        "           test_acc += acc\n",
        "\n",
        "        if use_wandb:\n",
        "            wandb.log({\"test acc (beam search)\": test_acc / upto})\n",
        "\n",
        "        print(f\"Test Accuracy on {upto*batch_size} samples: {test_acc / upto:.4f}\\n\")\n",
        "\n",
        "    def translate(self, word):\n",
        "\n",
        "        word = \"\\t\" + word + \"\\n\"\n",
        "        sequences = []\n",
        "        result = []\n",
        "\n",
        "        inputs = self.model.input_tokenizer.texts_to_sequences([word])\n",
        "        inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                               maxlen=self.model.max_input_len,\n",
        "                                                               padding=\"post\")\n",
        "\n",
        "\n",
        "        enc_state = self.model.encoder.initialize_hidden_state(1)\n",
        "        enc_out, enc_state = self.model.encoder(inputs, enc_state)\n",
        "\n",
        "        dec_state = enc_state\n",
        "        dec_input = tf.expand_dims([self.model.targ_tokenizer.word_index[\"\\t\"]]*1, 1)\n",
        "\n",
        "        for t in range(1, self.model.max_target_len):\n",
        "\n",
        "            preds, dec_state, _ = self.model.decoder(dec_input, dec_state, enc_out)\n",
        "\n",
        "            sequences.append(preds)\n",
        "            preds = tf.argmax(preds, 1)\n",
        "            dec_input = tf.expand_dims(preds, 1)\n",
        "\n",
        "        sequences = tf.concat(list(map(lambda x: tf.expand_dims(x, 1), sequences)), axis=1)\n",
        "\n",
        "        possibilities, scores = self.sample_beam_search(tf.squeeze(sequences, 0))\n",
        "        output_words = self.model.targ_tokenizer.sequences_to_texts(possibilities.numpy())\n",
        "        \n",
        "        def post_process(word):\n",
        "            word = word.split(\" \")[:-1]\n",
        "            return \"\".join([x for x in word])\n",
        "\n",
        "        output_words = list(map(post_process, output_words))\n",
        "\n",
        "        return output_words, scores"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwR1miyeRrIG"
      },
      "source": [
        "class Seq2SeqModel():\n",
        "    def __init__(self, embedding_dim, encoder_layers, decoder_layers, layer_type, units, dropout, attention=False):\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.encoder_layers = encoder_layers\n",
        "        self.decoder_layers = decoder_layers\n",
        "        self.layer_type = layer_type\n",
        "        self.units = units\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "        self.stats = []\n",
        "        self.batch_size = 128\n",
        "        self.use_beam_search = False\n",
        "\n",
        "    def build(self, loss, optimizer, metric):\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "        self.metric = metric\n",
        "\n",
        "    def set_vocabulary(self, input_tokenizer, targ_tokenizer):\n",
        "        self.input_tokenizer = input_tokenizer\n",
        "        self.targ_tokenizer = targ_tokenizer\n",
        "        self.create_model()\n",
        "    \n",
        "    def create_model(self):\n",
        "\n",
        "        encoder_vocab_size = len(self.input_tokenizer.word_index) + 1\n",
        "        decoder_vocab_size = len(self.targ_tokenizer.word_index) + 1\n",
        "\n",
        "        self.encoder = Encoder(self.layer_type, self.encoder_layers, self.units, encoder_vocab_size,\n",
        "                               self.embedding_dim, self.dropout)\n",
        "\n",
        "        self.decoder = Decoder(self.layer_type, self.decoder_layers, self.units, decoder_vocab_size,\n",
        "                               self.embedding_dim,  self.dropout, self.attention)\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, input, target, enc_state):\n",
        "\n",
        "        loss = 0 \n",
        "\n",
        "        with tf.GradientTape() as tape: \n",
        "\n",
        "            enc_out, enc_state = self.encoder(input, enc_state)\n",
        "\n",
        "            dec_state = enc_state\n",
        "            dec_input = tf.expand_dims([self.targ_tokenizer.word_index[\"\\t\"]]*self.batch_size ,1)\n",
        "\n",
        "            ## We use Teacher forcing to train the network\n",
        "            ## Each target at timestep t is passed as input for timestep t + 1\n",
        "\n",
        "            if random.random() < self.teacher_forcing_ratio:\n",
        "\n",
        "                for t in range(1, target.shape[1]):\n",
        "\n",
        "                    preds, dec_state, _ = self.decoder(dec_input, dec_state, enc_out)\n",
        "                    loss += self.loss(target[:,t], preds)\n",
        "                    self.metric.update_state(target[:,t], preds)\n",
        "                    \n",
        "                    dec_input = tf.expand_dims(target[:,t], 1)\n",
        "            \n",
        "            else:\n",
        "\n",
        "                for t in range(1, target.shape[1]):\n",
        "\n",
        "                    preds, dec_state, _ = self.decoder(dec_input, dec_state, enc_out)\n",
        "                    loss += self.loss(target[:,t], preds)\n",
        "                    self.metric.update_state(target[:,t], preds)\n",
        "\n",
        "                    preds = tf.argmax(preds, 1)\n",
        "                    dec_input = tf.expand_dims(preds, 1)\n",
        "\n",
        "\n",
        "            batch_loss = loss / target.shape[1]\n",
        "\n",
        "            variables = self.encoder.variables + self.decoder.variables\n",
        "            gradients = tape.gradient(loss, variables)\n",
        "\n",
        "            self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "        return batch_loss, self.metric.result()\n",
        "\n",
        "    @tf.function\n",
        "    def validation_step(self, input, target, enc_state):\n",
        "\n",
        "        loss = 0\n",
        "        \n",
        "        enc_out, enc_state = self.encoder(input, enc_state)\n",
        "\n",
        "        dec_state = enc_state\n",
        "        dec_input = tf.expand_dims([self.targ_tokenizer.word_index[\"\\t\"]]*self.batch_size ,1)\n",
        "\n",
        "        for t in range(1, target.shape[1]):\n",
        "\n",
        "            preds, dec_state, _ = self.decoder(dec_input, dec_state, enc_out)\n",
        "            loss += self.loss(target[:,t], preds)\n",
        "            self.metric.update_state(target[:,t], preds)\n",
        "\n",
        "            preds = tf.argmax(preds, 1)\n",
        "            dec_input = tf.expand_dims(preds, 1)\n",
        "\n",
        "        batch_loss = loss / target.shape[1]\n",
        "        \n",
        "        return batch_loss, self.metric.result()\n",
        "\n",
        "\n",
        "    def fit(self, dataset, val_dataset, batch_size=128, epochs=10, use_wandb=False, teacher_forcing_ratio=1.0):\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
        "\n",
        "        steps_per_epoch = len(dataset) // self.batch_size\n",
        "        steps_per_epoch_val = len(val_dataset) // self.batch_size\n",
        "        \n",
        "        dataset = dataset.batch(self.batch_size, drop_remainder=True)\n",
        "        val_dataset = val_dataset.batch(self.batch_size, drop_remainder=True)\n",
        "\n",
        "        # useful when we need to translate the sentence\n",
        "        sample_inp, sample_targ = next(iter(dataset))\n",
        "        self.max_target_len = sample_targ.shape[1]\n",
        "        self.max_input_len = sample_inp.shape[1]\n",
        "\n",
        "        template = \"\\nTrain Loss: {0:.4f} Train Accuracy: {1:.4f} Validation Loss: {2:.4f} Validation Accuracy: {3:.4f}\"\n",
        "\n",
        "        print(\"-\"*100)\n",
        "        for epoch in range(1, epochs+1):\n",
        "            print(f\"EPOCH {epoch}\\n\")\n",
        "\n",
        "            ## Training loop ##\n",
        "            total_loss = 0\n",
        "            total_acc = 0\n",
        "            self.metric.reset_states()\n",
        "\n",
        "            starting_time = time.time()\n",
        "            enc_state = self.encoder.initialize_hidden_state(self.batch_size)\n",
        "\n",
        "            print(\"Training ...\\n\")\n",
        "            for batch, (input, target) in enumerate(dataset.take(steps_per_epoch)):\n",
        "                batch_loss, acc = self.train_step(input, target, enc_state)\n",
        "                total_loss += batch_loss\n",
        "                total_acc += acc\n",
        "\n",
        "\n",
        "                if batch==0 or ((batch + 1) % 100 == 0):\n",
        "                    print(f\"Batch {batch+1} Loss {batch_loss:.4f}\")\n",
        "\n",
        "            avg_acc = total_acc / steps_per_epoch\n",
        "            avg_loss = total_loss / steps_per_epoch\n",
        "\n",
        "            # Validation loop ##\n",
        "            total_val_loss = 0\n",
        "            total_val_acc = 0\n",
        "            self.metric.reset_states()\n",
        "\n",
        "            enc_state = self.encoder.initialize_hidden_state(self.batch_size)\n",
        "\n",
        "            print(\"\\nValidating ...\")\n",
        "            for batch, (input, target) in enumerate(val_dataset.take(steps_per_epoch_val)):\n",
        "                batch_loss, acc = self.validation_step(input, target, enc_state)\n",
        "                total_val_loss += batch_loss\n",
        "                total_val_acc += acc\n",
        "\n",
        "            avg_val_acc = total_val_acc / steps_per_epoch_val\n",
        "            avg_val_loss = total_val_loss / steps_per_epoch_val\n",
        "\n",
        "            print(template.format(avg_loss, avg_acc*100, avg_val_loss, avg_val_acc*100))\n",
        "            \n",
        "            time_taken = time.time() - starting_time\n",
        "            self.stats.append({\"epoch\": epoch,\n",
        "                            \"train loss\": avg_loss,\n",
        "                            \"val loss\": avg_val_loss,\n",
        "                            \"train acc\": avg_acc*100,\n",
        "                            \"val acc\": avg_val_acc*100,\n",
        "                            \"training time\": time_taken})\n",
        "            \n",
        "            if use_wandb:\n",
        "                wandb.log(self.stats[-1])\n",
        "            \n",
        "            print(f\"\\nTime taken for the epoch {time_taken:.4f}\")\n",
        "            print(\"-\"*100)\n",
        "        \n",
        "        print(\"\\nModel trained successfully !!\")\n",
        "        \n",
        "    def evaluate(self, test_dataset, batch_size=None):\n",
        "\n",
        "        if batch_size is not None:\n",
        "            self.batch_size = batch_size\n",
        "\n",
        "        steps_per_epoch_test = len(test_dataset) // batch_size\n",
        "        test_dataset = test_dataset.batch(batch_size, drop_remainder=True)\n",
        "        \n",
        "        total_test_loss = 0\n",
        "        total_test_acc = 0\n",
        "        self.metric.reset_states()\n",
        "\n",
        "        enc_state = self.encoder.initialize_hidden_state(self.batch_size)\n",
        "\n",
        "        print(\"\\nRunning test dataset through the model...\\n\")\n",
        "        for batch, (input, target) in enumerate(test_dataset.take(steps_per_epoch_test)):\n",
        "            batch_loss, acc = self.validation_step(input, target, enc_state)\n",
        "            total_test_loss += batch_loss\n",
        "            total_test_acc += acc\n",
        "\n",
        "        avg_test_acc = total_test_acc / steps_per_epoch_test\n",
        "        avg_test_loss = total_test_loss / steps_per_epoch_test\n",
        "    \n",
        "        print(f\"Test Loss: {avg_test_loss:.4f} Test Accuracy: {avg_test_acc:.4f}\")\n",
        "        wandb.log({\"Test_Loss\": avg_test_loss})\n",
        "        wandb.log({\"Test Accuracy\": avg_test_loss})\n",
        "\n",
        "        return avg_test_loss, avg_test_acc\n",
        "\n",
        "\n",
        "    def translate(self, word, get_heatmap=True):\n",
        "\n",
        "        word = \"\\t\" + word + \"\\n\"\n",
        "\n",
        "        inputs = self.input_tokenizer.texts_to_sequences([word])\n",
        "        inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                               maxlen=self.max_input_len,\n",
        "                                                               padding=\"post\")\n",
        "\n",
        "        result = \"\"\n",
        "        att_wts = []\n",
        "\n",
        "        enc_state = self.encoder.initialize_hidden_state(1)\n",
        "        enc_out, enc_state = self.encoder(inputs, enc_state)\n",
        "\n",
        "        dec_state = enc_state\n",
        "        dec_input = tf.expand_dims([self.targ_tokenizer.word_index[\"\\t\"]]*1, 1)\n",
        "\n",
        "        for t in range(1, self.max_target_len):\n",
        "\n",
        "            preds, dec_state, attention_weights = self.decoder(dec_input, dec_state, enc_out)\n",
        "            \n",
        "            if get_heatmap:\n",
        "                att_wts.append(attention_weights)\n",
        "            \n",
        "            preds = tf.argmax(preds, 1)\n",
        "            next_char = self.targ_tokenizer.index_word[preds.numpy().item()]\n",
        "            result += next_char\n",
        "\n",
        "            dec_input = tf.expand_dims(preds, 1)\n",
        "\n",
        "            if next_char == \"\\n\":\n",
        "                return result[:-1], att_wts[:-1]\n",
        "\n",
        "        return result[:-1], att_wts[:-1]\n",
        "\n",
        "    def plot_attention_heatmap(self, word):\n",
        "\n",
        "        translated_word, attn_wts = self.translate(word, get_heatmap=True)\n",
        "        attn_heatmap = tf.squeeze(tf.concat(attn_wts, 0), -1).numpy()\n",
        "\n",
        "        input_word_len = len(word)\n",
        "        output_word_len = len(translated_word)\n",
        "        list(word).sort()\n",
        "        list(translated_word).sort()\n",
        "        wandb.log({'heatmap_with_text': wandb.plots.HeatMap(list(word), list(translated_word), attn_heatmap[:, :input_word_len], show_text=False)})"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSZFHEmw9KRw"
      },
      "source": [
        "# Visualizing Model Outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPPCBrHukCMZ"
      },
      "source": [
        "def get_colors(inputs, targets, preds):\n",
        "\n",
        "    n = len(targets)\n",
        "    smoother = SmoothingFunction().method2\n",
        "    def get_scores(target, output, smoother):\n",
        "        return sentence_bleu(list(list(target)), list(output), smoothing_function=smoother)\n",
        "\n",
        "    red = Color(\"red\")\n",
        "    colors = list(red.range_to(Color(\"violet\"),n))\n",
        "    colors = list(map(lambda c: c.hex, colors))\n",
        "\n",
        "    scores = []\n",
        "    for i in range(n):\n",
        "        scores.append(get_scores(targets[i], preds[i], smoother))\n",
        "\n",
        "    d = dict(zip(sorted(scores), list(range(n))))\n",
        "    ordered_colors = list(map(lambda x: colors[d[x]], scores))\n",
        "    \n",
        "    input_colors = dict(zip(inputs, ordered_colors))\n",
        "    target_colors = dict(zip(targets, ordered_colors))\n",
        "    pred_colors = dict(zip(preds, ordered_colors))\n",
        "\n",
        "    return input_colors, target_colors, pred_colors\n",
        "\n",
        "\n",
        "class Colorizer():\n",
        "    def __init__(self, word_to_color, default_color):\n",
        "       \n",
        "        self.word_to_color = word_to_color\n",
        "        self.default_color = default_color\n",
        "\n",
        "    def __call__(self, word, **kwargs):\n",
        "        return self.word_to_color.get(word, self.default_color)\n",
        "\n",
        "def randomly_evaluate(model, test_file=get_data_files(\"kn\")[2], n=10):\n",
        "\n",
        "    df = pd.read_csv(test_file, sep=\"\\t\", header=None)\n",
        "    df = df.sample(n=n).reset_index(drop=True)\n",
        "\n",
        "    print(f\"Randomly evaluating the model on {n} words\\n\")\n",
        "\n",
        "    for i in range(n):\n",
        "        word = str(df[1][i])\n",
        "\n",
        "        print(f\"Input word: {word}\")\n",
        "        print(f\"Actual translation: {str(df[0][i])}\")\n",
        "        print(f\"Model translation: {model.translate(word)[0]}\\n\")\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "def test_on_dataset(language, embedding_dim, encoder_layers, decoder_layers, layer_type, units, dropout, attention, teacher_forcing_ratio=1.0, save_outputs=None):\n",
        "    \n",
        "    TRAIN_TSV, VAL_TSV, TEST_TSV = get_data_files(language)\n",
        "\n",
        "    model = Seq2SeqModel(embedding_dim, \n",
        "                         encoder_layers, \n",
        "                         decoder_layers, \n",
        "                         layer_type, \n",
        "                         units,\n",
        "                         dropout,\n",
        "                         attention)\n",
        "\n",
        "    dataset, input_tokenizer, targ_tokenizer = preprocess_data(TRAIN_TSV)\n",
        "    val_dataset, _, _ = preprocess_data(VAL_TSV, input_tokenizer, targ_tokenizer)\n",
        "\n",
        "    model.set_vocabulary(input_tokenizer, targ_tokenizer)\n",
        "    model.build(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metric = tf.keras.metrics.SparseCategoricalAccuracy())\n",
        "    \n",
        "    model.fit(dataset, val_dataset, epochs=30, use_wandb=True, teacher_forcing_ratio=teacher_forcing_ratio)\n",
        "\n",
        "    ## Character level accuracy ##\n",
        "    test_dataset, _, _ = preprocess_data(TEST_TSV, model.input_tokenizer, model.targ_tokenizer)\n",
        "    test_loss, test_acc = model.evaluate(test_dataset, batch_size=100)\n",
        "\n",
        "    ##  Word level accuracy ##\n",
        "    test_tsv = pd.read_csv(TEST_TSV, sep=\"\\t\", header=None)\n",
        "    inputs = test_tsv[1].astype(str).tolist()\n",
        "    targets = test_tsv[0].astype(str).tolist()\n",
        "    \n",
        "    outputs = []\n",
        "\n",
        "    for word in inputs:\n",
        "        outputs.append(model.translate(word)[0])\n",
        "\n",
        "    def word_level_acc(outputs, targets):\n",
        "        return np.sum(np.asarray(outputs) == np.array(targets)) / len(outputs)\n",
        "\n",
        "    print(f\"Word level accuracy: {word_level_acc(outputs, targets)}\")\n",
        "\n",
        "    if save_outputs is not None:\n",
        "        df = pd.DataFrame()\n",
        "        df[\"inputs\"] = inputs\n",
        "        df[\"targets\"] = targets\n",
        "        df[\"outputs\"] = outputs\n",
        "        df.to_csv(save_outputs)\n",
        "\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvVSaWfGYvvn"
      },
      "source": [
        "# Visualizing Model Connectivity (Q6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeWT7Yno7LXd"
      },
      "source": [
        "# Tools for getting model connectivity between input and output characters\n",
        "def get_lstm_output(decoder, x, hidden, enc_out=None):\n",
        "    \n",
        "    x = decoder.embedding_layer(x)\n",
        "\n",
        "    if decoder.attention:\n",
        "        context_vector, attention_weights = decoder.attention_layer(hidden, enc_out)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], -1)\n",
        "    else:\n",
        "        attention_weights = None\n",
        "\n",
        "    x = decoder.rnn_layers[0](x, initial_state=hidden)\n",
        "\n",
        "    for layer in decoder.rnn_layers[1:]:\n",
        "        x = layer(x)\n",
        "\n",
        "    output, state = x[0], x[1:]\n",
        "\n",
        "    #output = decoder.dense(decoder.flatten(output))\n",
        "    \n",
        "    return output, state, attention_weights\n",
        "\n",
        "def get_output_from_embedding(encoder, x, hidden):\n",
        "\n",
        "    x = encoder.rnn_layers[0](x, initial_state=hidden)\n",
        "\n",
        "    for layer in encoder.rnn_layers[1:]:\n",
        "        x = layer(x)\n",
        "\n",
        "    output, state = x[0], x[1:]\n",
        "\n",
        "    return output, state\n",
        "\n",
        "\n",
        "def get_connectivity(model, word):\n",
        "\n",
        "    word = \"\\t\" + word + \"\\n\"\n",
        "\n",
        "    inputs = model.input_tokenizer.texts_to_sequences([word])\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                            maxlen=model.max_input_len,\n",
        "                                                            padding=\"post\")\n",
        "\n",
        "    result = \"\"\n",
        "\n",
        "    gradient_list = []\n",
        "\n",
        "    enc_state = model.encoder.initialize_hidden_state(1)\n",
        "    embedded_in = model.encoder.embedding(inputs)\n",
        "\n",
        "\n",
        "    with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n",
        "        tape.watch(embedded_in)\n",
        "\n",
        "        enc_out, enc_state = get_output_from_embedding(model.encoder, embedded_in, enc_state)\n",
        "\n",
        "        dec_state = enc_state\n",
        "        dec_input = tf.expand_dims([model.targ_tokenizer.word_index[\"\\t\"]]*1, 1)\n",
        "\n",
        "        for t in range(1, model.max_target_len):\n",
        "\n",
        "            lstm_out, dec_state, _ = get_lstm_output(model.decoder, dec_input, dec_state, enc_out)\n",
        "\n",
        "            preds = model.decoder.dense(model.decoder.flatten(lstm_out))\n",
        "            gradient_list.append(tape.gradient(lstm_out, embedded_in)[0])\n",
        "            \n",
        "            preds = tf.argmax(preds, 1)\n",
        "            next_char = model.targ_tokenizer.index_word[preds.numpy().item()]\n",
        "            result += next_char\n",
        "\n",
        "            dec_input = tf.expand_dims(preds, 1)\n",
        "\n",
        "            if next_char == \"\\n\":\n",
        "                return result[:-1], gradient_list[:-1]\n",
        "\n",
        "        return result[:-1], gradient_list[:-1]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22kfP4uvp63B"
      },
      "source": [
        "# Imports for visualising the model connectivity\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from IPython.display import HTML as html_print\n",
        "from IPython.display import display\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# get html element\n",
        "def cstr(s, color='black'):\n",
        "    if s == ' ':\n",
        "      return \"<text style=color:#000;padding-left:10px;background-color:{}> </text>\".format(color, s)\n",
        "    else:\n",
        "      return \"<text style=color:#000;background-color:{}>{} </text>\".format(color, s)\n",
        "\t\n",
        "# print html\n",
        "def print_color(t):\n",
        "\t  display(html_print(''.join([cstr(ti, color=ci) for ti,ci in t])))\n",
        "\n",
        "# get appropriate color for value\n",
        "def get_clr(value):\n",
        "    colors = ['#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8'\n",
        "      '#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8',\n",
        "      '#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f',\n",
        "      '#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e']\n",
        "    value = int(value * 19)\n",
        "    if value == 19:\n",
        "        value -= 1\n",
        "    return colors[value]\n",
        "\n",
        "# sigmoid function\n",
        "def sigmoid(x):\n",
        "    z = 1/(1 + np.exp(-x)) \n",
        "    return z\n",
        "\n",
        "def softmax(x):\n",
        "    v = np.exp(x)\n",
        "    v = v / np.sum(v)\n",
        "    return v\n",
        "\n",
        "def get_gradient_norms(grad_list, word, activation=\"sigmoid\"):\n",
        "    grad_norms = []\n",
        "    for grad_tensor in grad_list:\n",
        "        grad_mags = tf.norm(grad_tensor, axis=1)\n",
        "        grad_mags = grad_mags[:len(word)]\n",
        "        if activation == \"softmax\":\n",
        "            grad_mags_scaled = softmax(grad_mags)\n",
        "        elif activation == \"scaler\":\n",
        "            scaler = MinMaxScaler()\n",
        "            grad_mags = tf.reshape(grad_mags, (-1,1))\n",
        "            grad_mags_scaled = scaler.fit_transform(grad_mags)\n",
        "        else:\n",
        "            grad_mags_scaled = sigmoid(grad_mags)\n",
        "        grad_norms.append(grad_mags_scaled)\n",
        "    return grad_norms\n",
        "\n",
        "def visualize(grad_norms, word, translated_word):\n",
        "    print(\"Original Word:\", word)\n",
        "    print(\"Transliterated Word:\", translated_word)\n",
        "    for i in range(len(translated_word)):\n",
        "        print(\"Connectivity Visualization for\", translated_word[i],\":\")\n",
        "        text_colours = []\n",
        "        for j in range(len(grad_norms[i])):\n",
        "            text = (word[j], get_clr(grad_norms[i][j]))\n",
        "            text_colours.append(text)\n",
        "        print_color(text_colours)\n",
        "        data_table = wandb.Table(data=text_colours, columns=[\"s_ind\", \"t_ind\"])\n",
        "        fields = {\n",
        "                     \"s_index\": \"s_ind\",\n",
        "                     \"t_index\": \"t_ind\",\n",
        "\n",
        "                }\n",
        "        wandb.log({\"heatmap\": wandb.plot_table(\n",
        "                       vega_spec_name=\"spec-visualization\",\n",
        "                       data_table=data_table,\n",
        "                       fields=fields\n",
        "                       )\n",
        "                   })\n",
        "\n",
        "def visualise_connectivity(model, word, activation=\"sigmoid\"):\n",
        "    translated_word, grad_list = get_connectivity(model, word)\n",
        "    grad_norms = get_gradient_norms(grad_list, word, activation)\n",
        "    visualize(grad_norms, word, translated_word)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzkdsWRlZzTf",
        "outputId": "11d132e8-e6ca-4520-9015-0deabec9400a"
      },
      "source": [
        "model = test_on_dataset(language=\"kn\",\n",
        "                        embedding_dim=64,\n",
        "                        encoder_layers=2,\n",
        "                        decoder_layers=2,\n",
        "                        layer_type=\"lstm\",\n",
        "                        units=256,\n",
        "                        dropout=0.2,\n",
        "                        attention=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 1\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 3.9982\n",
            "Batch 100 Loss 1.2038\n",
            "Batch 200 Loss 1.1164\n",
            "Batch 300 Loss 0.9150\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 1.1210 Train Accuracy: 65.0869 Validation Loss: 2.4732 Validation Accuracy: 53.0391\n",
            "\n",
            "Time taken for the epoch 260.9536\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 2\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.8850\n",
            "Batch 100 Loss 0.8085\n",
            "Batch 200 Loss 0.7513\n",
            "Batch 300 Loss 0.7172\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.7751 Train Accuracy: 75.5699 Validation Loss: 2.1033 Validation Accuracy: 60.6673\n",
            "\n",
            "Time taken for the epoch 131.6696\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 3\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.6604\n",
            "Batch 100 Loss 0.6400\n",
            "Batch 200 Loss 0.5771\n",
            "Batch 300 Loss 0.4861\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.5566 Train Accuracy: 81.4947 Validation Loss: 2.0017 Validation Accuracy: 66.5197\n",
            "\n",
            "Time taken for the epoch 121.0917\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 4\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.4406\n",
            "Batch 100 Loss 0.3936\n",
            "Batch 200 Loss 0.3263\n",
            "Batch 300 Loss 0.2392\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.3258 Train Accuracy: 88.3615 Validation Loss: 1.5919 Validation Accuracy: 75.9266\n",
            "\n",
            "Time taken for the epoch 130.5211\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 5\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.2213\n",
            "Batch 100 Loss 0.1980\n",
            "Batch 200 Loss 0.1783\n",
            "Batch 300 Loss 0.1722\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1804 Train Accuracy: 93.7258 Validation Loss: 1.2258 Validation Accuracy: 82.7495\n",
            "\n",
            "Time taken for the epoch 123.4195\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 6\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.1310\n",
            "Batch 100 Loss 0.1240\n",
            "Batch 200 Loss 0.1071\n",
            "Batch 300 Loss 0.0902\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.1131 Train Accuracy: 96.2532 Validation Loss: 1.0288 Validation Accuracy: 86.5541\n",
            "\n",
            "Time taken for the epoch 113.3674\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 7\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0853\n",
            "Batch 100 Loss 0.0849\n",
            "Batch 200 Loss 0.0935\n",
            "Batch 300 Loss 0.0674\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0813 Train Accuracy: 97.3929 Validation Loss: 0.9603 Validation Accuracy: 88.5046\n",
            "\n",
            "Time taken for the epoch 119.1722\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 8\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0681\n",
            "Batch 100 Loss 0.0636\n",
            "Batch 200 Loss 0.0745\n",
            "Batch 300 Loss 0.0607\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0641 Train Accuracy: 97.9952 Validation Loss: 0.9221 Validation Accuracy: 89.0338\n",
            "\n",
            "Time taken for the epoch 110.6204\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 9\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0418\n",
            "Batch 100 Loss 0.0517\n",
            "Batch 200 Loss 0.0606\n",
            "Batch 300 Loss 0.0615\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0515 Train Accuracy: 98.4028 Validation Loss: 0.9312 Validation Accuracy: 89.3181\n",
            "\n",
            "Time taken for the epoch 116.9555\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 10\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0518\n",
            "Batch 100 Loss 0.0493\n",
            "Batch 200 Loss 0.0462\n",
            "Batch 300 Loss 0.0514\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0431 Train Accuracy: 98.6973 Validation Loss: 0.9190 Validation Accuracy: 90.1469\n",
            "\n",
            "Time taken for the epoch 127.8366\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 11\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0307\n",
            "Batch 100 Loss 0.0330\n",
            "Batch 200 Loss 0.0401\n",
            "Batch 300 Loss 0.0346\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0369 Train Accuracy: 98.9325 Validation Loss: 0.9590 Validation Accuracy: 89.5955\n",
            "\n",
            "Time taken for the epoch 134.7622\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 12\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0339\n",
            "Batch 100 Loss 0.0301\n",
            "Batch 200 Loss 0.0383\n",
            "Batch 300 Loss 0.0318\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0316 Train Accuracy: 99.0241 Validation Loss: 0.9867 Validation Accuracy: 90.0257\n",
            "\n",
            "Time taken for the epoch 137.5726\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 13\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0254\n",
            "Batch 100 Loss 0.0223\n",
            "Batch 200 Loss 0.0218\n",
            "Batch 300 Loss 0.0245\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0266 Train Accuracy: 99.2033 Validation Loss: 0.9761 Validation Accuracy: 89.7637\n",
            "\n",
            "Time taken for the epoch 143.1832\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 14\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0228\n",
            "Batch 100 Loss 0.0162\n",
            "Batch 200 Loss 0.0296\n",
            "Batch 300 Loss 0.0254\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0254 Train Accuracy: 99.1988 Validation Loss: 0.9907 Validation Accuracy: 89.8077\n",
            "\n",
            "Time taken for the epoch 140.9731\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 15\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0222\n",
            "Batch 100 Loss 0.0224\n",
            "Batch 200 Loss 0.0186\n",
            "Batch 300 Loss 0.0261\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0208 Train Accuracy: 99.3701 Validation Loss: 1.0153 Validation Accuracy: 89.5974\n",
            "\n",
            "Time taken for the epoch 140.2264\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 16\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0192\n",
            "Batch 100 Loss 0.0131\n",
            "Batch 200 Loss 0.0180\n",
            "Batch 300 Loss 0.0155\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0189 Train Accuracy: 99.4418 Validation Loss: 1.0020 Validation Accuracy: 90.5961\n",
            "\n",
            "Time taken for the epoch 140.2842\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 17\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0172\n",
            "Batch 100 Loss 0.0121\n",
            "Batch 200 Loss 0.0172\n",
            "Batch 300 Loss 0.0172\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0166 Train Accuracy: 99.5167 Validation Loss: 0.9936 Validation Accuracy: 90.8338\n",
            "\n",
            "Time taken for the epoch 146.5386\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 18\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0143\n",
            "Batch 100 Loss 0.0144\n",
            "Batch 200 Loss 0.0136\n",
            "Batch 300 Loss 0.0164\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0153 Train Accuracy: 99.5376 Validation Loss: 1.0000 Validation Accuracy: 90.6559\n",
            "\n",
            "Time taken for the epoch 118.8472\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 19\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0110\n",
            "Batch 100 Loss 0.0134\n",
            "Batch 200 Loss 0.0177\n",
            "Batch 300 Loss 0.0169\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0151 Train Accuracy: 99.5808 Validation Loss: 1.0915 Validation Accuracy: 89.9785\n",
            "\n",
            "Time taken for the epoch 145.7512\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 20\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0193\n",
            "Batch 100 Loss 0.0142\n",
            "Batch 200 Loss 0.0142\n",
            "Batch 300 Loss 0.0182\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0139 Train Accuracy: 99.5431 Validation Loss: 1.0027 Validation Accuracy: 90.7436\n",
            "\n",
            "Time taken for the epoch 102.7086\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 21\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0127\n",
            "Batch 100 Loss 0.0112\n",
            "Batch 200 Loss 0.0102\n",
            "Batch 300 Loss 0.0098\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0129 Train Accuracy: 99.5883 Validation Loss: 1.0833 Validation Accuracy: 90.2401\n",
            "\n",
            "Time taken for the epoch 105.7237\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 22\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0187\n",
            "Batch 100 Loss 0.0158\n",
            "Batch 200 Loss 0.0102\n",
            "Batch 300 Loss 0.0134\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0117 Train Accuracy: 99.6479 Validation Loss: 1.1573 Validation Accuracy: 89.8666\n",
            "\n",
            "Time taken for the epoch 147.0969\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 23\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0124\n",
            "Batch 100 Loss 0.0104\n",
            "Batch 200 Loss 0.0153\n",
            "Batch 300 Loss 0.0132\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0125 Train Accuracy: 99.6408 Validation Loss: 1.0303 Validation Accuracy: 91.2678\n",
            "\n",
            "Time taken for the epoch 143.4022\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 24\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0118\n",
            "Batch 100 Loss 0.0146\n",
            "Batch 200 Loss 0.0156\n",
            "Batch 300 Loss 0.0099\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0112 Train Accuracy: 99.6515 Validation Loss: 1.0430 Validation Accuracy: 90.9988\n",
            "\n",
            "Time taken for the epoch 140.6828\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 25\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0171\n",
            "Batch 100 Loss 0.0068\n",
            "Batch 200 Loss 0.0102\n",
            "Batch 300 Loss 0.0216\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0124 Train Accuracy: 99.6437 Validation Loss: 1.1091 Validation Accuracy: 90.5885\n",
            "\n",
            "Time taken for the epoch 138.5979\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 26\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0156\n",
            "Batch 100 Loss 0.0033\n",
            "Batch 200 Loss 0.0108\n",
            "Batch 300 Loss 0.0084\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0097 Train Accuracy: 99.6857 Validation Loss: 1.1600 Validation Accuracy: 90.3822\n",
            "\n",
            "Time taken for the epoch 140.0645\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 27\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0077\n",
            "Batch 100 Loss 0.0079\n",
            "Batch 200 Loss 0.0040\n",
            "Batch 300 Loss 0.0081\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0087 Train Accuracy: 99.7503 Validation Loss: 1.1689 Validation Accuracy: 90.4262\n",
            "\n",
            "Time taken for the epoch 142.5499\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 28\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0103\n",
            "Batch 100 Loss 0.0070\n",
            "Batch 200 Loss 0.0057\n",
            "Batch 300 Loss 0.0103\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0095 Train Accuracy: 99.7169 Validation Loss: 1.1522 Validation Accuracy: 90.8521\n",
            "\n",
            "Time taken for the epoch 140.0701\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 29\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0095\n",
            "Batch 100 Loss 0.0120\n",
            "Batch 200 Loss 0.0093\n",
            "Batch 300 Loss 0.0164\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0123 Train Accuracy: 99.6707 Validation Loss: 1.1707 Validation Accuracy: 90.2250\n",
            "\n",
            "Time taken for the epoch 143.3946\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 30\n",
            "\n",
            "Training ...\n",
            "\n",
            "Batch 1 Loss 0.0112\n",
            "Batch 100 Loss 0.0096\n",
            "Batch 200 Loss 0.0081\n",
            "Batch 300 Loss 0.0089\n",
            "\n",
            "Validating ...\n",
            "\n",
            "Train Loss: 0.0089 Train Accuracy: 99.7060 Validation Loss: 1.0864 Validation Accuracy: 90.9801\n",
            "\n",
            "Time taken for the epoch 141.8512\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Model trained successfully !!\n",
            "\n",
            "Running test dataset through the model...\n",
            "\n",
            "Test Loss: 1.1337 Test Accuracy: 0.9076\n",
            "Word level accuracy: 0.5167426193778483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgY9jxQo2I1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fbcaf62-e66e-4921-bd11-ff7d60c80fa0"
      },
      "source": [
        "def get_test_words(n):\n",
        "    test_df = pd.read_csv(get_data_files(\"kn\")[2])\n",
        "    test_sample = test_df.sample(n)\n",
        "    test_sample.reset_index(inplace=True, drop=True)\n",
        "    test_words = []\n",
        "    for i in test_sample.index:\n",
        "        entry = test_sample[\"\\tangadi\\t3\"].loc[i]\n",
        "        parts = entry.split(\"\\t\")\n",
        "        word = parts[1]\n",
        "        test_words.append(word)\n",
        "    return test_words\n",
        "\n",
        "test_words = get_test_words(5)\n",
        "print(test_words)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['chakravartiyaada', 'meelmaiyyu', 'hanneradaneya', 'drustikonakke', 'lekhakarinda']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgDA-bWL1ueE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "414698a9-4247-411b-88ce-9bc6ce60e15e"
      },
      "source": [
        "for word in test_words:\n",
        "    visualise_connectivity(model, word, activation=\"scaler\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-beb1f461e37e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mvisualise_connectivity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"scaler\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_words' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E9pUPNGndTS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d25130f8-0d24-457f-c7d0-bb932367aced"
      },
      "source": [
        "randomly_evaluate(model, n=5)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Randomly evaluating the model on 5 words\n",
            "\n",
            "Input word: bhiksatane\n",
            "Actual translation: \n",
            "Model translation: \n",
            "\n",
            "Input word: parokshavaagi\n",
            "Actual translation: \n",
            "Model translation: \n",
            "\n",
            "Input word: aaeronaautics\n",
            "Actual translation: \n",
            "Model translation: \n",
            "\n",
            "Input word: bhaavagithegala\n",
            "Actual translation: \n",
            "Model translation: \n",
            "\n",
            "Input word: mumbayiyinda\n",
            "Actual translation: \n",
            "Model translation: \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVjQ4r9f1Xoz"
      },
      "source": [
        "# WandB Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_FiH40wgm0i"
      },
      "source": [
        "def train_with_wandb(language, test_beam_search=False):\n",
        "\n",
        "    config_defaults = {\"embedding_dim\": 64, \n",
        "                       \"enc_dec_layers\": 1,\n",
        "                       \"layer_type\": \"lstm\",\n",
        "                       \"units\": 128,\n",
        "                       \"dropout\": 0,\n",
        "                       \"attention\": False,\n",
        "                       \"beam_width\": 3,\n",
        "                       \"teacher_forcing_ratio\": 1.0\n",
        "                       }\n",
        "\n",
        "    wandb.init(config=config_defaults, project='Assignment3-partB_attn', entity='cs21s002-ee21s113-dlassignment-1')\n",
        "    # Below is an example of a custom run name for sweep 4\n",
        "    # This line was different for all sweeps\n",
        "    #wandb.run.name = f\"beam_width_{wandb.config.beam_width}\"\n",
        "\n",
        "    ## 1. SELECT LANGUAGE ##\n",
        "    TRAIN_TSV, VAL_TSV, TEST_TSV = get_data_files(language)\n",
        "\n",
        "    ## 2. DATA PREPROCESSING ##\n",
        "    dataset, input_tokenizer, targ_tokenizer = preprocess_data(TRAIN_TSV)\n",
        "    val_dataset, _, _ = preprocess_data(VAL_TSV, input_tokenizer, targ_tokenizer)\n",
        "\n",
        "    ## 3. CREATING THE MODEL ##\n",
        "    model = Seq2SeqModel(embedding_dim=wandb.config.embedding_dim,\n",
        "                         encoder_layers=wandb.config.enc_dec_layers,\n",
        "                         decoder_layers=wandb.config.enc_dec_layers,\n",
        "                         layer_type=wandb.config.layer_type,\n",
        "                         units=wandb.config.units,\n",
        "                         dropout=wandb.config.dropout,\n",
        "                         attention=wandb.config.attention)\n",
        "    \n",
        "    ## 4. COMPILING THE MODEL \n",
        "    model.set_vocabulary(input_tokenizer, targ_tokenizer)\n",
        "    model.build(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metric = tf.keras.metrics.SparseCategoricalAccuracy())\n",
        "    \n",
        "    ## 5. FITTING AND VALIDATING THE MODEL\n",
        "    model.fit(dataset, val_dataset, epochs=30, use_wandb=True, teacher_forcing_ratio=wandb.config.teacher_forcing_ratio)\n",
        "\n",
        "    if test_beam_search:\n",
        "        ## OPTIONAL :- Evaluate the dataset using beam search and without beam search\n",
        "        val_dataset, _, _ = preprocess_data(VAL_TSV, model.input_tokenizer, model.targ_tokenizer)\n",
        "        subset = val_dataset.take(500)\n",
        "\n",
        "        # a) Without beam search\n",
        "        _, test_acc_without = model.evaluate(subset, batch_size=100) \n",
        "        wandb.log({\"test acc\": test_acc_without})\n",
        "        \n",
        "        # b) With beam search\n",
        "        beam_search = BeamSearch(model=model, k=wandb.config.beam_width)\n",
        "        beam_search.evaluate(subset, batch_size=100, use_wandb=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Kx44JYk6uXr"
      },
      "source": [
        "# Sweeps with Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6ZvJVNy6mxE"
      },
      "source": [
        "sweep_config5 = {\n",
        "  \"name\": \"Attention Sweep - Assignment3\",\n",
        "  \"description\": \"Hyperparameter sweep for Seq2Seq Model with Attention\",\n",
        "  \"method\": \"grid\",\n",
        "  \"parameters\": {\n",
        "        \"enc_dec_layers\": {\n",
        "           \"values\": [1, 2, 3]\n",
        "        },\n",
        "        \"units\": {\n",
        "            \"values\": [128, 256]\n",
        "        },\n",
        "        \"dropout\": {\n",
        "            \"values\": [0, 0.2]\n",
        "        },\n",
        "        \"attention\": {\n",
        "            \"values\": [True]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AntGSlwU7RDm"
      },
      "source": [
        "#sweep_id5 = wandb.sweep(sweep_config5, project='Assignment3-partB_attn', entity='cs21s002-ee21s113-dlassignment-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZtXMBc47h2Z"
      },
      "source": [
        "#wandb.agent(sweep_id5, function=lambda: train_with_wandb(\"kn\"), project='Assignment3-partB_attn', entity='cs21s002-ee21s113-dlassignment-1')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}