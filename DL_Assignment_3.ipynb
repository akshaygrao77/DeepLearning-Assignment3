{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL-Assignment-3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOYnTye0dCSW5jB87gPzajM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshaygrao77/DeepLearning-Assignment3/blob/main/DL_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "v8grmhdOGcQW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar --output daksh.tar\n",
        "!tar -xvf  'daksh.tar' "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGA8GCP1GY0r",
        "outputId": "46e8c541-503a-470d-dfea-c050366a1040"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1915M  100 1915M    0     0  86.4M      0  0:00:22  0:00:22 --:--:--  208M\n",
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def obtain_input_target_data_from_path(path,tokenizer_obj):\n",
        "  input_texts = []\n",
        "  target_texts = []\n",
        "  \n",
        "  df = pd.read_csv(path,sep=\"\\t\",names=[\"1\", \"2\",\"3\"]).astype(str)\n",
        "  if tokenizer_obj is None:\n",
        "    # Shuffle rows in random order with a fixed seed(for reproducability)\n",
        "    df=df.sample(frac=1,random_state=1)\n",
        "  # Add all the  input and target texts with start sequence and end sequence added to target \n",
        "  for index, row in df.iterrows():\n",
        "      input_text=row['2']\n",
        "      target_text= row['1']\n",
        "      # Skip empty lines/words\n",
        "      if target_text =='</s>' or input_text=='</s>':\n",
        "        continue\n",
        "      \n",
        "      target_text = \"\\t\" + target_text + \"\\n\"\n",
        "      input_texts.append(input_text)\n",
        "      target_texts.append(target_text)\n",
        "  \n",
        "  return input_texts, target_texts"
      ],
      "metadata": {
        "id": "mzR1_lcKgW5O"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_text_to_sequences(tokenizer_obj,inp_texts):\n",
        "  if tokenizer_obj is None:\n",
        "    tokenizer_obj = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "    tokenizer_obj.fit_on_texts(inp_texts)\n",
        "  ret_tensor = tokenizer_obj.texts_to_sequences(inp_texts)\n",
        "  ret_tensor = tf.keras.preprocessing.sequence.pad_sequences(ret_tensor,padding='post')\n",
        "\n",
        "  return ret_tensor,tokenizer_obj"
      ],
      "metadata": {
        "id": "1pov4thdppKt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This method converts a dataset(from path) to input and target sequences\n",
        "def pre_process_data(path,input_tokenizer=None,target_tokenizer=None,input_length=None,target_length=None):\n",
        "  \n",
        "  input_texts, target_texts = obtain_input_target_data_from_path(path,input_tokenizer)\n",
        "  \n",
        "  input_tensor,input_tokenizer = convert_text_to_sequences(input_tokenizer,input_texts)\n",
        "  \n",
        "  target_tensor,target_tokenizer = convert_text_to_sequences(target_tokenizer,target_texts)\n",
        "  \n",
        "  # Above functions return padded version wrt longest sequence in the given list of sequence\n",
        "  # The below function, pads more zeros wrt input_length and target_length\n",
        "  if input_length is not None and target_length is not None:\n",
        "      input_tensor=tf.concat([input_tensor,tf.zeros((input_tensor.shape[0],input_length-input_tensor.shape[1]))],axis=1)\n",
        "      target_tensor=tf.concat([target_tensor,tf.zeros((target_tensor.shape[0],target_length-target_tensor.shape[1]))],axis=1)\n",
        "  return input_texts,input_tensor,input_tokenizer,target_texts,target_tensor,target_tokenizer"
      ],
      "metadata": {
        "id": "s-fHBjgNHJNY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transliteration_target_language = 'kn'"
      ],
      "metadata": {
        "id": "KL3-IxpstPlS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_texts,train_input_tensor,input_tokenizer,train_target_texts,train_target_tensor,target_tokenizer = pre_process_data(\"/content/dakshina_dataset_v1.0/\"+transliteration_target_language+\"/lexicons/\"+transliteration_target_language+\".translit.sampled.train.tsv\")\n",
        "# Only training dataset is used to fit the tokenizer on text. Other datasets just use this vocab for pre-processing\n",
        "# The length for padding is also set from training datasets\n",
        "val_input_texts,val_input_tensor,val_input_tokenizer,val_target_texts,val_target_tensor,val_target_tokenizer = pre_process_data(\"/content/dakshina_dataset_v1.0/\"+transliteration_target_language+\"/lexicons/\"+transliteration_target_language+\".translit.sampled.dev.tsv\",input_tokenizer,target_tokenizer,train_input_tensor.shape[1],train_target_tensor.shape[1])\n",
        "test_input_texts,test_input_tensor,test_input_tokenizer,test_target_texts,test_target_tensor,test_target_tokenizer = pre_process_data(\"/content/dakshina_dataset_v1.0/\"+transliteration_target_language+\"/lexicons/\"+transliteration_target_language+\".translit.sampled.test.tsv\",input_tokenizer,target_tokenizer,train_input_tensor.shape[1],train_target_tensor.shape[1])"
      ],
      "metadata": {
        "id": "FWLwIxUwISEb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_input_texts[:10])\n",
        "# print(train_input_tensor[:10])\n",
        "# print(train_target_texts[:10])\n",
        "# print(train_target_tensor[:10])\n",
        "print(len(train_input_texts))\n",
        "print(train_input_tensor.shape)\n",
        "# print(len(train_target_texts))\n",
        "# print(train_target_tensor.shape)"
      ],
      "metadata": {
        "id": "7x__RcWFtDlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_encoder_tokens = len(input_tokenizer.word_index)+1\n",
        "num_decoder_tokens = len(target_tokenizer.word_index)+1\n",
        "max_encoder_seq_length =  train_input_tensor.shape[1]\n",
        "max_decoder_seq_length = train_target_tensor.shape[1]"
      ],
      "metadata": {
        "id": "pi_oWsEotJq8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_tokenizer.word_index.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JrohAOrwYR8",
        "outputId": "cf86fc78-a85d-4d2c-b406-061b5778ab06"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['a', 'i', 'n', 'r', 'd', 't', 'u', 'h', 'e', 'l', 's', 'g', 'v', 'k', 'y', 'o', 'm', 'p', 'b', 'c', 'j', 'w', 'f', 'q', 'z', 'x'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_layered_RNN_model(rnn_type,embedding_in_dim,embedding_out_dim,layers,dropout,inp_length,model_out_dim,initial_state):\n",
        "   #input layer ; takes in tokenize input\n",
        "  model_inputs = keras.Input(shape=( inp_length))\n",
        "  #embedding layer\n",
        "  embed = keras.layers.Embedding(embedding_in_dim, embedding_out_dim)(model_inputs)\n",
        "  #will store output of last added layer so that we can add multiple layers\n",
        "  last_layer_model = None\n",
        "  if rnn_type == 'LSTM':\n",
        "    #adding everything except the last LSTM layer, because in last layer return state=True\n",
        "    for i in range(layers):\n",
        "      layered_model = keras.layers.LSTM(model_out_dim, return_sequences=True,return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        model_layer_out,state_h, state_c = layered_model(embed,initial_state)\n",
        "      else:\n",
        "        model_layer_out,state_h, state_c = layered_model(last_layer_model,initial_state)\n",
        "      last_layer_model = model_layer_out\n",
        "    \n",
        "    model_states = [state_h, state_c]\n",
        "  elif rnn_type=='GRU':\n",
        "    #adding everything except the last GRU layer, because in last layer return state=True    \n",
        "    for i in range(layers):\n",
        "      layered_model = keras.layers.GRU(model_out_dim, return_sequences=True,return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        model_layer_out,state = layered_model(embed,initial_state)\n",
        "      else:\n",
        "        model_layer_out,state = layered_model(last_layer_model,initial_state)\n",
        "      last_layer_model = model_layer_out\n",
        "\n",
        "    model_states = [state]\n",
        "  elif rnn_type=='RNN':\n",
        "    #adding everything except the last RNN layer, because in last layer return state=True\n",
        "    for i in range(layers):      \n",
        "      layered_model = keras.layers.SimpleRNN(model_out_dim, return_sequences=True,return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        model_layer_out,state = layered_model(embed,initial_state)\n",
        "      else:\n",
        "        model_layer_out,state = layered_model(last_layer_model,initial_state)\n",
        "      last_layer_model = model_layer_out\n",
        "\n",
        "    model_states = [state]\n",
        "\n",
        "    return model_states,last_layer_model,model_inputs\n"
      ],
      "metadata": {
        "id": "mF1p5Vjl6gGZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Build the model\n",
        "def build_model(rnn_type,embedding_dim,encoder_layers,decoder_layers,dropout,latent_dim):\n",
        "  encoder_states,encoder_outputs,encoder_inputs = build_layered_RNN_model(rnn_type,embedding_in_dim = num_encoder_tokens,embedding_out_dim = embedding_dim,layers = encoder_layers,dropout = dropout,inp_length = max_encoder_seq_length,model_out_dim = latent_dim,initial_state=None)\n",
        "\n",
        "  decoder_states,decoder_outputs,decoder_inputs = build_layered_RNN_model(rnn_type,embedding_in_dim = num_decoder_tokens,embedding_out_dim = embedding_dim,layers = decoder_layers,dropout = dropout,inp_length = max_decoder_seq_length,model_out_dim = latent_dim,initial_state = encoder_states)\n",
        "  \n",
        "  decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\",name='final')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "  model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "  return model"
      ],
      "metadata": {
        "id": "Br2JpV-7xET_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ogRAn6HcdRL2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}