{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import all necessary libraries"
      ],
      "metadata": {
        "id": "eCUhCbkRn2VT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v8grmhdOGcQW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.callbacks import History\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse"
      ],
      "metadata": {
        "id": "8mx6XHIOXAwM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset using CURL"
      ],
      "metadata": {
        "id": "G1lknpXhnyHT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGA8GCP1GY0r",
        "outputId": "7ea8c082-37f9-456e-a608-9141179d8d37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1915M  100 1915M    0     0   187M      0  0:00:10  0:00:10 --:--:--  156M\n",
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ]
        }
      ],
      "source": [
        "!curl https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar --output daksh.tar\n",
        "!tar -xvf  'daksh.tar' "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login to wandb"
      ],
      "metadata": {
        "id": "uPcgDtyIoBCt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qck0iG7IyN60",
        "outputId": "45ef7769-bee5-431d-98b8-7ccc09ae7a5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.15-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 3.8 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 34.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.11-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 33.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=270bb9672617eb7f86ba29e13aa0016915b422a12c03234abf6484fc3172b0f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.11 setproctitle-1.2.3 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!wandb login\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "SFfsHST6z4y4",
        "outputId": "8c2da5b3-e324-4d7c-9464-2bc955c7527d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makshaygrao\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.15"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220503_142725-1g33tli5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/runs/1g33tli5\" target=\"_blank\">deft-bush-193</a></strong> to <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/runs/1g33tli5?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f01c67d4190>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# wandb.init(project=\"DeepLearningAssignment-3\", entity='cs21s002-ee21s113-dlassignment-1')\n",
        "wandb.init(project=\"DL-Assignment3\", entity='cs21s002-ee21s113-dlassignment-1')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-process dataset"
      ],
      "metadata": {
        "id": "SgmmMZQuoIGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model_prefix = \"Enc_\"\n",
        "decoder_model_prefix = \"Dec_\""
      ],
      "metadata": {
        "id": "RWMRI6z0yak3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mzR1_lcKgW5O"
      },
      "outputs": [],
      "source": [
        "def obtain_input_target_data_from_path(path,tokenizer_obj):\n",
        "  input_texts = []\n",
        "  target_texts = []\n",
        "  \n",
        "  df = pd.read_csv(path,sep=\"\\t\",names=[\"1\", \"2\",\"3\"]).astype(str)\n",
        "  if tokenizer_obj is None:\n",
        "    # Shuffle rows in random order with a fixed seed(for reproducability)\n",
        "    df=df.sample(frac=1,random_state=1)\n",
        "  # Add all the  input and target texts with start sequence and end sequence added to target \n",
        "  for index, row in df.iterrows():\n",
        "      input_text=row['2']\n",
        "      target_text= row['1']\n",
        "      # Skip empty lines/words\n",
        "      if target_text =='</s>' or input_text=='</s>':\n",
        "        continue\n",
        "      \n",
        "      target_text = \"\\t\" + target_text + \"\\n\"\n",
        "      input_texts.append(input_text)\n",
        "      target_texts.append(target_text)\n",
        "  \n",
        "  return input_texts, target_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1pov4thdppKt"
      },
      "outputs": [],
      "source": [
        "def convert_text_to_sequences(tokenizer_obj,inp_texts):\n",
        "  if tokenizer_obj is None:\n",
        "    tokenizer_obj = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "    tokenizer_obj.fit_on_texts(inp_texts)\n",
        "  ret_tensor = tokenizer_obj.texts_to_sequences(inp_texts)\n",
        "  ret_tensor = tf.keras.preprocessing.sequence.pad_sequences(ret_tensor,padding='post')\n",
        "\n",
        "  return ret_tensor,tokenizer_obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "s-fHBjgNHJNY"
      },
      "outputs": [],
      "source": [
        "# This method converts a dataset(from path) to input and target sequences\n",
        "def pre_process_data(path,input_tokenizer=None,target_tokenizer=None,input_length=None,target_length=None):\n",
        "  \n",
        "  input_texts, target_texts = obtain_input_target_data_from_path(path,input_tokenizer)\n",
        "  \n",
        "  input_tensor,input_tokenizer = convert_text_to_sequences(input_tokenizer,input_texts)\n",
        "  \n",
        "  target_tensor,target_tokenizer = convert_text_to_sequences(target_tokenizer,target_texts)\n",
        "  \n",
        "  # Above functions return padded version wrt longest sequence in the given list of sequence\n",
        "  # The below function, pads more zeros wrt input_length and target_length\n",
        "  if input_length is not None and target_length is not None:\n",
        "      input_tensor=tf.concat([input_tensor,tf.zeros((input_tensor.shape[0],input_length-input_tensor.shape[1]))],axis=1)\n",
        "      target_tensor=tf.concat([target_tensor,tf.zeros((target_tensor.shape[0],target_length-target_tensor.shape[1]))],axis=1)\n",
        "  return input_texts,input_tensor,input_tokenizer,target_texts,target_tensor,target_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KL3-IxpstPlS"
      },
      "outputs": [],
      "source": [
        "transliteration_target_language = 'kn'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FWLwIxUwISEb"
      },
      "outputs": [],
      "source": [
        "train_input_texts,train_input_tensor,input_tokenizer,train_target_texts,train_target_tensor,target_tokenizer = pre_process_data(\"/content/dakshina_dataset_v1.0/\"+transliteration_target_language+\"/lexicons/\"+transliteration_target_language+\".translit.sampled.train.tsv\")\n",
        "# Only training dataset is used to fit the tokenizer on text. Other datasets just use this vocab for pre-processing\n",
        "# The length for padding is also set from training datasets\n",
        "val_input_texts,val_input_tensor,val_input_tokenizer,val_target_texts,val_target_tensor,val_target_tokenizer = pre_process_data(\"/content/dakshina_dataset_v1.0/\"+transliteration_target_language+\"/lexicons/\"+transliteration_target_language+\".translit.sampled.dev.tsv\",input_tokenizer,target_tokenizer,train_input_tensor.shape[1],train_target_tensor.shape[1])\n",
        "test_input_texts,test_input_tensor,test_input_tokenizer,test_target_texts,test_target_tensor,test_target_tokenizer = pre_process_data(\"/content/dakshina_dataset_v1.0/\"+transliteration_target_language+\"/lexicons/\"+transliteration_target_language+\".translit.sampled.test.tsv\",input_tokenizer,target_tokenizer,train_input_tensor.shape[1],train_target_tensor.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pi_oWsEotJq8"
      },
      "outputs": [],
      "source": [
        "num_encoder_tokens = len(input_tokenizer.word_index)+1\n",
        "num_decoder_tokens = len(target_tokenizer.word_index)+1\n",
        "max_encoder_seq_length =  train_input_tensor.shape[1]\n",
        "max_decoder_seq_length = train_target_tensor.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASxKQYeZ7knF",
        "outputId": "35d487fb-0e0d-4591-a9a5-cd287d512a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27\n",
            "64\n",
            "26\n",
            "26\n"
          ]
        }
      ],
      "source": [
        "print(num_encoder_tokens)\n",
        "print(num_decoder_tokens)\n",
        "print(max_encoder_seq_length)\n",
        "print(max_decoder_seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions"
      ],
      "metadata": {
        "id": "zezy7OPwoRVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_encoder_decoder_layers_from_model(model):\n",
        "  decoder_layers = 0\n",
        "  encoder_layers = 0\n",
        "  for each_layer in model.layers:\n",
        "    layer_name = each_layer.name\n",
        "    if(decoder_model_prefix+\"cell\" in layer_name):\n",
        "      decoder_layers += 1\n",
        "    elif(encoder_model_prefix+\"cell\" in layer_name):\n",
        "      encoder_layers += 1\n",
        "  return encoder_layers,decoder_layers"
      ],
      "metadata": {
        "id": "zLyeKv586RC4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_latent_dim_from_model(model):\n",
        "  return model.get_layer(str(encoder_model_prefix)+\"cell_0\").output[0].shape[2]"
      ],
      "metadata": {
        "id": "sePAFz1Lr_ky"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rnntype_from_model(model):\n",
        "  if isinstance(model.get_layer(encoder_model_prefix+\"cell_0\"), keras.layers.LSTM):\n",
        "    return 'LSTM'\n",
        "  elif isinstance(model.get_layer(encoder_model_prefix+\"cell_0\"), keras.layers.GRU):\n",
        "    return \"GRU\"\n",
        "  elif isinstance(model.get_layer(encoder_model_prefix+\"cell_0\"), keras.layers.RNN):\n",
        "    return \"RNN\""
      ],
      "metadata": {
        "id": "cXzE_MFawowh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(code,lr):\n",
        "  if(code==\"SGD\"):\n",
        "    return keras.optimizers.SGD(lr)\n",
        "  elif(code == \"RMSprop\"):\n",
        "    return keras.optimizers.RMSprop(lr)\n",
        "  elif(code == \"Adam\"):\n",
        "    return keras.optimizers.Adam(lr)\n",
        "  elif(code == \"Nadam\"):\n",
        "    return keras.optimizers.Nadam(lr)\n",
        "  else:\n",
        "    return keras.optimizers.Adam(lr)"
      ],
      "metadata": {
        "id": "Mq6YOZgj47Av"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Y0gUSgh26w03"
      },
      "outputs": [],
      "source": [
        "index_to_char_target = dict((target_tokenizer.word_index[key], key) for key in target_tokenizer.word_index.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for constructing seq-seq model"
      ],
      "metadata": {
        "id": "gSXGniWdosl9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mF1p5Vjl6gGZ"
      },
      "outputs": [],
      "source": [
        "def build_layered_RNN_model(rnn_type,embedding_in_dim,embedding_out_dim,layers,dropout,inp_length,model_out_dim,prefix,initial_state = None):\n",
        "   #input layer ; takes in tokenize input\n",
        "  model_inputs = keras.Input(shape=( inp_length),name=prefix+\"inp\")\n",
        "  #embedding layer\n",
        "  embed = keras.layers.Embedding(embedding_in_dim, embedding_out_dim,name=prefix+\"embed\")(model_inputs)\n",
        "  \n",
        "  last_layer_model = None\n",
        "  if rnn_type == 'LSTM':\n",
        "    #adding everything except the last LSTM layer, because in last layer return state=True\n",
        "    for i in range(layers):\n",
        "      layered_model = keras.layers.LSTM(model_out_dim, return_sequences=True,return_state=True,dropout=dropout,name=prefix+\"cell_\"+str(i))\n",
        "      if i==0:\n",
        "        inp_layer = embed\n",
        "      else:\n",
        "        inp_layer = last_layer_model\n",
        "      \n",
        "      model_layer_out,state_h, state_c = layered_model(inp_layer,initial_state)\n",
        "      \n",
        "      last_layer_model = model_layer_out\n",
        "    \n",
        "    model_states = [state_h, state_c]\n",
        "    \n",
        "  elif rnn_type=='GRU':\n",
        "    #adding everything except the last GRU layer, because in last layer return state=True    \n",
        "    for i in range(layers):\n",
        "      layered_model = keras.layers.GRU(model_out_dim, return_sequences=True,return_state=True,dropout=dropout,name=prefix+\"cell_\"+str(i))\n",
        "      if i==0:\n",
        "        inp_layer = embed\n",
        "      else:\n",
        "        inp_layer = last_layer_model\n",
        "      \n",
        "      model_layer_out,state = layered_model(inp_layer,initial_state)\n",
        "      \n",
        "      last_layer_model = model_layer_out\n",
        "\n",
        "    model_states = [state]\n",
        "  elif rnn_type=='RNN':\n",
        "    #adding everything except the last RNN layer, because in last layer return state=True\n",
        "    for i in range(layers):      \n",
        "      layered_model = keras.layers.SimpleRNN(model_out_dim, return_sequences=True,return_state=True,dropout=dropout,name=prefix+\"cell_\"+str(i))\n",
        "      if i==0:\n",
        "        inp_layer = embed\n",
        "      else:\n",
        "        inp_layer = last_layer_model\n",
        "        \n",
        "      model_layer_out,state = layered_model(inp_layer,initial_state)\n",
        "      \n",
        "      last_layer_model = model_layer_out\n",
        "\n",
        "    model_states = [state]\n",
        "    \n",
        "  return model_states,last_layer_model,model_inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Br2JpV-7xET_"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Build the model\n",
        "def build_model(rnn_type,embedding_dim,encoder_layers,decoder_layers,dropout,latent_dim):\n",
        "  \n",
        "  encoder_states,encoder_outputs,encoder_inputs = build_layered_RNN_model(rnn_type=rnn_type,embedding_in_dim = num_encoder_tokens,embedding_out_dim = embedding_dim,layers = encoder_layers,dropout = dropout,inp_length = max_encoder_seq_length,model_out_dim = latent_dim,prefix=encoder_model_prefix)\n",
        "\n",
        "  _,decoder_outputs,decoder_inputs = build_layered_RNN_model(rnn_type=rnn_type,embedding_in_dim = num_decoder_tokens,embedding_out_dim = embedding_dim,layers = decoder_layers,dropout = dropout,inp_length = max_decoder_seq_length,model_out_dim = latent_dim,prefix=decoder_model_prefix,initial_state = encoder_states)\n",
        "  \n",
        "  decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\",name='final')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "  model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for constructing inference model"
      ],
      "metadata": {
        "id": "vTi5-tj7oye0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "iUuV-IQ8ltes"
      },
      "outputs": [],
      "source": [
        "def get_inference_encoder_model(model):\n",
        "  encoder_layers,_ = get_encoder_decoder_layers_from_model(model)\n",
        "  encoder_inputs = model.input[0]  \n",
        "  if isinstance(model.get_layer(encoder_model_prefix+\"cell_0\"), keras.layers.LSTM):\n",
        "    encoder_outputs, state_h_enc, state_c_enc = model.get_layer(encoder_model_prefix+\"cell_\"+str(encoder_layers - 1)).output  \n",
        "    encoder_states = [state_h_enc, state_c_enc]\n",
        "  elif (isinstance(model.get_layer(encoder_model_prefix+\"cell_0\"), keras.layers.GRU) or isinstance(model.get_layer(encoder_model_prefix+\"cell_0\"), keras.layers.RNN)):\n",
        "    encoder_outputs, state = model.get_layer(encoder_model_prefix+\"cell_\"+str(encoder_layers - 1)).output  \n",
        "    encoder_states = [state]\n",
        "\n",
        "  encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "  return encoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WhC8eYW4mPv-"
      },
      "outputs": [],
      "source": [
        "def get_inference_decoder_model(model):\n",
        "  latent_dim = get_latent_dim_from_model(model)\n",
        "  _,decoder_layers = get_encoder_decoder_layers_from_model(model)\n",
        "\n",
        "  # Decoder during inference takes just one character(i.e vector rep of a character). This is either from previous timestep or start of sequence(\"\\t\")\n",
        "  decoder_inputs =  keras.Input(shape=( 1))\n",
        "  # Contains input to each decoder layer\n",
        "  decoder_states_inputs=[]\n",
        "  # Contains state output from each decoder layer\n",
        "  decoder_states=[]\n",
        "  previous_decoder_output = None\n",
        "\n",
        "  emdedded_rep_of_decoder_input = model.get_layer(decoder_model_prefix+\"embed\")(decoder_inputs)\n",
        "  \n",
        "  if isinstance(model.get_layer(decoder_model_prefix+\"cell_0\"), keras.layers.LSTM):\n",
        "    for i in range(decoder_layers):\n",
        "      #every layer must have an input through which we can supply it's hidden state\n",
        "      decoder_state_input_h = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "      decoder_state_input_c = keras.Input(shape=(latent_dim,),name='inp4_'+str(i))\n",
        "      init_state = [decoder_state_input_h, decoder_state_input_c]\n",
        "      decoder_lstm = model.get_layer(decoder_model_prefix+\"cell_\"+str(i))\n",
        "      if i==0:\n",
        "        decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(emdedded_rep_of_decoder_input, initial_state=init_state)\n",
        "      else:\n",
        "        decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(previous_decoder_output, initial_state=init_state )\n",
        "      \n",
        "      previous_decoder_output = decoder_outputs\n",
        "      decoder_states_inputs.append (decoder_state_input_h)\n",
        "      decoder_states_inputs.append (decoder_state_input_c)\n",
        "      decoder_states.append (state_h_dec)\n",
        "      decoder_states.append (state_c_dec)\n",
        "  elif isinstance(model.get_layer(decoder_model_prefix+\"cell_0\"), keras.layers.GRU):\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_state_input = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "      init_state = [decoder_state_input]\n",
        "      decoder_gru = model.get_layer(decoder_model_prefix+\"cell_\"+str(i))\n",
        "      if i==0:\n",
        "        decoder_outputs, state = decoder_gru(emdedded_rep_of_decoder_input, initial_state=init_state)\n",
        "      else:\n",
        "        decoder_outputs, state = decoder_gru(previous_decoder_output, initial_state=init_state )\n",
        "      \n",
        "      previous_decoder_output = decoder_outputs\n",
        "      decoder_states_inputs.append (decoder_state_input)\n",
        "      decoder_states.append (state)\n",
        "  elif isinstance(model.get_layer(decoder_model_prefix+\"cell_0\"), keras.layers.RNN):\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_state_input = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "      init_state = [decoder_state_input]\n",
        "      decoder_rnn = model.get_layer(decoder_model_prefix+\"cell_\"+str(i))\n",
        "      if i==0:\n",
        "        decoder_outputs, state = decoder_rnn(emdedded_rep_of_decoder_input, initial_state=init_state)\n",
        "      else:\n",
        "        decoder_outputs, state = decoder_rnn(previous_decoder_output, initial_state=init_state )\n",
        "      \n",
        "      previous_decoder_output = decoder_outputs\n",
        "      decoder_states_inputs.append (decoder_state_input)\n",
        "      decoder_states.append (state)      \n",
        "  decoder_dense = model.get_layer('final')\n",
        "  decoder_outputs = decoder_dense(previous_decoder_output)\n",
        "  decoder_model = keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "  return decoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NDJJuJM8koKB"
      },
      "outputs": [],
      "source": [
        "def build_inference_model(model):\n",
        "    encoder_model = get_inference_encoder_model(model)\n",
        "    \n",
        "    decoder_model = get_inference_decoder_model(model)\n",
        "\n",
        "    return encoder_model,decoder_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for running samples on inference model(with and without beam search)"
      ],
      "metadata": {
        "id": "FpLlZooEo8U_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XshCgw_96g5y"
      },
      "outputs": [],
      "source": [
        "class BeamRecordKeeping:\n",
        "  def __init__(self,decoder_input_state_values,prev_char_index,joint_probability,accumulated_previous_chars):\n",
        "    self.decoder_input_state_values = decoder_input_state_values.copy()\n",
        "    self.prev_char_index = np.copy(prev_char_index)\n",
        "    self.joint_probability = joint_probability\n",
        "    self.accumulated_previous_chars = accumulated_previous_chars\n",
        "\n",
        "  def __str__(self):\n",
        "    return \"decoder_input_state_values: \"+str(self.decoder_input_state_values) + \"\\n prev_char_index: \"+str(self.prev_char_index)+\"\\n joint_probability: \"+str(self.joint_probability)+\" accumulated_previous_chars: \"+str(self.accumulated_previous_chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "510BPMuu6eS7"
      },
      "outputs": [],
      "source": [
        "def get_sampled_char(sampled_token_index):\n",
        "  if sampled_token_index == 0:\n",
        "    sampled_char='\\n'\n",
        "  else:\n",
        "    sampled_char = index_to_char_target[sampled_token_index]\n",
        "  return sampled_char\n",
        "\n",
        "def get_predicted_word_from_beam(list_of_beam_objs):\n",
        "  current_highest_score = list_of_beam_objs[0].joint_probability\n",
        "  predicted_word = list_of_beam_objs[0].accumulated_previous_chars\n",
        "  for each_obj in list_of_beam_objs:\n",
        "    if(each_obj.joint_probability > current_highest_score):\n",
        "      predicted_word = each_obj.accumulated_previous_chars\n",
        "  return predicted_word"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This function is for cases where beam_width=1"
      ],
      "metadata": {
        "id": "hOla8vEXpW2e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Qojx7JC35S5y"
      },
      "outputs": [],
      "source": [
        "def decode_batch_of_sequences(rnn_type,input_seq,encoder_model,decoder_model,batch_size,encoder_layers,decoder_layers):\n",
        "    # Get encoder output\n",
        "    encoder_output_state_values = encoder_model.predict(input_seq)\n",
        "    if rnn_type=='GRU' or 'RNN':\n",
        "      decoder_input_state_values=[encoder_output_state_values]\n",
        "    \n",
        "    # This is needed because encoder state is fed to all decoder layers\n",
        "    decoder_input_state_values = decoder_input_state_values * decoder_layers\n",
        "    \n",
        "    # This is contain previously predicted character's index for every words in batch.\n",
        "    prev_char_index = np.zeros((batch_size, 1))\n",
        "    # We start with \\t for every word in batch\n",
        "    prev_char_index[:, 0] = target_tokenizer.word_index['\\t']\n",
        "    \n",
        "    predicted_words = [ \"\" for i in range(batch_size)]\n",
        "    done=[False for i in range(batch_size)]\n",
        "    for i in range(max_decoder_seq_length):\n",
        "        decoder_out = decoder_model.predict(tuple([prev_char_index] + decoder_input_state_values))\n",
        "        # Decoder output has both output of all timesteps followed by hidden states\n",
        "        output_probability = decoder_out[0]\n",
        "        # Decoder state input is previous layer state output\n",
        "        decoder_input_state_values = decoder_out[1:]\n",
        "        for j in range(batch_size):\n",
        "          if done[j]:\n",
        "            continue          \n",
        "          sampled_token_index = np.argmax(output_probability[j, -1, :])\n",
        "          if sampled_token_index == 0:\n",
        "            sampled_char='\\n'\n",
        "          else:\n",
        "            sampled_char = index_to_char_target[sampled_token_index]\n",
        "          if sampled_char == '\\n':\n",
        "            done[j]=True\n",
        "            continue            \n",
        "          predicted_words[j] += sampled_char\n",
        "          #update the previously predicted characters        \n",
        "          prev_char_index[j,0]=target_tokenizer.word_index[sampled_char]\n",
        "    return predicted_words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This function is called when beam_width>1 during running inference model"
      ],
      "metadata": {
        "id": "uNkF8x_npNY4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Mwfd_Mx3WQmP"
      },
      "outputs": [],
      "source": [
        "def decode_batch_of_sequences_for_bigger_beam_width(rnn_type,input_seq,encoder_model,decoder_model,batch_size,encoder_layers,decoder_layers,beam_search_width):\n",
        "    print(\"batch_size\"+str(batch_size))\n",
        "    print(\"input_seq:\"+str(input_seq.shape))\n",
        "    next_list_of_beam_record_objects = []\n",
        "    predicted_words = [ \"\" for i in range(batch_size)]\n",
        "    list_of_beam_record_objects = []\n",
        "    for j in range(batch_size):\n",
        "      next_list_of_beam_record_objects = []\n",
        "      list_of_beam_record_objects = []\n",
        "      if(j % 100 == 0):\n",
        "        print(\"**********Batch number*************:\"+str(j))\n",
        "      current_seq = input_seq[j]\n",
        "      current_seq = tf.expand_dims(current_seq, 0)\n",
        "      # Get encoder output\n",
        "      decoder_input_state_values = encoder_model.predict(current_seq)\n",
        "      if rnn_type=='GRU' or 'RNN':\n",
        "        decoder_input_state_values=[decoder_input_state_values]\n",
        "        decoder_input_state_values = decoder_input_state_values * decoder_layers\n",
        "      else:\n",
        "        decoder_input_state_values = decoder_input_state_values[0] * decoder_layers\n",
        "      \n",
        "      prev_char_index = np.zeros((1, 1))\n",
        "      # We start with \\t for every word in batch\n",
        "      prev_char_index[:, 0] = target_tokenizer.word_index['\\t']\n",
        "      done  = False\n",
        "      for _ in range(beam_search_width):\n",
        "        current_beam_search_obj = BeamRecordKeeping(decoder_input_state_values,prev_char_index,0,\"\")\n",
        "        list_of_beam_record_objects.append(current_beam_search_obj)\n",
        "\n",
        "      for i in range(max_decoder_seq_length):\n",
        "        if(done):\n",
        "          break\n",
        "\n",
        "        if(i != 0):\n",
        "          list_of_beam_record_objects = next_list_of_beam_record_objects\n",
        "        next_list_of_beam_record_objects = []\n",
        "        for beam_index in range(beam_search_width):\n",
        "          # print(\"prev_char_index\"+str(list_of_beam_record_objects[beam_index].prev_char_index.shape))\n",
        "          # print(\"decoder_input_state_values\"+str(len(list_of_beam_record_objects[beam_index].decoder_input_state_values)))\n",
        "          # print(\"decoder_input_state_values\"+str(list_of_beam_record_objects[beam_index].decoder_input_state_values[0].shape))\n",
        "\n",
        "          decoder_out = decoder_model.predict(tuple([list_of_beam_record_objects[beam_index].prev_char_index] + list_of_beam_record_objects[beam_index].decoder_input_state_values))\n",
        "          # Decoder output has both output of all timesteps followed by hidden states\n",
        "          output_probability = decoder_out[0]\n",
        "          # Decoder state input is previous layer state output\n",
        "          decoder_input_state_values = decoder_out[1:]\n",
        "          sampled_token_index = np.argsort(output_probability[0][-1, :])[-beam_search_width:]\n",
        "          sampled_probability_values = output_probability[0][-1, :][sampled_token_index]\n",
        "\n",
        "          for each_candidate in range(1,len(sampled_probability_values)+1):\n",
        "            new_joint_probability = list_of_beam_record_objects[beam_index].joint_probability + math.log(sampled_probability_values[-each_candidate])\n",
        "            if(len(next_list_of_beam_record_objects) < beam_search_width):\n",
        "              sampled_char = get_sampled_char(sampled_token_index[-each_candidate])\n",
        "              if sampled_char == '\\n':\n",
        "                done = True\n",
        "                break\n",
        "              accumulated_previous_chars = list_of_beam_record_objects[beam_index].accumulated_previous_chars + sampled_char\n",
        "              prev_char_index[:, 0]=target_tokenizer.word_index[sampled_char]\n",
        "              next_beam_record_keeping_obj = BeamRecordKeeping(decoder_input_state_values,prev_char_index,new_joint_probability,accumulated_previous_chars)\n",
        "              next_list_of_beam_record_objects.append(next_beam_record_keeping_obj)\n",
        "            else:\n",
        "              replace_indx = -1\n",
        "              for (current_indx,each_obj) in enumerate(next_list_of_beam_record_objects):\n",
        "                if(each_obj.joint_probability < new_joint_probability):\n",
        "                  replace_indx = current_indx\n",
        "                  break\n",
        "              if(replace_indx != -1):\n",
        "                sampled_char = get_sampled_char(sampled_token_index[-each_candidate])\n",
        "                if sampled_char == '\\n':\n",
        "                  done = True\n",
        "                  break\n",
        "                accumulated_previous_chars = list_of_beam_record_objects[beam_index].accumulated_previous_chars + sampled_char\n",
        "                prev_char_index[:, 0]=target_tokenizer.word_index[sampled_char]\n",
        "                next_beam_record_keeping_obj = BeamRecordKeeping(decoder_input_state_values,prev_char_index,new_joint_probability,accumulated_previous_chars)\n",
        "                next_list_of_beam_record_objects[replace_indx] = next_beam_record_keeping_obj\n",
        "          \n",
        "          if(done or i == max_decoder_seq_length-1):\n",
        "            if( len(next_list_of_beam_record_objects) == 0):\n",
        "              predicted_words[j] = get_predicted_word_from_beam(list_of_beam_record_objects)\n",
        "            else:\n",
        "              predicted_words[j] = get_predicted_word_from_beam(next_list_of_beam_record_objects)\n",
        "            break\n",
        "        \n",
        "    return predicted_words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for obtaining accuracies on inference model"
      ],
      "metadata": {
        "id": "36MKoOcnpbs-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "kbTznqNEx4sR"
      },
      "outputs": [],
      "source": [
        "def test_accuracy(model,encoder_model,decoder_model,beam_search_width=1):\n",
        "  rnn_type = get_rnntype_from_model(model)\n",
        "  encoder_layers,decoder_layers = get_encoder_decoder_layers_from_model(model)\n",
        "  \n",
        "  success=0\n",
        "  success_char = 0\n",
        "  total_chars = 0\n",
        "  #Get all the predicted words\n",
        "  if(beam_search_width == 0 or beam_search_width == 1):\n",
        "    pred=decode_batch_of_sequences(rnn_type,test_input_tensor,encoder_model,decoder_model,test_input_tensor.shape[0],encoder_layers,decoder_layers)\n",
        "  else:\n",
        "    pred=decode_batch_of_sequences_for_bigger_beam_width(rnn_type,test_input_tensor,encoder_model,decoder_model,test_input_tensor.shape[0],encoder_layers,decoder_layers,beam_search_width)\n",
        "  for seq_index in range(test_input_tensor.shape[0]):\n",
        "      predicted_word = pred[seq_index]\n",
        "      target_word=test_target_texts[seq_index][1:-1]\n",
        "      for (indx,each_ele) in enumerate(target_word):\n",
        "        total_chars += 1\n",
        "        if(indx < len(predicted_word)):\n",
        "          if(target_word[indx] == predicted_word[indx]):\n",
        "            success_char += 1\n",
        "\n",
        "      #test the word one by one and write to files\n",
        "      if target_word == predicted_word:\n",
        "        success+=1\n",
        "        f = open(\"success.txt\", \"a\")\n",
        "        f.write(test_input_texts[seq_index]+' '+target_word+' '+predicted_word+'\\n')\n",
        "        f.close()\n",
        "      else:\n",
        "        f = open(\"failure.txt\", \"a\")\n",
        "        f.write(test_input_texts[seq_index]+' '+target_word+' '+predicted_word+'\\n')\n",
        "        f.close()\n",
        "  \n",
        "  print(\"success:\"+str(success))\n",
        "  print(\"success_char:\"+str(success_char))\n",
        "  return float(success)/float(test_input_tensor.shape[0]),float(success_char)/float(total_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "jcWesBWKm2hn"
      },
      "outputs": [],
      "source": [
        "def batch_validate(model,encoder_model,decoder_model,beam_search_width=1):\n",
        "  rnn_type = get_rnntype_from_model(model)\n",
        "  encoder_layers,decoder_layers = get_encoder_decoder_layers_from_model(model)\n",
        "\n",
        "  success = 0\n",
        "  success_char = 0\n",
        "  total_chars = 0\n",
        "  #get all the predicted words\n",
        "  if(beam_search_width == 0 or beam_search_width == 1):\n",
        "    pred=decode_batch_of_sequences(rnn_type,val_input_tensor,encoder_model,decoder_model,val_input_tensor.shape[0],encoder_layers,decoder_layers)\n",
        "  else:\n",
        "    pred=decode_batch_of_sequences_for_bigger_beam_width(rnn_type,val_input_tensor,encoder_model,decoder_model,val_input_tensor.shape[0],encoder_layers,decoder_layers,beam_search_width)\n",
        "\n",
        "  for seq_index in range(val_input_tensor.shape[0]):\n",
        "    predicted_word = pred[seq_index]\n",
        "    target_word = val_target_texts[seq_index][1:-1]\n",
        "    #test the words one by one\n",
        "    if predicted_word == target_word:\n",
        "      # print(\"pred:\"+str(pred[seq_index]))\n",
        "      # print(\"Target: \"+str(val_target_texts[seq_index][1:-1]))\n",
        "      success+=1\n",
        "      \n",
        "    for (indx,each_ele) in enumerate(target_word):\n",
        "      total_chars += 1\n",
        "      if(indx < len(predicted_word)):\n",
        "        if(target_word[indx] == predicted_word[indx]):\n",
        "          # print(\"pred:\"+str(pred[seq_index]))\n",
        "          # print(\"Target: \"+str(target_word))\n",
        "          success_char += 1\n",
        "  \n",
        "  print(\"success:\"+str(success))\n",
        "  print(\"success_char:\"+str(success_char))\n",
        "  # print(\"val_input_tensor.shape[0]:\"+str(val_input_tensor.shape[0]))\n",
        "  return float(success)/float(val_input_tensor.shape[0]),float(success_char)/float(total_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code to train model and return inference and root model "
      ],
      "metadata": {
        "id": "6zIFQx1pptUS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "P_psbbJxVblX"
      },
      "outputs": [],
      "source": [
        "def run_custom_model(encoder_layers,decoder_layers,epochs,lr,latent_dim,rnn_type,embedding_dim,dropout,bs,optimizer,model_save_path,encoder_save_path,decoder_save_path,save=False,use_wandb=False):\n",
        "    if(save == True):\n",
        "      from google.colab import drive\n",
        "      drive.mount('/content/drive')\n",
        "      # Create a MirroredStrategy.\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        strategy = tf.distribute.MirroredStrategy()\n",
        "    else:  # use default strategy\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
        "        # Open a strategy scope and create the model\n",
        "    with strategy.scope():\n",
        "      model = build_model(rnn_type,embedding_dim,encoder_layers,decoder_layers,dropout,latent_dim)\n",
        "\n",
        "    plot_model(model, to_file=str(model_save_path)+'.png', show_shapes=True, show_dtype=True,show_layer_names=True)\n",
        "\n",
        "    model.summary()\n",
        "    \n",
        "    optimizerObj = get_optimizer(optimizer,lr)\n",
        "    \n",
        "    model.compile(optimizer=optimizerObj, loss=keras.losses.SparseCategoricalCrossentropy(reduction='none'), metrics=[\"accuracy\"])\n",
        "    if(use_wandb == False):\n",
        "      hist=model.fit([train_input_tensor, train_target_tensor],tf.concat([train_target_tensor[:,1:],tf.zeros((train_target_tensor[:,:].shape[0],1))], axis=1),batch_size=bs,epochs=epochs,shuffle=True)\n",
        "    else:\n",
        "      hist=model.fit([train_input_tensor, train_target_tensor],tf.concat([train_target_tensor[:,1:],tf.zeros((train_target_tensor[:,:].shape[0],1))], axis=1),batch_size=bs,epochs=epochs,shuffle=True,callbacks=[WandbCallback(), history])\n",
        "\n",
        "    encoder_inference_model,decoder_inference_model=build_inference_model(model)\n",
        "    plot_model(encoder_inference_model, to_file=str(encoder_save_path)+'.png', show_shapes=True)\n",
        "    plot_model(decoder_inference_model, to_file=str(decoder_save_path)+'.png', show_shapes=True)\n",
        "\n",
        "    if(save == True):\n",
        "      model.save('drive/MyDrive/Colab Notebooks/'+str(model_save_path)+'.h5')\n",
        "      encoder_inference_model.save('drive/MyDrive/Colab Notebooks/'+str(encoder_save_path)+\".h5\")\n",
        "      decoder_inference_model.save('drive/MyDrive/Colab Notebooks/'+str(decoder_save_path)+\".h5\")\n",
        "      plot_model(encoder_inference_model, to_file='drive/MyDrive/Colab Notebooks/'+str(encoder_save_path)+\".png\", show_shapes=True)\n",
        "      plot_model(decoder_inference_model, to_file='drive/MyDrive/Colab Notebooks/'+str(decoder_save_path)+'.png', show_shapes=True)\n",
        "      plot_model(model, to_file='drive/MyDrive/Colab Notebooks/'+str(model_save_path)+'.png', show_shapes=True, show_dtype=True,show_layer_names=True)\n",
        "\n",
        "\n",
        "    model.save(str(model_save_path)+'.h5')\n",
        "    encoder_inference_model.save(str(encoder_save_path)+'.h5')\n",
        "    decoder_inference_model.save(str(decoder_save_path)+'.h5')\n",
        "\n",
        "    return model,encoder_inference_model,decoder_inference_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code used to obtain accuracy either by loading models from path(in GDrive or local) or by passing model objects"
      ],
      "metadata": {
        "id": "XGAcBNw6qGNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_run_accuracy(is_test=False,model_path=None,encoder_inf_path=None,decoder_inf_path=None,from_gdrive=False,beam_width = 1,model=None,encoder_inference_model=None,decoder_inference_model=None):\n",
        "  if(from_gdrive == True):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "  \n",
        "  if(model is None):\n",
        "    print(\"Load model from path\")\n",
        "    model = keras.models.load_model(model_path)\n",
        "  \n",
        "  if(encoder_inf_path is None or decoder_inf_path is None):\n",
        "    if(decoder_inference_model is None or encoder_inference_model is None):\n",
        "      encoder_inference_model,decoder_inference_model=build_inference_model(model)\n",
        "  else:\n",
        "    encoder_inference_model= keras.models.load_model(encoder_inf_path)\n",
        "    decoder_inference_model= keras.models.load_model(decoder_inf_path)\n",
        "  \n",
        "  if(is_test == True):\n",
        "    word_val_acc,char_val_acc=test_accuracy(model,encoder_inference_model,decoder_inference_model,beam_width)\n",
        "  else:\n",
        "    word_val_acc,char_val_acc=batch_validate(model,encoder_inference_model,decoder_inference_model,beam_width)\n",
        "\n",
        "  return word_val_acc,char_val_acc"
      ],
      "metadata": {
        "id": "MYpd1tvBn59I"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code to run the best model observed and return inference and root models\n",
        "Later, load_model_run_accuracy needs to be called to obtain accuracies"
      ],
      "metadata": {
        "id": "0OAWGT3BqPyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_best_model(save=False):\n",
        "  encoder_layers = 3\n",
        "  decoder_layers = 3\n",
        "  epochs = 20\n",
        "  lr = 0.0001\n",
        "  latent_dim = 1024\n",
        "  rnn_type = 'GRU'\n",
        "  embedding_dim = 512\n",
        "  dropout = 0.4\n",
        "  bs = 64\n",
        "  optimizer = \"Adam\"\n",
        "  model,encoder_inference_model,decoder_inference_model = run_custom_model(encoder_layers,decoder_layers,epochs,lr,latent_dim,rnn_type,embedding_dim,dropout,bs,optimizer,\"best_model_assignment_3\",\"best_encoder_inference_model_assignment_3\",\"best_decoder_inference_model_assignment_3\",save)\n",
        "  return model,encoder_inference_model,decoder_inference_model"
      ],
      "metadata": {
        "id": "EpOpF5Rv1FFG"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code related to hyper-parameter tuning"
      ],
      "metadata": {
        "id": "HaatzB6vqIaA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "VX5pjUqzDVr-"
      },
      "outputs": [],
      "source": [
        "default_config = {\n",
        "        \"rnn_type\": \"LSTM\",\n",
        "        \"dropout\": 0.5,\n",
        "        \"encoder_layers\":3,\n",
        "        \"decoder_layers\":3,\n",
        "        \"latent_dim\": 64,\n",
        "        \"epochs\": 1,\n",
        "        \"lr\": 0.0001,\n",
        "        \"embedding_out_dim\": 64,\n",
        "        \"beam_search\":1,\n",
        "        \"batch_size\":64,\n",
        "        \"optimizer\": \"Adam\"\n",
        "    }\n",
        "\n",
        "#Keras callback    \n",
        "history = History()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "PvOMovPZyGqj"
      },
      "outputs": [],
      "source": [
        "def HP_tuning_run():\n",
        "    # Create a MirroredStrategy.\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        strategy = tf.distribute.MirroredStrategy()\n",
        "    else:  # use default strategy\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
        "\n",
        "    # wandb.init(config=default_config, magic=True,project=\"DeepLearningAssignment-3\", entity='cs21s002-ee21s113-dlassignment-1')\n",
        "    wandb.init(config=default_config, magic=True,project=\"DL-Assignment3\", entity='cs21s002-ee21s113-dlassignment-1')\n",
        "    # wandb.init(config=default_config, magic=True,project=\"DeepLearningAssignment-3\", entity='akshaygrao')\n",
        "    config = wandb.config\n",
        "    print(\"Config: \"+str(config))\n",
        "    run_name = str(config).replace(\"{\", \"\").replace(\"}\",\"\").replace(\":\",\"-\")\n",
        "    wandb.run.name = run_name\n",
        "    \n",
        "    model,encoder_inference_model,decoder_inference_model = run_custom_model(encoder_layers=config.encoder_layers,decoder_layers=config.decoder_layers,epochs=config.epochs,lr=config.lr,latent_dim=config.latent_dim,rnn_type=config.rnn_type,embedding_dim=config.embedding_out_dim,dropout=config.dropout,bs=config.batch_size,optimizer=config.optimizer,model_save_path=f'{run_name.replace(\",\",\"-\")}_model',encoder_save_path=f'{run_name.replace(\",\",\"-\")}_encoder',decoder_save_path=f'{run_name.replace(\",\",\"-\")}_decoder',save=False,use_wandb=True)\n",
        "\n",
        "    word_val_acc,char_val_acc = load_model_run_accuracy(model=model,encoder_inference_model=encoder_inference_model,decoder_inference_model=decoder_inference_model,beam_width=config.beam_search)\n",
        "    print(\"word_val_acc\"+str(word_val_acc))\n",
        "    print(\"char_val_acc\"+str(char_val_acc))\n",
        "    wandb.log({\"word_val_acc\":round(word_val_acc,5)})\n",
        "    wandb.log({\"char_val_acc\":round(char_val_acc,5)})\n",
        "    wandb.log({\"language\":transliteration_target_language})\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "w5KOeeV13mvj"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    \"name\": \"Assignment-3-final-batch-optimizer\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\":{\n",
        "      \"goal\": \"maximize\",\n",
        "      \"name\": \"word_val_acc\"\n",
        "    },\n",
        "    \"project\": 'DL-Assignment3',\n",
        "    \"parameters\": {\n",
        "        \"rnn_type\": {\n",
        "            \"values\": [\"LSTM\",\"GRU\",\"RNN\"]\n",
        "        },\n",
        "        \"dropout\": {\n",
        "            \"values\": [0.2,0.4]\n",
        "        },\n",
        "        \"encoder_layers\": {\n",
        "            \"values\": [3]\n",
        "        },\n",
        "        \"decoder_layers\": {\n",
        "            \"values\": [3]\n",
        "        },\n",
        "        \"latent_dim\": {\n",
        "            \"values\": [512,1024,2048]\n",
        "        },\n",
        "        \"epochs\": {\n",
        "            \"values\": [20]\n",
        "        },\n",
        "        \"lr\": {\n",
        "            \"values\": [0.0001]\n",
        "        },\n",
        "        \"embedding_out_dim\": {\n",
        "            \"values\":[128,256]\n",
        "        },\n",
        "        \"beam_search\":{\n",
        "            \"values\":[1]\n",
        "        },\n",
        "        \"batch_size\":{\n",
        "            \"values\":[64,128,256]\n",
        "        },\n",
        "        \"optimizer\":{\n",
        "            \"values\":[\"Nadam\",\"SGD\",\"RMSprop\"]\n",
        "        }\n",
        "        \n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "fag3IotW5Qu2"
      },
      "outputs": [],
      "source": [
        "# sweep_id = wandb.sweep(sweep_config,  project='DeepLearningAssignment-3', entity='akshaygrao')\n",
        "# sweep_id = wandb.sweep(sweep_config,  project='DeepLearningAssignment-3', entity='cs21s002-ee21s113-dlassignment-1')\n",
        "# sweep_id = wandb.sweep(sweep_config,  project='DL-Assignment3', entity='cs21s002-ee21s113-dlassignment-1')\n",
        "sweep_id=\"1fwq5qge\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "itgfsYWC5oCn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "75143f40e2844022852a4765ac70c3f1",
            "1da3e3c2b4234050a2000dc121d64a5e",
            "170647ce3aa640718e9e85c833a5cbe1",
            "e32c689decd4454fad7879bad17aa25a",
            "249426163d644fd5879784cd7cca2d1d",
            "65b5e644785e481b8b8977052ecab1c4",
            "0d7a9564b7cf4b72ace029af9f3efb09",
            "c75cb0d6518648e2b29d624246cadfc1",
            "27548c64df0c4d04a524dd221eeb8da2",
            "6c0f6bc73cf44af0a56f0d7ce5de0c75",
            "49db4423decb4a12bdbfb46243aa6544",
            "b72189638096422a8d95c0a7dc479b72",
            "37cce3bc001f4f118f7bc32cd84d6fb6",
            "f8bcdb52d9a04fee80818b28a01141d1",
            "bd96766aa2564b3595fa91ebbc4f0926",
            "29811cf1d5754af191235ad357d48a69"
          ]
        },
        "outputId": "3830127d-32bb-4c77-9e58-0ee5476329ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jb9hx5cn with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_out_dim: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatent_dim: 1024\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: RMSprop\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_type: RNN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Number of devices: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.15"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220503_154109-jb9hx5cn</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/runs/jb9hx5cn\" target=\"_blank\">super-sweep-6</a></strong> to <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/sweeps/1fwq5qge\" target=\"_blank\">https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/sweeps/1fwq5qge</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:jb9hx5cn) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75143f40e2844022852a4765ac70c3f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">super-sweep-6</strong>: <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/runs/jb9hx5cn\" target=\"_blank\">https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/runs/jb9hx5cn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220503_154109-jb9hx5cn/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:jb9hx5cn). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.15"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220503_154116-jb9hx5cn</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/runs/jb9hx5cn\" target=\"_blank\">super-sweep-6</a></strong> to <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/sweeps/1fwq5qge\" target=\"_blank\">https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/sweeps/1fwq5qge</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config: {'batch_size': 128, 'beam_search': 1, 'decoder_layers': 3, 'dropout': 0.2, 'embedding_out_dim': 256, 'encoder_layers': 3, 'epochs': 20, 'latent_dim': 1024, 'lr': 0.0001, 'optimizer': 'RMSprop', 'rnn_type': 'RNN'}\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Number of devices: 1\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Enc_inp (InputLayer)           [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " Enc_embed (Embedding)          (None, 26, 256)      6912        ['Enc_inp[0][0]']                \n",
            "                                                                                                  \n",
            " Enc_cell_0 (SimpleRNN)         [(None, 26, 1024),   1311744     ['Enc_embed[0][0]']              \n",
            "                                 (None, 1024)]                                                    \n",
            "                                                                                                  \n",
            " Dec_inp (InputLayer)           [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " Enc_cell_1 (SimpleRNN)         [(None, 26, 1024),   2098176     ['Enc_cell_0[0][0]']             \n",
            "                                 (None, 1024)]                                                    \n",
            "                                                                                                  \n",
            " Dec_embed (Embedding)          (None, 26, 256)      16384       ['Dec_inp[0][0]']                \n",
            "                                                                                                  \n",
            " Enc_cell_2 (SimpleRNN)         [(None, 26, 1024),   2098176     ['Enc_cell_1[0][0]']             \n",
            "                                 (None, 1024)]                                                    \n",
            "                                                                                                  \n",
            " Dec_cell_0 (SimpleRNN)         [(None, 26, 1024),   1311744     ['Dec_embed[0][0]',              \n",
            "                                 (None, 1024)]                    'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " Dec_cell_1 (SimpleRNN)         [(None, 26, 1024),   2098176     ['Dec_cell_0[0][0]',             \n",
            "                                 (None, 1024)]                    'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " Dec_cell_2 (SimpleRNN)         [(None, 26, 1024),   2098176     ['Dec_cell_1[0][0]',             \n",
            "                                 (None, 1024)]                    'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " final (Dense)                  (None, 26, 64)       65600       ['Dec_cell_2[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 11,105,088\n",
            "Trainable params: 11,105,088\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "396/396 [==============================] - 157s 375ms/step - loss: 0.9376 - accuracy: 0.7600 - _timestamp: 1651592647.0000 - _runtime: 164.0000\n",
            "Epoch 2/20\n",
            "396/396 [==============================] - 148s 374ms/step - loss: 0.3443 - accuracy: 0.8977 - _timestamp: 1651592795.0000 - _runtime: 312.0000\n",
            "Epoch 3/20\n",
            "396/396 [==============================] - 147s 371ms/step - loss: 0.2408 - accuracy: 0.9276 - _timestamp: 1651592942.0000 - _runtime: 459.0000\n",
            "Epoch 4/20\n",
            "396/396 [==============================] - 147s 372ms/step - loss: 0.1857 - accuracy: 0.9439 - _timestamp: 1651593090.0000 - _runtime: 607.0000\n",
            "Epoch 5/20\n",
            "396/396 [==============================] - 148s 374ms/step - loss: 0.1513 - accuracy: 0.9541 - _timestamp: 1651593238.0000 - _runtime: 755.0000\n",
            "Epoch 6/20\n",
            "396/396 [==============================] - 148s 373ms/step - loss: 0.1271 - accuracy: 0.9614 - _timestamp: 1651593385.0000 - _runtime: 902.0000\n",
            "Epoch 7/20\n",
            "396/396 [==============================] - 147s 372ms/step - loss: 0.1093 - accuracy: 0.9667 - _timestamp: 1651593533.0000 - _runtime: 1050.0000\n",
            "Epoch 8/20\n",
            "396/396 [==============================] - 148s 375ms/step - loss: 0.0960 - accuracy: 0.9707 - _timestamp: 1651593681.0000 - _runtime: 1198.0000\n",
            "Epoch 9/20\n",
            "396/396 [==============================] - 148s 373ms/step - loss: 0.0850 - accuracy: 0.9739 - _timestamp: 1651593829.0000 - _runtime: 1346.0000\n",
            "Epoch 10/20\n",
            "396/396 [==============================] - 148s 374ms/step - loss: 0.0756 - accuracy: 0.9768 - _timestamp: 1651593977.0000 - _runtime: 1494.0000\n",
            "Epoch 11/20\n",
            "396/396 [==============================] - 149s 375ms/step - loss: 0.0691 - accuracy: 0.9786 - _timestamp: 1651594125.0000 - _runtime: 1642.0000\n",
            "Epoch 12/20\n",
            "396/396 [==============================] - 148s 373ms/step - loss: 0.0623 - accuracy: 0.9808 - _timestamp: 1651594273.0000 - _runtime: 1790.0000\n",
            "Epoch 13/20\n",
            "396/396 [==============================] - 148s 373ms/step - loss: 0.0572 - accuracy: 0.9822 - _timestamp: 1651594420.0000 - _runtime: 1937.0000\n",
            "Epoch 14/20\n",
            "396/396 [==============================] - 149s 376ms/step - loss: 0.0529 - accuracy: 0.9835 - _timestamp: 1651594569.0000 - _runtime: 2086.0000\n",
            "Epoch 15/20\n",
            "396/396 [==============================] - 154s 390ms/step - loss: 0.0487 - accuracy: 0.9848 - _timestamp: 1651594724.0000 - _runtime: 2241.0000\n",
            "Epoch 16/20\n",
            "396/396 [==============================] - 150s 378ms/step - loss: 0.0450 - accuracy: 0.9859 - _timestamp: 1651594873.0000 - _runtime: 2390.0000\n",
            "Epoch 17/20\n",
            "396/396 [==============================] - 148s 373ms/step - loss: 0.0422 - accuracy: 0.9867 - _timestamp: 1651595021.0000 - _runtime: 2538.0000\n",
            "Epoch 18/20\n",
            "396/396 [==============================] - 149s 376ms/step - loss: 0.0391 - accuracy: 0.9877 - _timestamp: 1651595170.0000 - _runtime: 2687.0000\n",
            "Epoch 19/20\n",
            "396/396 [==============================] - 150s 378ms/step - loss: 0.0370 - accuracy: 0.9884 - _timestamp: 1651595320.0000 - _runtime: 2837.0000\n",
            "Epoch 20/20\n",
            "396/396 [==============================] - 149s 377ms/step - loss: 0.0350 - accuracy: 0.9889 - _timestamp: 1651595469.0000 - _runtime: 2986.0000\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "success:1965\n",
            "success_char:30174\n",
            "word_val_acc0.3908891983290233\n",
            "char_val_acc0.7429091983454796\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.857 MB of 0.857 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27548c64df0c4d04a524dd221eeb8da2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▅▆▇▇▇▇▇████████████</td></tr><tr><td>char_val_acc</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>loss</td><td>█▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>word_val_acc</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.98888</td></tr><tr><td>char_val_acc</td><td>0.74291</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>language</td><td>kn</td></tr><tr><td>loss</td><td>0.03501</td></tr><tr><td>word_val_acc</td><td>0.39089</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">super-sweep-6</strong>: <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/runs/jb9hx5cn\" target=\"_blank\">https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/runs/jb9hx5cn</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220503_154116-jb9hx5cn/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pmqyzng5 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_out_dim: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatent_dim: 2048\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: RMSprop\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_type: GRU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Number of devices: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.15"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220503_163246-pmqyzng5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/runs/pmqyzng5\" target=\"_blank\">volcanic-sweep-9</a></strong> to <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/sweeps/1fwq5qge\" target=\"_blank\">https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/sweeps/1fwq5qge</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config: {'batch_size': 128, 'beam_search': 1, 'decoder_layers': 3, 'dropout': 0.4, 'embedding_out_dim': 256, 'encoder_layers': 3, 'epochs': 20, 'latent_dim': 2048, 'lr': 0.0001, 'optimizer': 'RMSprop', 'rnn_type': 'GRU'}\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Number of devices: 1\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Enc_inp (InputLayer)           [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " Enc_embed (Embedding)          (None, 26, 256)      6912        ['Enc_inp[0][0]']                \n",
            "                                                                                                  \n",
            " Enc_cell_0 (GRU)               [(None, 26, 2048),   14168064    ['Enc_embed[0][0]']              \n",
            "                                 (None, 2048)]                                                    \n",
            "                                                                                                  \n",
            " Dec_inp (InputLayer)           [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " Enc_cell_1 (GRU)               [(None, 26, 2048),   25178112    ['Enc_cell_0[0][0]']             \n",
            "                                 (None, 2048)]                                                    \n",
            "                                                                                                  \n",
            " Dec_embed (Embedding)          (None, 26, 256)      16384       ['Dec_inp[0][0]']                \n",
            "                                                                                                  \n",
            " Enc_cell_2 (GRU)               [(None, 26, 2048),   25178112    ['Enc_cell_1[0][0]']             \n",
            "                                 (None, 2048)]                                                    \n",
            "                                                                                                  \n",
            " Dec_cell_0 (GRU)               [(None, 26, 2048),   14168064    ['Dec_embed[0][0]',              \n",
            "                                 (None, 2048)]                    'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " Dec_cell_1 (GRU)               [(None, 26, 2048),   25178112    ['Dec_cell_0[0][0]',             \n",
            "                                 (None, 2048)]                    'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " Dec_cell_2 (GRU)               [(None, 26, 2048),   25178112    ['Dec_cell_1[0][0]',             \n",
            "                                 (None, 2048)]                    'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " final (Dense)                  (None, 26, 64)       131136      ['Dec_cell_2[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 129,203,008\n",
            "Trainable params: 129,203,008\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "228/396 [================>.............] - ETA: 3:52 - loss: 1.2861 - accuracy: 0.6835"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ],
      "source": [
        "# wandb.agent(sweep_id, function=HP_tuning_run, project='DeepLearningAssignment-3', entity='akshaygrao')\n",
        "# wandb.agent(sweep_id, function=HP_tuning_run, project='DeepLearningAssignment-3', entity='cs21s002-ee21s113-dlassignment-1')\n",
        "wandb.agent(sweep_id, function=HP_tuning_run, project='DL-Assignment3', entity='cs21s002-ee21s113-dlassignment-1')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run this code in colab to get test accuracy for best model"
      ],
      "metadata": {
        "id": "LgC5dYaPqdgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model,encoder_inference_model,decoder_inference_model = run_best_model(True)\n",
        "word_val_acc,char_val_acc = load_model_run_accuracy(model=model,encoder_inference_model=encoder_inference_model,decoder_inference_model=decoder_inference_model,is_test=True)\n",
        "print(\"Test word_val_acc\"+str(word_val_acc))\n",
        "print(\"Test char_val_acc\"+str(char_val_acc))"
      ],
      "metadata": {
        "id": "EyVtENGJ76B0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77093b49-91d4-4a3d-efb6-6a8963b202e1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Number of devices: 1\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Enc_inp (InputLayer)           [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " Enc_embed (Embedding)          (None, 26, 512)      13824       ['Enc_inp[0][0]']                \n",
            "                                                                                                  \n",
            " Enc_cell_0 (GRU)               [(None, 26, 1024),   4724736     ['Enc_embed[0][0]']              \n",
            "                                 (None, 1024)]                                                    \n",
            "                                                                                                  \n",
            " Dec_inp (InputLayer)           [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " Enc_cell_1 (GRU)               [(None, 26, 1024),   6297600     ['Enc_cell_0[0][0]']             \n",
            "                                 (None, 1024)]                                                    \n",
            "                                                                                                  \n",
            " Dec_embed (Embedding)          (None, 26, 512)      32768       ['Dec_inp[0][0]']                \n",
            "                                                                                                  \n",
            " Enc_cell_2 (GRU)               [(None, 26, 1024),   6297600     ['Enc_cell_1[0][0]']             \n",
            "                                 (None, 1024)]                                                    \n",
            "                                                                                                  \n",
            " Dec_cell_0 (GRU)               [(None, 26, 1024),   4724736     ['Dec_embed[0][0]',              \n",
            "                                 (None, 1024)]                    'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " Dec_cell_1 (GRU)               [(None, 26, 1024),   6297600     ['Dec_cell_0[0][0]',             \n",
            "                                 (None, 1024)]                    'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " Dec_cell_2 (GRU)               [(None, 26, 1024),   6297600     ['Dec_cell_1[0][0]',             \n",
            "                                 (None, 1024)]                    'Enc_cell_2[0][1]']             \n",
            "                                                                                                  \n",
            " final (Dense)                  (None, 26, 64)       65600       ['Dec_cell_2[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 34,752,064\n",
            "Trainable params: 34,752,064\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "Epoch 1/20\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "791/791 [==============================] - 221s 262ms/step - loss: 1.0752 - accuracy: 0.7148\n",
            "Epoch 2/20\n",
            "791/791 [==============================] - 209s 264ms/step - loss: 0.8129 - accuracy: 0.7685\n",
            "Epoch 3/20\n",
            "791/791 [==============================] - 209s 264ms/step - loss: 0.6499 - accuracy: 0.8127\n",
            "Epoch 4/20\n",
            "791/791 [==============================] - 209s 264ms/step - loss: 0.4866 - accuracy: 0.8590\n",
            "Epoch 5/20\n",
            "791/791 [==============================] - 209s 264ms/step - loss: 0.3540 - accuracy: 0.8963\n",
            "Epoch 6/20\n",
            "791/791 [==============================] - 208s 263ms/step - loss: 0.2587 - accuracy: 0.9241\n",
            "Epoch 7/20\n",
            "791/791 [==============================] - 208s 263ms/step - loss: 0.1937 - accuracy: 0.9435\n",
            "Epoch 8/20\n",
            "791/791 [==============================] - 208s 263ms/step - loss: 0.1497 - accuracy: 0.9564\n",
            "Epoch 9/20\n",
            "791/791 [==============================] - 209s 264ms/step - loss: 0.1185 - accuracy: 0.9655\n",
            "Epoch 10/20\n",
            "791/791 [==============================] - 209s 264ms/step - loss: 0.0956 - accuracy: 0.9723\n",
            "Epoch 11/20\n",
            "791/791 [==============================] - 209s 264ms/step - loss: 0.0784 - accuracy: 0.9773\n",
            "Epoch 12/20\n",
            "791/791 [==============================] - 209s 264ms/step - loss: 0.0652 - accuracy: 0.9813\n",
            "Epoch 13/20\n",
            "791/791 [==============================] - 208s 263ms/step - loss: 0.0548 - accuracy: 0.9843\n",
            "Epoch 14/20\n",
            "791/791 [==============================] - 208s 264ms/step - loss: 0.0461 - accuracy: 0.9869\n",
            "Epoch 15/20\n",
            "791/791 [==============================] - 208s 263ms/step - loss: 0.0394 - accuracy: 0.9888\n",
            "Epoch 16/20\n",
            "791/791 [==============================] - 209s 264ms/step - loss: 0.0339 - accuracy: 0.9904\n",
            "Epoch 17/20\n",
            "791/791 [==============================] - 209s 264ms/step - loss: 0.0292 - accuracy: 0.9917\n",
            "Epoch 18/20\n",
            "791/791 [==============================] - 208s 264ms/step - loss: 0.0253 - accuracy: 0.9929\n",
            "Epoch 19/20\n",
            "791/791 [==============================] - 208s 263ms/step - loss: 0.0226 - accuracy: 0.9935\n",
            "Epoch 20/20\n",
            "791/791 [==============================] - 208s 263ms/step - loss: 0.0199 - accuracy: 0.9944\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "success:2237\n",
            "success_char:31484\n",
            "Test word_val_acc0.44323360412126017\n",
            "Test char_val_acc0.7593275932759328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_val_acc,char_val_acc = load_model_run_accuracy(model=model,encoder_inference_model=encoder_inference_model,decoder_inference_model=decoder_inference_model,is_test=False)\n",
        "print(\"Validation word_val_acc\"+str(word_val_acc))\n",
        "print(\"Validation char_val_acc\"+str(char_val_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnRAsh3CyjVJ",
        "outputId": "0408bb55-089e-4489-d870-8c475b2c79a8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "success:2276\n",
            "success_char:31069\n",
            "Validation word_val_acc0.45275512233936743\n",
            "Validation char_val_acc0.7649448493204648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Command line argument handlers"
      ],
      "metadata": {
        "id": "mpsSeuYIqkCQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hBu8EDUmm-f"
      },
      "outputs": [],
      "source": [
        "def init_argparse() -> argparse.ArgumentParser:\n",
        "    parser = argparse.ArgumentParser()\n",
        "    \n",
        "    subparsers = parser.add_subparsers(dest='command')\n",
        "    run_model_parser = subparsers.add_parser('run_model')\n",
        "    run_model_parser.add_argument(\"--encoder_layers\", action=\"store\",dest='encoder_layers', help=\"Specify number of layers in encoder(default:3)\",default=3,required=False)\n",
        "    run_model_parser.add_argument(\"--decoder_layers\", action=\"store\",dest='decoder_layers', help=\"Specify number of layers in decoder(default:3)\",default=3,required=False)\n",
        "    run_model_parser.add_argument(\"--epochs\", action=\"store\",dest='epochs', help=\"Specify number of epochs(Default:15)\",default=15,required=False)\n",
        "    run_model_parser.add_argument(\"--lr\", action=\"store\",dest='lr', help=\"Specify learning rate(Default:0.0001)\",default=0.0001,required=False)\n",
        "    run_model_parser.add_argument(\"--latent_dim\", action=\"store\",dest='latent_dim', help=\"Specify latent dimensions(Default:1024)\",default=1024,required=False)\n",
        "    run_model_parser.add_argument(\"--rnn_type\", action=\"store\",dest='rnn_type', help=\"Specify cell type of RNN('RNN','GRU','LSTM'))(Default:GRU)\",default='GRU',required=False)\n",
        "    run_model_parser.add_argument(\"--embedding_dim\", action=\"store\",dest='embedding_dim', help=\"Specify dimension of embedding layer output(Default:256)\",default=256,required=False)\n",
        "    run_model_parser.add_argument(\"--dropout\", action=\"store\",dest='dropout', help=\"Specify dropout in input (applies at all layers of encoder and decoder)(Default:0.4)\",default=0.4,required=False)\n",
        "    run_model_parser.add_argument(\"--bs\", action=\"store\",dest='bs', help=\"Specify batch size(Default:64)\",default=64,required=False)\n",
        "    run_model_parser.add_argument(\"--optimizer\", action=\"store\",dest='optimizer', help=\"Specify optimizer algorithm('Adam','Nadam','SGD','RMSprop')(Default:'Adam')\",default='Adam',required=False)\n",
        "    run_model_parser.add_argument(\"--model_save_path\", action=\"store\",dest='model_save_path', help=\"Specify path to save model into(Default:'model')\",default='model',required=False)\n",
        "    run_model_parser.add_argument(\"--encoder_save_path\", action=\"store\",dest='encoder_save_path', help=\"Specify path to save encoder model into(Default:'encoder_inference_model')\",default='encoder_inference_model',required=False)\n",
        "    run_model_parser.add_argument(\"--decoder_save_path\", action=\"store\",dest='decoder_save_path', help=\"Specify path to save decoder model into(Default:'decoder_inference_model')\",default='decoder_inference_model',required=False)\n",
        "    run_model_parser.add_argument(\"--save\", action=\"store_true\",dest='save', help=\"Save model to google drive(default false)\",required=False)\n",
        "\n",
        "    run_best_model_parser = subparsers.add_parser('run_best_model')\n",
        "    run_best_model_parser.add_argument(\"--save\", action=\"store_true\",dest='save', help=\"Save model to google drive(default false)\",required=False)\n",
        "\n",
        "    last_accuracy_parser = subparsers.add_parser('last_model_run_accuracy')\n",
        "    last_accuracy_parser.add_argument(\"--is_test\", action=\"store_true\",dest='is_test', help=\"Specify if you need test accuracy(or validation accuracy). If passed assumes True otherwise returns validation accuracy\",required=False)\n",
        "    last_accuracy_parser.add_argument(\"--beam_width\", action=\"store\",dest='beam_width', help=\"Specify beam width(Default:1)\",default=1,required=False)\n",
        "\n",
        "    accuracy_parser = subparsers.add_parser('load_model_run_accuracy')\n",
        "    accuracy_parser.add_argument(\"--model_path\", action=\"store\",dest='model_path', help=\"Specify model path\",required=True)\n",
        "    accuracy_parser.add_argument(\"--encoder_inf_path\", action=\"store\",dest='encoder_inf_path', help=\"Specify encoder model path\",required=True)\n",
        "    accuracy_parser.add_argument(\"--decoder_inf_path\", action=\"store\",dest='decoder_inf_path', help=\"Specify decoder model path\",required=True)\n",
        "    accuracy_parser.add_argument(\"--from_gdrive\", action=\"store_true\",dest='from_gdrive', help=\"Retrieve path from google drive(If passed assumes True)\",required=False)\n",
        "    accuracy_parser.add_argument(\"--beam_width\", action=\"store\",dest='beam_width', help=\"Specify beam width(Default:1)\",default=1,required=False)\n",
        "    accuracy_parser.add_argument(\"--is_test\", action=\"store_true\",dest='is_test', help=\"Specify if you need test accuracy(or validation accuracy). If passed assumes True otherwise returns validation accuracy\",required=False)\n",
        "\n",
        "    return parser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  parser = init_argparse()\n",
        "  args = parser.parse_args(['--help'])\n",
        "\n",
        "  model = None\n",
        "  encoder_inference_model = None\n",
        "  decoder_inference_model = None\n",
        "  \n",
        "  if(args.command == 'run_model'):\n",
        "    encoder_layers = args.encoder_layers \n",
        "    decoder_layers = args.decoder_layers \n",
        "    epochs = args.epochs \n",
        "    lr = args.lr \n",
        "    latent_dim = args.latent_dim \n",
        "    rnn_type = args.rnn_type \n",
        "    embedding_dim = args.embedding_dim \n",
        "    dropout = args.dropout \n",
        "    bs = args.bs \n",
        "    optimizer = args.optimizer \n",
        "    model_save_path = args.model_save_path \n",
        "    encoder_save_path = args.encoder_save_path \n",
        "    decoder_save_path = args.decoder_save_path \n",
        "    save = args.save\n",
        "\n",
        "    model,encoder_inference_model,decoder_inference_model = run_custom_model(encoder_layers,decoder_layers,epochs,lr,latent_dim,rnn_type,embedding_dim,dropout,bs,beam_width,optimizer,model_save_path,encoder_save_path,decoder_save_path,save=save)\n",
        "  \n",
        "  elif(args.command == 'run_best_model'):\n",
        "    print(\"Running best model----\")\n",
        "    model,encoder_inference_model,decoder_inference_model = run_best_model(save=args.save)\n",
        "  elif(args.command == 'last_model_run_accuracy'):\n",
        "    if(model is None or encoder_inference_model is None or decoder_inference_model is None):\n",
        "      print(\"Producing accuracy of last run model\")\n",
        "      is_test = args.is_test\n",
        "      beam_width = args.beam_width\n",
        "      if(is_test):\n",
        "        word_val_acc,char_val_acc = load_model_run_accuracy(model=model,encoder_inference_model=encoder_inference_model,decoder_inference_model=decoder_inference_model,beam_width=beam_width,is_test=is_test)\n",
        "        print(\"Test word_val_acc\"+str(word_val_acc))\n",
        "        print(\"Test char_val_acc\"+str(char_val_acc))\n",
        "      else:\n",
        "        word_val_acc,char_val_acc = load_model_run_accuracy(model=model,encoder_inference_model=encoder_inference_model,decoder_inference_model=decoder_inference_model,beam_width=beam_width,is_test=is_test)\n",
        "        print(\"Validation word_val_acc\"+str(word_val_acc))\n",
        "        print(\"Validation char_val_acc\"+str(char_val_acc))\n",
        "    else:\n",
        "      print(\"Invalid command!!! Run a model before running accuracy\")\n",
        "  elif(args.command == 'load_model_run_accuracy'):\n",
        "    print(\"Producing accuracy by running model loaded from path\")\n",
        "    is_test = args.is_test\n",
        "    beam_width = args.beam_width\n",
        "    model_path = args.model_path\n",
        "    encoder_inf_path = args.encoder_inf_path\n",
        "    decoder_inf_path = args.decoder_inf_path\n",
        "    from_gdrive = args.from_gdrive\n",
        "    \n",
        "    if(is_test):\n",
        "      word_val_acc,char_val_acc = load_model_run_accuracy(model_path=model_path,encoder_inf_path=encoder_inf_path,decoder_inf_path=decoder_inf_path,beam_width=beam_width,is_test=is_test,from_gdrive=from_gdrive)\n",
        "      print(\"Test word_val_acc\"+str(word_val_acc))\n",
        "      print(\"Test char_val_acc\"+str(char_val_acc))\n",
        "    else:\n",
        "      word_val_acc,char_val_acc = load_model_run_accuracy(model_path=model_path,encoder_inf_path=encoder_inf_path,decoder_inf_path=decoder_inf_path,beam_width=beam_width,is_test=is_test,from_gdrive=from_gdrive)\n",
        "      print(\"Validation word_val_acc\"+str(word_val_acc))\n",
        "      print(\"Validation char_val_acc\"+str(char_val_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "QkmYTo7hZ17s",
        "outputId": "d973a595-dc02-4083-b797-787f931478a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: ipykernel_launcher.py [-h]\n",
            "                             {run_model,run_best_model,last_model_run_accuracy,load_model_run_accuracy}\n",
            "                             ...\n",
            "\n",
            "positional arguments:\n",
            "  {run_model,run_best_model,last_model_run_accuracy,load_model_run_accuracy}\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xxycHaDOlpgp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DL_Assignment_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "75143f40e2844022852a4765ac70c3f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1da3e3c2b4234050a2000dc121d64a5e",
              "IPY_MODEL_170647ce3aa640718e9e85c833a5cbe1"
            ],
            "layout": "IPY_MODEL_e32c689decd4454fad7879bad17aa25a"
          }
        },
        "1da3e3c2b4234050a2000dc121d64a5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_249426163d644fd5879784cd7cca2d1d",
            "placeholder": "​",
            "style": "IPY_MODEL_65b5e644785e481b8b8977052ecab1c4",
            "value": "0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "170647ce3aa640718e9e85c833a5cbe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d7a9564b7cf4b72ace029af9f3efb09",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c75cb0d6518648e2b29d624246cadfc1",
            "value": 1
          }
        },
        "e32c689decd4454fad7879bad17aa25a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "249426163d644fd5879784cd7cca2d1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65b5e644785e481b8b8977052ecab1c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d7a9564b7cf4b72ace029af9f3efb09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c75cb0d6518648e2b29d624246cadfc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27548c64df0c4d04a524dd221eeb8da2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c0f6bc73cf44af0a56f0d7ce5de0c75",
              "IPY_MODEL_49db4423decb4a12bdbfb46243aa6544"
            ],
            "layout": "IPY_MODEL_b72189638096422a8d95c0a7dc479b72"
          }
        },
        "6c0f6bc73cf44af0a56f0d7ce5de0c75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37cce3bc001f4f118f7bc32cd84d6fb6",
            "placeholder": "​",
            "style": "IPY_MODEL_f8bcdb52d9a04fee80818b28a01141d1",
            "value": "0.877 MB of 0.877 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "49db4423decb4a12bdbfb46243aa6544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd96766aa2564b3595fa91ebbc4f0926",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_29811cf1d5754af191235ad357d48a69",
            "value": 1
          }
        },
        "b72189638096422a8d95c0a7dc479b72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37cce3bc001f4f118f7bc32cd84d6fb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8bcdb52d9a04fee80818b28a01141d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd96766aa2564b3595fa91ebbc4f0926": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29811cf1d5754af191235ad357d48a69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
