{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL-Assignment-3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOxWFaUebbhSMPXhr2k5zod",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "db38e8e14c2945dba61dd7634688873f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea098da2201b43a2a09fb08138f72daa",
              "IPY_MODEL_a19b5e3a3c80451b869793bd065fbcc5"
            ],
            "layout": "IPY_MODEL_5e220f82394e4bbd9bf758642d450c7f"
          }
        },
        "ea098da2201b43a2a09fb08138f72daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1776f64a0b0745089335ccdcaa9b768c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_713b60eaf9994712b091a03e414742cf",
            "value": "0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "a19b5e3a3c80451b869793bd065fbcc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1a8b02bf4094eacb6a85aa12a5529a9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d0b12ee9b8941b499ae761ef1e6ad41",
            "value": 1
          }
        },
        "5e220f82394e4bbd9bf758642d450c7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1776f64a0b0745089335ccdcaa9b768c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "713b60eaf9994712b091a03e414742cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1a8b02bf4094eacb6a85aa12a5529a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d0b12ee9b8941b499ae761ef1e6ad41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce0e69fc7e694aada60bd6492de8433a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_558d94ea852c4fb7b2f6f9960c940cc5",
              "IPY_MODEL_34d089a5b63b4d7a90751db8f6ca35b8"
            ],
            "layout": "IPY_MODEL_b831dd6a36a2479b9fc3856ed4216c1a"
          }
        },
        "558d94ea852c4fb7b2f6f9960c940cc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f1c9ab79c014f84975f1e6c782b40fb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d5226b481661403592631ded64d72958",
            "value": "0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "34d089a5b63b4d7a90751db8f6ca35b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e5010ef08fd49a59a9f857009f1c326",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fd9b5f68af44e76ba6c7f868a6daf81",
            "value": 1
          }
        },
        "b831dd6a36a2479b9fc3856ed4216c1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f1c9ab79c014f84975f1e6c782b40fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5226b481661403592631ded64d72958": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e5010ef08fd49a59a9f857009f1c326": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fd9b5f68af44e76ba6c7f868a6daf81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshaygrao77/DeepLearning-Assignment3/blob/main/DL_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.callbacks import History"
      ],
      "metadata": {
        "id": "v8grmhdOGcQW"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar --output daksh.tar\n",
        "!tar -xvf  'daksh.tar' "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGA8GCP1GY0r",
        "outputId": "661c5536-de46-4b35-a1f8-8a62b8c34b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1915M  100 1915M    0     0   147M      0  0:00:13  0:00:13 --:--:--  138M\n",
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!wandb login\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qck0iG7IyN60",
        "outputId": "fb3ff667-8ce2-473a-d180-5effd5ab8a52"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.14-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.8 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.10-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144 kB 43.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181 kB 49.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=4d6787f64bca6b411b6c0b23f8f4bd027385f8ab80f98de7868542cb569b4117\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.10 setproctitle-1.2.3 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.init(project=\"DeepLearningAssignment-3\", entity='cs21s002-ee21s113-dlassignment-1')"
      ],
      "metadata": {
        "id": "SFfsHST6z4y4"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"DeepLearningAssignment-3\", entity='akshaygrao')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213,
          "referenced_widgets": [
            "db38e8e14c2945dba61dd7634688873f",
            "ea098da2201b43a2a09fb08138f72daa",
            "a19b5e3a3c80451b869793bd065fbcc5",
            "5e220f82394e4bbd9bf758642d450c7f",
            "1776f64a0b0745089335ccdcaa9b768c",
            "713b60eaf9994712b091a03e414742cf",
            "c1a8b02bf4094eacb6a85aa12a5529a9",
            "5d0b12ee9b8941b499ae761ef1e6ad41"
          ]
        },
        "id": "lxSP8Ba92LEB",
        "outputId": "4bd3ee58-f3e7-49cc-e064-f5a9559eed05"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:10tbj2mg) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db38e8e14c2945dba61dd7634688873f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">revived-spaceship-1</strong>: <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DeepLearningAssignment-3/runs/10tbj2mg\" target=\"_blank\">https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DeepLearningAssignment-3/runs/10tbj2mg</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220419_171238-10tbj2mg/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:10tbj2mg). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220419_172244-8s6ntcaw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/akshaygrao/DeepLearningAssignment-3/runs/8s6ntcaw\" target=\"_blank\">sage-waterfall-1</a></strong> to <a href=\"https://wandb.ai/akshaygrao/DeepLearningAssignment-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/akshaygrao/DeepLearningAssignment-3/runs/8s6ntcaw?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f32544aa810>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def obtain_input_target_data_from_path(path,tokenizer_obj):\n",
        "  input_texts = []\n",
        "  target_texts = []\n",
        "  \n",
        "  df = pd.read_csv(path,sep=\"\\t\",names=[\"1\", \"2\",\"3\"]).astype(str)\n",
        "  if tokenizer_obj is None:\n",
        "    # Shuffle rows in random order with a fixed seed(for reproducability)\n",
        "    df=df.sample(frac=1,random_state=1)\n",
        "  # Add all the  input and target texts with start sequence and end sequence added to target \n",
        "  for index, row in df.iterrows():\n",
        "      input_text=row['2']\n",
        "      target_text= row['1']\n",
        "      # Skip empty lines/words\n",
        "      if target_text =='</s>' or input_text=='</s>':\n",
        "        continue\n",
        "      \n",
        "      target_text = \"\\t\" + target_text + \"\\n\"\n",
        "      input_texts.append(input_text)\n",
        "      target_texts.append(target_text)\n",
        "  \n",
        "  return input_texts, target_texts"
      ],
      "metadata": {
        "id": "mzR1_lcKgW5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_text_to_sequences(tokenizer_obj,inp_texts):\n",
        "  if tokenizer_obj is None:\n",
        "    tokenizer_obj = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "    tokenizer_obj.fit_on_texts(inp_texts)\n",
        "  ret_tensor = tokenizer_obj.texts_to_sequences(inp_texts)\n",
        "  ret_tensor = tf.keras.preprocessing.sequence.pad_sequences(ret_tensor,padding='post')\n",
        "\n",
        "  return ret_tensor,tokenizer_obj"
      ],
      "metadata": {
        "id": "1pov4thdppKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This method converts a dataset(from path) to input and target sequences\n",
        "def pre_process_data(path,input_tokenizer=None,target_tokenizer=None,input_length=None,target_length=None):\n",
        "  \n",
        "  input_texts, target_texts = obtain_input_target_data_from_path(path,input_tokenizer)\n",
        "  \n",
        "  input_tensor,input_tokenizer = convert_text_to_sequences(input_tokenizer,input_texts)\n",
        "  \n",
        "  target_tensor,target_tokenizer = convert_text_to_sequences(target_tokenizer,target_texts)\n",
        "  \n",
        "  # Above functions return padded version wrt longest sequence in the given list of sequence\n",
        "  # The below function, pads more zeros wrt input_length and target_length\n",
        "  if input_length is not None and target_length is not None:\n",
        "      input_tensor=tf.concat([input_tensor,tf.zeros((input_tensor.shape[0],input_length-input_tensor.shape[1]))],axis=1)\n",
        "      target_tensor=tf.concat([target_tensor,tf.zeros((target_tensor.shape[0],target_length-target_tensor.shape[1]))],axis=1)\n",
        "  return input_texts,input_tensor,input_tokenizer,target_texts,target_tensor,target_tokenizer"
      ],
      "metadata": {
        "id": "s-fHBjgNHJNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transliteration_target_language = 'kn'"
      ],
      "metadata": {
        "id": "KL3-IxpstPlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_texts,train_input_tensor,input_tokenizer,train_target_texts,train_target_tensor,target_tokenizer = pre_process_data(\"/content/dakshina_dataset_v1.0/\"+transliteration_target_language+\"/lexicons/\"+transliteration_target_language+\".translit.sampled.train.tsv\")\n",
        "# Only training dataset is used to fit the tokenizer on text. Other datasets just use this vocab for pre-processing\n",
        "# The length for padding is also set from training datasets\n",
        "val_input_texts,val_input_tensor,val_input_tokenizer,val_target_texts,val_target_tensor,val_target_tokenizer = pre_process_data(\"/content/dakshina_dataset_v1.0/\"+transliteration_target_language+\"/lexicons/\"+transliteration_target_language+\".translit.sampled.dev.tsv\",input_tokenizer,target_tokenizer,train_input_tensor.shape[1],train_target_tensor.shape[1])\n",
        "test_input_texts,test_input_tensor,test_input_tokenizer,test_target_texts,test_target_tensor,test_target_tokenizer = pre_process_data(\"/content/dakshina_dataset_v1.0/\"+transliteration_target_language+\"/lexicons/\"+transliteration_target_language+\".translit.sampled.test.tsv\",input_tokenizer,target_tokenizer,train_input_tensor.shape[1],train_target_tensor.shape[1])"
      ],
      "metadata": {
        "id": "FWLwIxUwISEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_input_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObEnhBOMAHkw",
        "outputId": "09eceebd-af9d-4410-b1a5-69b10cb5fb8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5047, 26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_input_texts[:2])\n",
        "# print(train_input_tensor[:2])\n",
        "# print(val_input_texts[:2])\n",
        "# print(val_input_tensor[:2])\n",
        "# print(train_target_texts[:10])\n",
        "# print(train_target_tensor[:10])\n",
        "print(len(train_input_texts))\n",
        "print(train_input_tensor.shape)\n",
        "# print(len(train_target_texts))\n",
        "# print(train_target_tensor.shape)"
      ],
      "metadata": {
        "id": "7x__RcWFtDlt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2844b5c-3b51-4fc0-cf23-04c86f406978"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50624\n",
            "(50624, 26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_encoder_tokens = len(input_tokenizer.word_index)+1\n",
        "num_decoder_tokens = len(target_tokenizer.word_index)+1\n",
        "max_encoder_seq_length =  train_input_tensor.shape[1]\n",
        "max_decoder_seq_length = train_target_tensor.shape[1]"
      ],
      "metadata": {
        "id": "pi_oWsEotJq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_encoder_tokens)\n",
        "print(num_decoder_tokens)\n",
        "print(max_encoder_seq_length)\n",
        "print(max_decoder_seq_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASxKQYeZ7knF",
        "outputId": "d2850bf3-6415-432c-bd47-23ae574b3f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27\n",
            "64\n",
            "26\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_tokenizer.word_index.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JrohAOrwYR8",
        "outputId": "971c110f-4b8e-4b93-f73f-7508a5c43977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['a', 'i', 'n', 'r', 'd', 't', 'u', 'h', 'e', 'l', 's', 'g', 'v', 'k', 'y', 'o', 'm', 'p', 'b', 'c', 'j', 'w', 'f', 'q', 'z', 'x'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_layered_RNN_model(rnn_type,embedding_in_dim,embedding_out_dim,layers,dropout,inp_length,model_out_dim,initial_state = None):\n",
        "   #input layer ; takes in tokenize input\n",
        "  model_inputs = keras.Input(shape=( inp_length))\n",
        "  #embedding layer\n",
        "  embed = keras.layers.Embedding(embedding_in_dim, embedding_out_dim)(model_inputs)\n",
        "  \n",
        "  last_layer_model = None\n",
        "  if rnn_type == 'LSTM':\n",
        "    #adding everything except the last LSTM layer, because in last layer return state=True\n",
        "    for i in range(layers):\n",
        "      layered_model = keras.layers.LSTM(model_out_dim, return_sequences=True,return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        inp_layer = embed\n",
        "      else:\n",
        "        inp_layer = last_layer_model\n",
        "      \n",
        "      model_layer_out,state_h, state_c = layered_model(inp_layer,initial_state)\n",
        "      \n",
        "      last_layer_model = model_layer_out\n",
        "    \n",
        "    model_states = [state_h, state_c]\n",
        "    \n",
        "  elif rnn_type=='GRU':\n",
        "    #adding everything except the last GRU layer, because in last layer return state=True    \n",
        "    for i in range(layers):\n",
        "      layered_model = keras.layers.GRU(model_out_dim, return_sequences=True,return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        inp_layer = embed\n",
        "      else:\n",
        "        inp_layer = last_layer_model\n",
        "      \n",
        "      model_layer_out,state = layered_model(inp_layer,initial_state)\n",
        "      \n",
        "      last_layer_model = model_layer_out\n",
        "\n",
        "    model_states = [state]\n",
        "  elif rnn_type=='RNN':\n",
        "    #adding everything except the last RNN layer, because in last layer return state=True\n",
        "    for i in range(layers):      \n",
        "      layered_model = keras.layers.SimpleRNN(model_out_dim, return_sequences=True,return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        inp_layer = embed\n",
        "      else:\n",
        "        inp_layer = last_layer_model\n",
        "        \n",
        "      model_layer_out,state = layered_model(inp_layer,initial_state)\n",
        "      \n",
        "      last_layer_model = model_layer_out\n",
        "\n",
        "    model_states = [state]\n",
        "    \n",
        "  return model_states,last_layer_model,model_inputs\n"
      ],
      "metadata": {
        "id": "mF1p5Vjl6gGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Build the model\n",
        "def build_model(rnn_type,embedding_dim,encoder_layers,decoder_layers,dropout,latent_dim):\n",
        "  \n",
        "  encoder_states,encoder_outputs,encoder_inputs = build_layered_RNN_model(rnn_type=rnn_type,embedding_in_dim = num_encoder_tokens,embedding_out_dim = embedding_dim,layers = encoder_layers,dropout = dropout,inp_length = max_encoder_seq_length,model_out_dim = latent_dim)\n",
        "\n",
        "  _,decoder_outputs,decoder_inputs = build_layered_RNN_model(rnn_type=rnn_type,embedding_in_dim = num_decoder_tokens,embedding_out_dim = embedding_dim,layers = decoder_layers,dropout = dropout,inp_length = max_decoder_seq_length,model_out_dim = latent_dim,initial_state = encoder_states)\n",
        "  \n",
        "  decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\",name='final')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "  model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "Br2JpV-7xET_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inference_encoder_model(model,encoder_layers):\n",
        "  encoder_inputs = model.input[0]  \n",
        "  if isinstance(model.layers[encoder_layers+3], keras.layers.LSTM):\n",
        "    encoder_outputs, state_h_enc, state_c_enc = model.layers[encoder_layers+3].output  \n",
        "    encoder_states = [state_h_enc, state_c_enc]\n",
        "  elif (isinstance(model.layers[encoder_layers+3], keras.layers.GRU) or isinstance(model.layers[encoder_layers+3], keras.layers.RNN)):\n",
        "    encoder_outputs, state = model.layers[encoder_layers+3].output  \n",
        "    encoder_states = [state]\n",
        "\n",
        "  encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "  return encoder_model"
      ],
      "metadata": {
        "id": "iUuV-IQ8ltes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inference_decoder_model(model,encoder_layers,decoder_layers,latent_dim):\n",
        "  # Decoder during inference takes just one character(i.e vector rep of a character). This is either from previous timestep or start of sequence(\"\\t\")\n",
        "  decoder_inputs =  keras.Input(shape=( 1))\n",
        "  # Contains input to each decoder layer\n",
        "  decoder_states_inputs=[]\n",
        "  # Contains state output from each decoder layer\n",
        "  decoder_states=[]\n",
        "  previous_decoder_output = None\n",
        "\n",
        "  emdedded_rep_of_decoder_input = model.layers[encoder_layers+2](decoder_inputs)\n",
        "  # \"encoder_layer + 4\" because inp,embedding of encoder + inp,embedding of decoder\n",
        "  if isinstance(model.layers[encoder_layers+4], keras.layers.LSTM):\n",
        "    for i in range(decoder_layers):\n",
        "      #every layer must have an input through which we can supply it's hidden state\n",
        "      decoder_state_input_h = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "      decoder_state_input_c = keras.Input(shape=(latent_dim,),name='inp4_'+str(i))\n",
        "      init_state = [decoder_state_input_h, decoder_state_input_c]\n",
        "      decoder_lstm = model.layers[i+encoder_layers+4]\n",
        "      if i==0:\n",
        "        decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(emdedded_rep_of_decoder_input, initial_state=init_state)\n",
        "      else:\n",
        "        decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(previous_decoder_output, initial_state=init_state )\n",
        "      \n",
        "      previous_decoder_output = decoder_outputs\n",
        "      decoder_states_inputs.append (decoder_state_input_h)\n",
        "      decoder_states_inputs.append (decoder_state_input_c)\n",
        "      decoder_states.append (state_h_dec)\n",
        "      decoder_states.append (state_c_dec)\n",
        "  elif isinstance(model.layers[encoder_layers + 4], keras.layers.GRU):\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_state_input = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "      init_state = [decoder_state_input]\n",
        "      decoder_lstm = model.layers[i+encoder_layers+4]\n",
        "      if i==0:\n",
        "        decoder_outputs, state = decoder_lstm(emdedded_rep_of_decoder_input, initial_state=init_state)\n",
        "      else:\n",
        "        decoder_outputs, state = decoder_lstm(previous_decoder_output, initial_state=init_state )\n",
        "      \n",
        "      previous_decoder_output = decoder_outputs\n",
        "      decoder_states_inputs.append (decoder_state_input)\n",
        "      decoder_states.append (state)\n",
        "  elif isinstance(model.layers[encoder_layers + 4], keras.layers.RNN):\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_state_input = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "      init_state = [decoder_state_input]\n",
        "      decoder_lstm = model.layers[i+encoder_layers+4]\n",
        "      if i==0:\n",
        "        decoder_outputs, state = decoder_lstm(emdedded_rep_of_decoder_input, initial_state=init_state)\n",
        "      else:\n",
        "        decoder_outputs, state = decoder_lstm(previous_decoder_output, initial_state=init_state )\n",
        "      \n",
        "      previous_decoder_output = decoder_outputs\n",
        "      decoder_states_inputs.append (decoder_state_input)\n",
        "      decoder_states.append (state)      \n",
        "  decoder_dense = model.get_layer('final')\n",
        "  decoder_outputs = decoder_dense(previous_decoder_output)\n",
        "  decoder_model = keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "  return decoder_model"
      ],
      "metadata": {
        "id": "WhC8eYW4mPv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_inference_model(model,encoder_layers,decoder_layers,latent_dim):\n",
        "    encoder_model = get_inference_encoder_model(model,encoder_layers)\n",
        "    \n",
        "    decoder_model = get_inference_decoder_model(model,encoder_layers,decoder_layers,latent_dim)\n",
        "\n",
        "    return encoder_model,decoder_model"
      ],
      "metadata": {
        "id": "NDJJuJM8koKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_char_target = dict((target_tokenizer.word_index[key], key) for key in target_tokenizer.word_index.keys())"
      ],
      "metadata": {
        "id": "Y0gUSgh26w03"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_batch_of_sequences(rnn_type,input_seq,encoder_model,decoder_model,batch_size,encoder_layers,decoder_layers):\n",
        "    # Get encoder output\n",
        "    encoder_output_state_values = encoder_model.predict(input_seq)\n",
        "    if rnn_type=='GRU' or 'RNN':\n",
        "      decoder_input_state_values=[encoder_output_state_values]\n",
        "    \n",
        "    # This is needed because encoder state is fed to all decoder layers\n",
        "    decoder_input_state_values = decoder_input_state_values * decoder_layers\n",
        "    \n",
        "    # This is contain previously predicted character's index for every words in batch.\n",
        "    prev_char_index = np.zeros((batch_size, 1))\n",
        "    # We start with \\t for every word in batch\n",
        "    prev_char_index[:, 0] = target_tokenizer.word_index['\\t']\n",
        "    \n",
        "    predicted_words = [ \"\" for i in range(batch_size)]\n",
        "    done=[False for i in range(batch_size)]\n",
        "    for i in range(max_decoder_seq_length):\n",
        "        decoder_out = decoder_model.predict(tuple([prev_char_index] + decoder_input_state_values))\n",
        "        # Decoder output has both output of all timesteps followed by hidden states\n",
        "        output_probability = decoder_out[0]\n",
        "        # Decoder state input is previous layer state output\n",
        "        decoder_input_state_values = decoder_out[1:]\n",
        "        for j in range(batch_size):\n",
        "          if done[j]:\n",
        "            continue          \n",
        "          sampled_token_index = np.argmax(output_probability[j, -1, :])\n",
        "          if sampled_token_index == 0:\n",
        "            sampled_char='\\n'\n",
        "          else:\n",
        "            sampled_char = index_to_char_target[sampled_token_index]\n",
        "          if sampled_char == '\\n':\n",
        "            done[j]=True\n",
        "            continue            \n",
        "          predicted_words[j] += sampled_char\n",
        "          #update the previously predicted characters        \n",
        "          prev_char_index[j,0]=target_tokenizer.word_index[sampled_char]\n",
        "    return predicted_words"
      ],
      "metadata": {
        "id": "Qojx7JC35S5y"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_accuracy(rnn_type,encoder_model,decoder_model,encoder_layers,decoder_layers):\n",
        "  success=0\n",
        "  #Get all the predicted words\n",
        "  pred=decode_batch_of_sequences(rnn_type,test_input_tensor,encoder_model,decoder_model,test_input_tensor.shape[0],encoder_layers,decoder_layers)\n",
        "  for seq_index in range(test_input_tensor.shape[0]):\n",
        "      predicted_word = pred[seq_index]\n",
        "      target_word=test_target_texts[seq_index][1:-1]\n",
        "      #test the word one by one and write to files\n",
        "      if target_word == predicted_word:\n",
        "        success+=1\n",
        "        f = open(\"success.txt\", \"a\")\n",
        "        f.write(test_input_texts[seq_index]+' '+target_word+' '+predicted_word+'\\n')\n",
        "        f.close()\n",
        "      else:\n",
        "        f = open(\"failure.txt\", \"a\")\n",
        "        f.write(test_input_texts[seq_index]+' '+target_word+' '+predicted_word+'\\n')\n",
        "        f.close()\n",
        "  return float(success)/float(test_input_tensor.shape[0])"
      ],
      "metadata": {
        "id": "kbTznqNEx4sR"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_validate(rnn_type,encoder_model,decoder_model,encoder_layers,decoder_layers):\n",
        "  success=0\n",
        "  #get all the predicted words\n",
        "  pred=decode_batch_of_sequences(rnn_type,val_input_tensor,encoder_model,decoder_model,val_input_tensor.shape[0],encoder_layers,decoder_layers)\n",
        "\n",
        "  for seq_index in range(val_input_tensor.shape[0]):\n",
        "      predicted_word = pred[seq_index]\n",
        "      target_word=val_target_texts[seq_index][1:-1]\n",
        "      #test the words one by one\n",
        "      if predicted_word == target_word:\n",
        "        # print(\"pred:\"+str(pred[seq_index]))\n",
        "        # print(\"Target: \"+str(val_target_texts[seq_index][1:-1]))\n",
        "        success+=1\n",
        "  # print(\"success:\"+str(success))\n",
        "  # print(\"val_input_tensor.shape[0]:\"+str(val_input_tensor.shape[0]))\n",
        "  return float(success)/float(val_input_tensor.shape[0])"
      ],
      "metadata": {
        "id": "jcWesBWKm2hn"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_config = {\n",
        "        \"filter_list\":[64, 32, 48, 32, 32], \n",
        "        \"dropout\":0.2, \n",
        "        \"batch_norm\":False, \n",
        "        \"dense_layer_size\":512,\n",
        "        \"epochs\": 10,\n",
        "        \"lr\": 0.001,\n",
        "        \"train_data_aug\":False,\n",
        "        \"activations\": \"relu\"\n",
        "    }\n",
        "\n",
        "#Keras callback    \n",
        "history = History()"
      ],
      "metadata": {
        "id": "VX5pjUqzDVr-"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def HP_tuning_run():\n",
        "    # Create a MirroredStrategy.\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        strategy = tf.distribute.MirroredStrategy()\n",
        "    else:  # use default strategy\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
        "\n",
        "    # wandb.init(config=default_config, magic=True,project=\"DeepLearningAssignment-3\", entity='cs21s002-ee21s113-dlassignment-1')\n",
        "    wandb.init(config=default_config, magic=True,project=\"DeepLearningAssignment-3\", entity='akshaygrao')\n",
        "    config = wandb.config\n",
        "    print(\"Config: \"+str(config))\n",
        "    run_name = str(config).replace(\"{\", \"\").replace(\"}\",\"\").replace(\":\",\"-\")\n",
        "    wandb.run.name = run_name\n",
        "\n",
        "    # Open a strategy scope and create the model\n",
        "    with strategy.scope():\n",
        "      model = build_model(config.rnn_type,config.embedding_out_dim,config.encoder_layers,config.decoder_layers,config.dropout,config.latent_dim)\n",
        "\n",
        "    plot_model(model, to_file='model.png', show_shapes=True, show_dtype=True,show_layer_names=True)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(config.lr), loss=keras.losses.SparseCategoricalCrossentropy(reduction='none'), metrics=[\"accuracy\"])\n",
        "\n",
        "    hist=model.fit([train_input_tensor, train_target_tensor],tf.concat([train_target_tensor[:,1:],tf.zeros((train_target_tensor[:,:].shape[0],1))], axis=1),batch_size=32,epochs=config.epochs,shuffle=True,callbacks=[WandbCallback(), history])\n",
        "\n",
        "    model.save(f'{run_name.replace(\",\",\"-\")}.h5')  # creates a HDF5 file 'my_model.h5'\n",
        "    inf = keras.models.load_model(f'{run_name.replace(\",\",\"-\")}.h5')\n",
        "    encoder_inference_model,decoder_inference_model=build_inference_model(inf,encoder_layers=config.encoder_layers,decoder_layers=config.decoder_layers,latent_dim=config.latent_dim)\n",
        "    plot_model(encoder_inference_model, to_file='encoder_model.png', show_shapes=True)\n",
        "    plot_model(decoder_inference_model, to_file='decoder_model.png', show_shapes=True)\n",
        "    \n",
        "    val_acc=batch_validate(config.rnn_type,encoder_model,decoder_model,config.encoder_layers,config.decoder_layers)\n",
        "    wandb.log({\"val_acc\":val_acc}) \n"
      ],
      "metadata": {
        "id": "PvOMovPZyGqj"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    \"name\": \"Assignment 3 - alpha\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\":{\n",
        "      \"goal\": \"maximize\",\n",
        "      \"name\": \"val_acc\"\n",
        "    },\n",
        "    \"early_terminate\":{\n",
        "      \"type\": \"hyperband\",\n",
        "      \"min_iter\": 2,\n",
        "      \"eta\":2\n",
        "    },\n",
        "    \"project\": 'DeepLearningAssignment-3',\n",
        "    \"parameters\": {\n",
        "        \"rnn_type\": {\n",
        "            \"values\": [\"LSTM\",\"RNN\",\"GRU\"]\n",
        "        },\n",
        "        \"dropout\": {\n",
        "            \"values\": [0.3,0.5,0.7]\n",
        "        },\n",
        "        \"encoder_layers\": {\n",
        "            \"values\": [2,5,7,10]\n",
        "        },\n",
        "        \"decoder_layers\": {\n",
        "            \"values\": [2,5,7,10]\n",
        "        },\n",
        "        \"latent_dim\": {\n",
        "            \"values\": [32,64,128]\n",
        "        },\n",
        "        \"epochs\": {\n",
        "            \"values\": [10,20,30]\n",
        "        },\n",
        "        \"lr\": {\n",
        "            \"values\": [0.0001]\n",
        "        },\n",
        "        \"embedding_out_dim\": {\n",
        "            \"values\":[32,64,128]\n",
        "        }\n",
        "        \n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "w5KOeeV13mvj"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config,  project='DeepLearningAssignment-3', entity='akshaygrao')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fag3IotW5Qu2",
        "outputId": "49c60748-6d0c-4b02-d6dd-e7cc2036e420"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: zrjivzbv\n",
            "Sweep URL: https://wandb.ai/akshaygrao/DeepLearningAssignment-3/sweeps/zrjivzbv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, function=HP_tuning_run, project='DeepLearningAssignment-3', entity='akshaygrao')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ce0e69fc7e694aada60bd6492de8433a",
            "558d94ea852c4fb7b2f6f9960c940cc5",
            "34d089a5b63b4d7a90751db8f6ca35b8",
            "b831dd6a36a2479b9fc3856ed4216c1a",
            "5f1c9ab79c014f84975f1e6c782b40fb",
            "d5226b481661403592631ded64d72958",
            "8e5010ef08fd49a59a9f857009f1c326",
            "6fd9b5f68af44e76ba6c7f868a6daf81"
          ]
        },
        "id": "itgfsYWC5oCn",
        "outputId": "4989374f-9f3f-48e7-b9df-dc005c78e0a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x5b4bwci with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_out_dim: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatent_dim: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_type: RNN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of devices: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220419_173741-x5b4bwci</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/akshaygrao/DeepLearningAssignment-3/runs/x5b4bwci\" target=\"_blank\">laced-sweep-1</a></strong> to <a href=\"https://wandb.ai/akshaygrao/DeepLearningAssignment-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/akshaygrao/DeepLearningAssignment-3/sweeps/zrjivzbv\" target=\"_blank\">https://wandb.ai/akshaygrao/DeepLearningAssignment-3/sweeps/zrjivzbv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:x5b4bwci) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce0e69fc7e694aada60bd6492de8433a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">laced-sweep-1</strong>: <a href=\"https://wandb.ai/akshaygrao/DeepLearningAssignment-3/runs/x5b4bwci\" target=\"_blank\">https://wandb.ai/akshaygrao/DeepLearningAssignment-3/runs/x5b4bwci</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220419_173741-x5b4bwci/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:x5b4bwci). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220419_173753-x5b4bwci</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/akshaygrao/DeepLearningAssignment-3/runs/x5b4bwci\" target=\"_blank\">laced-sweep-1</a></strong> to <a href=\"https://wandb.ai/akshaygrao/DeepLearningAssignment-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/akshaygrao/DeepLearningAssignment-3/sweeps/zrjivzbv\" target=\"_blank\">https://wandb.ai/akshaygrao/DeepLearningAssignment-3/sweeps/zrjivzbv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config: {'decoder_layers': 10, 'dropout': 0.5, 'embedding_out_dim': 128, 'encoder_layers': 5, 'epochs': 30, 'latent_dim': 64, 'lr': 0.0001, 'rnn_type': 'RNN', 'filter_list': [64, 32, 48, 32, 32], 'batch_norm': False, 'dense_layer_size': 512, 'train_data_aug': False, 'activations': 'relu'}\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 26, 128)      3456        ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " simple_rnn (SimpleRNN)         [(None, 26, 64),     12352       ['embedding[0][0]']              \n",
            "                                 (None, 64)]                                                      \n",
            "                                                                                                  \n",
            " simple_rnn_1 (SimpleRNN)       [(None, 26, 64),     8256        ['simple_rnn[0][0]']             \n",
            "                                 (None, 64)]                                                      \n",
            "                                                                                                  \n",
            " simple_rnn_2 (SimpleRNN)       [(None, 26, 64),     8256        ['simple_rnn_1[0][0]']           \n",
            "                                 (None, 64)]                                                      \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " simple_rnn_3 (SimpleRNN)       [(None, 26, 64),     8256        ['simple_rnn_2[0][0]']           \n",
            "                                 (None, 64)]                                                      \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 26, 128)      8192        ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " simple_rnn_4 (SimpleRNN)       [(None, 26, 64),     8256        ['simple_rnn_3[0][0]']           \n",
            "                                 (None, 64)]                                                      \n",
            "                                                                                                  \n",
            " simple_rnn_5 (SimpleRNN)       [(None, 26, 64),     12352       ['embedding_1[0][0]',            \n",
            "                                 (None, 64)]                      'simple_rnn_4[0][1]']           \n",
            "                                                                                                  \n",
            " simple_rnn_6 (SimpleRNN)       [(None, 26, 64),     8256        ['simple_rnn_5[0][0]',           \n",
            "                                 (None, 64)]                      'simple_rnn_4[0][1]']           \n",
            "                                                                                                  \n",
            " simple_rnn_7 (SimpleRNN)       [(None, 26, 64),     8256        ['simple_rnn_6[0][0]',           \n",
            "                                 (None, 64)]                      'simple_rnn_4[0][1]']           \n",
            "                                                                                                  \n",
            " simple_rnn_8 (SimpleRNN)       [(None, 26, 64),     8256        ['simple_rnn_7[0][0]',           \n",
            "                                 (None, 64)]                      'simple_rnn_4[0][1]']           \n",
            "                                                                                                  \n",
            " simple_rnn_9 (SimpleRNN)       [(None, 26, 64),     8256        ['simple_rnn_8[0][0]',           \n",
            "                                 (None, 64)]                      'simple_rnn_4[0][1]']           \n",
            "                                                                                                  \n",
            " simple_rnn_10 (SimpleRNN)      [(None, 26, 64),     8256        ['simple_rnn_9[0][0]',           \n",
            "                                 (None, 64)]                      'simple_rnn_4[0][1]']           \n",
            "                                                                                                  \n",
            " simple_rnn_11 (SimpleRNN)      [(None, 26, 64),     8256        ['simple_rnn_10[0][0]',          \n",
            "                                 (None, 64)]                      'simple_rnn_4[0][1]']           \n",
            "                                                                                                  \n",
            " simple_rnn_12 (SimpleRNN)      [(None, 26, 64),     8256        ['simple_rnn_11[0][0]',          \n",
            "                                 (None, 64)]                      'simple_rnn_4[0][1]']           \n",
            "                                                                                                  \n",
            " simple_rnn_13 (SimpleRNN)      [(None, 26, 64),     8256        ['simple_rnn_12[0][0]',          \n",
            "                                 (None, 64)]                      'simple_rnn_4[0][1]']           \n",
            "                                                                                                  \n",
            " simple_rnn_14 (SimpleRNN)      [(None, 26, 64),     8256        ['simple_rnn_13[0][0]',          \n",
            "                                 (None, 64)]                      'simple_rnn_4[0][1]']           \n",
            "                                                                                                  \n",
            " final (Dense)                  (None, 26, 64)       4160        ['simple_rnn_14[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 147,840\n",
            "Trainable params: 147,840\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/30\n",
            " 263/1582 [===>..........................] - ETA: 2:29 - loss: 3.3696 - accuracy: 0.2805"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model('LSTM',32,2,3,0.3,64)"
      ],
      "metadata": {
        "id": "ogRAn6HcdRL2"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss=keras.losses.SparseCategoricalCrossentropy(reduction='none'), metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "TzZNiKxOGs61"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist=model.fit([train_input_tensor, train_target_tensor],tf.concat([train_target_tensor[:,1:],tf.zeros((train_target_tensor[:,:].shape[0],1))], axis=1),batch_size=32,epochs=5,shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GxbPO6Yf6Fx",
        "outputId": "64fd2688-c22f-4c91-cd12-bfac71a848f5"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1582/1582 [==============================] - 111s 70ms/step - loss: 0.8703 - accuracy: 0.7522\n",
            "Epoch 2/5\n",
            "1582/1582 [==============================] - 109s 69ms/step - loss: 0.7699 - accuracy: 0.7803\n",
            "Epoch 3/5\n",
            "1582/1582 [==============================] - 114s 72ms/step - loss: 0.6420 - accuracy: 0.8164\n",
            "Epoch 4/5\n",
            "1582/1582 [==============================] - 111s 70ms/step - loss: 0.5391 - accuracy: 0.8443\n",
            "Epoch 5/5\n",
            "1582/1582 [==============================] - 111s 70ms/step - loss: 0.4597 - accuracy: 0.8663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "model.save(\"s2s.keras\")\n",
        "# Run inferencing\n",
        "# Restore the model and construct the encoder and decoder.\n",
        "inf = keras.models.load_model(\"/content/s2s.keras\")\n",
        "encoder_model,decoder_model=build_inference_model(inf,encoder_layers=2,decoder_layers=3,latent_dim=64)"
      ],
      "metadata": {
        "id": "_4PpsZv6mahJ"
      },
      "execution_count": 60,
      "outputs": []
    }
  ]
}