{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import all necessary libraries"
      ],
      "metadata": {
        "id": "eCUhCbkRn2VT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v8grmhdOGcQW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.callbacks import History\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse"
      ],
      "metadata": {
        "id": "8mx6XHIOXAwM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset using CURL"
      ],
      "metadata": {
        "id": "G1lknpXhnyHT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGA8GCP1GY0r",
        "outputId": "d93c007e-1944-49e7-fb27-f39823ce01a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1915M  100 1915M    0     0  97.2M      0  0:00:19  0:00:19 --:--:-- 78.4M\n",
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ]
        }
      ],
      "source": [
        "!curl https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar --output daksh.tar\n",
        "!tar -xvf  'daksh.tar' "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login to wandb"
      ],
      "metadata": {
        "id": "uPcgDtyIoBCt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qck0iG7IyN60",
        "outputId": "84ec0287-da40-4621-df21-8021d28840a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.15)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.10)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makshaygrao\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!wandb login\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "SFfsHST6z4y4",
        "outputId": "b13a1133-81bd-4d9f-9a61-ca27fea30496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makshaygrao\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.15"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220503_110912-1a9n3kk1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/runs/1a9n3kk1\" target=\"_blank\">happy-resonance-179</a></strong> to <a href=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/cs21s002-ee21s113-dlassignment-1/DL-Assignment3/runs/1a9n3kk1?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fb4108db450>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# wandb.init(project=\"DeepLearningAssignment-3\", entity='cs21s002-ee21s113-dlassignment-1')\n",
        "wandb.init(project=\"DL-Assignment3\", entity='cs21s002-ee21s113-dlassignment-1')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-process dataset"
      ],
      "metadata": {
        "id": "SgmmMZQuoIGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model_prefix = \"Enc_\"\n",
        "decoder_model_prefix = \"Dec_\""
      ],
      "metadata": {
        "id": "RWMRI6z0yak3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mzR1_lcKgW5O"
      },
      "outputs": [],
      "source": [
        "def obtain_input_target_data_from_path(path,tokenizer_obj):\n",
        "  input_texts = []\n",
        "  target_texts = []\n",
        "  \n",
        "  df = pd.read_csv(path,sep=\"\\t\",names=[\"1\", \"2\",\"3\"]).astype(str)\n",
        "  if tokenizer_obj is None:\n",
        "    # Shuffle rows in random order with a fixed seed(for reproducability)\n",
        "    df=df.sample(frac=1,random_state=1)\n",
        "  # Add all the  input and target texts with start sequence and end sequence added to target \n",
        "  for index, row in df.iterrows():\n",
        "      input_text=row['2']\n",
        "      target_text= row['1']\n",
        "      # Skip empty lines/words\n",
        "      if target_text =='</s>' or input_text=='</s>':\n",
        "        continue\n",
        "      \n",
        "      target_text = \"\\t\" + target_text + \"\\n\"\n",
        "      input_texts.append(input_text)\n",
        "      target_texts.append(target_text)\n",
        "  \n",
        "  return input_texts, target_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1pov4thdppKt"
      },
      "outputs": [],
      "source": [
        "def convert_text_to_sequences(tokenizer_obj,inp_texts):\n",
        "  if tokenizer_obj is None:\n",
        "    tokenizer_obj = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "    tokenizer_obj.fit_on_texts(inp_texts)\n",
        "  ret_tensor = tokenizer_obj.texts_to_sequences(inp_texts)\n",
        "  ret_tensor = tf.keras.preprocessing.sequence.pad_sequences(ret_tensor,padding='post')\n",
        "\n",
        "  return ret_tensor,tokenizer_obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s-fHBjgNHJNY"
      },
      "outputs": [],
      "source": [
        "# This method converts a dataset(from path) to input and target sequences\n",
        "def pre_process_data(path,input_tokenizer=None,target_tokenizer=None,input_length=None,target_length=None):\n",
        "  \n",
        "  input_texts, target_texts = obtain_input_target_data_from_path(path,input_tokenizer)\n",
        "  \n",
        "  input_tensor,input_tokenizer = convert_text_to_sequences(input_tokenizer,input_texts)\n",
        "  \n",
        "  target_tensor,target_tokenizer = convert_text_to_sequences(target_tokenizer,target_texts)\n",
        "  \n",
        "  # Above functions return padded version wrt longest sequence in the given list of sequence\n",
        "  # The below function, pads more zeros wrt input_length and target_length\n",
        "  if input_length is not None and target_length is not None:\n",
        "      input_tensor=tf.concat([input_tensor,tf.zeros((input_tensor.shape[0],input_length-input_tensor.shape[1]))],axis=1)\n",
        "      target_tensor=tf.concat([target_tensor,tf.zeros((target_tensor.shape[0],target_length-target_tensor.shape[1]))],axis=1)\n",
        "  return input_texts,input_tensor,input_tokenizer,target_texts,target_tensor,target_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KL3-IxpstPlS"
      },
      "outputs": [],
      "source": [
        "transliteration_target_language = 'kn'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FWLwIxUwISEb"
      },
      "outputs": [],
      "source": [
        "train_input_texts,train_input_tensor,input_tokenizer,train_target_texts,train_target_tensor,target_tokenizer = pre_process_data(\"/content/dakshina_dataset_v1.0/\"+transliteration_target_language+\"/lexicons/\"+transliteration_target_language+\".translit.sampled.train.tsv\")\n",
        "# Only training dataset is used to fit the tokenizer on text. Other datasets just use this vocab for pre-processing\n",
        "# The length for padding is also set from training datasets\n",
        "val_input_texts,val_input_tensor,val_input_tokenizer,val_target_texts,val_target_tensor,val_target_tokenizer = pre_process_data(\"/content/dakshina_dataset_v1.0/\"+transliteration_target_language+\"/lexicons/\"+transliteration_target_language+\".translit.sampled.dev.tsv\",input_tokenizer,target_tokenizer,train_input_tensor.shape[1],train_target_tensor.shape[1])\n",
        "test_input_texts,test_input_tensor,test_input_tokenizer,test_target_texts,test_target_tensor,test_target_tokenizer = pre_process_data(\"/content/dakshina_dataset_v1.0/\"+transliteration_target_language+\"/lexicons/\"+transliteration_target_language+\".translit.sampled.test.tsv\",input_tokenizer,target_tokenizer,train_input_tensor.shape[1],train_target_tensor.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pi_oWsEotJq8"
      },
      "outputs": [],
      "source": [
        "num_encoder_tokens = len(input_tokenizer.word_index)+1\n",
        "num_decoder_tokens = len(target_tokenizer.word_index)+1\n",
        "max_encoder_seq_length =  train_input_tensor.shape[1]\n",
        "max_decoder_seq_length = train_target_tensor.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASxKQYeZ7knF",
        "outputId": "f0c8de74-6620-4e1b-abd0-20878cc44c45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27\n",
            "64\n",
            "26\n",
            "26\n"
          ]
        }
      ],
      "source": [
        "print(num_encoder_tokens)\n",
        "print(num_decoder_tokens)\n",
        "print(max_encoder_seq_length)\n",
        "print(max_decoder_seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions"
      ],
      "metadata": {
        "id": "zezy7OPwoRVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_encoder_decoder_layers_from_model(model):\n",
        "  decoder_layers = 0\n",
        "  encoder_layers = 0\n",
        "  for each_layer in model.layers:\n",
        "    layer_name = each_layer.name\n",
        "    if(decoder_model_prefix+\"cell\" in layer_name):\n",
        "      decoder_layers += 1\n",
        "    elif(encoder_model_prefix+\"cell\" in layer_name):\n",
        "      encoder_layers += 1\n",
        "  return encoder_layers,decoder_layers"
      ],
      "metadata": {
        "id": "zLyeKv586RC4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_latent_dim_from_model(model):\n",
        "  return model.get_layer(str(encoder_model_prefix)+\"cell_0\").output[0].shape[2]"
      ],
      "metadata": {
        "id": "sePAFz1Lr_ky"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rnntype_from_model(model):\n",
        "  if isinstance(model.get_layer(encoder_model_prefix+\"cell_0\"), keras.layers.LSTM):\n",
        "    return 'LSTM'\n",
        "  elif isinstance(model.get_layer(encoder_model_prefix+\"cell_0\"), keras.layers.GRU):\n",
        "    return \"GRU\"\n",
        "  elif isinstance(model.get_layer(encoder_model_prefix+\"cell_0\"), keras.layers.RNN):\n",
        "    return \"RNN\""
      ],
      "metadata": {
        "id": "cXzE_MFawowh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(code,lr):\n",
        "  if(code==\"SGD\"):\n",
        "    return keras.optimizers.SGD(lr)\n",
        "  elif(code == \"RMSprop\"):\n",
        "    return keras.optimizers.RMSprop(lr)\n",
        "  elif(code == \"Adam\"):\n",
        "    return keras.optimizers.Adam(lr)\n",
        "  elif(code == \"Nadam\"):\n",
        "    return keras.optimizers.Nadam(lr)\n",
        "  else:\n",
        "    return keras.optimizers.Adam(lr)"
      ],
      "metadata": {
        "id": "Mq6YOZgj47Av"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Y0gUSgh26w03"
      },
      "outputs": [],
      "source": [
        "index_to_char_target = dict((target_tokenizer.word_index[key], key) for key in target_tokenizer.word_index.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for constructing seq-seq model"
      ],
      "metadata": {
        "id": "gSXGniWdosl9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mF1p5Vjl6gGZ"
      },
      "outputs": [],
      "source": [
        "def build_layered_RNN_model(rnn_type,embedding_in_dim,embedding_out_dim,layers,dropout,inp_length,model_out_dim,prefix,initial_state = None):\n",
        "   #input layer ; takes in tokenize input\n",
        "  model_inputs = keras.Input(shape=( inp_length),name=prefix+\"inp\")\n",
        "  #embedding layer\n",
        "  embed = keras.layers.Embedding(embedding_in_dim, embedding_out_dim,name=prefix+\"embed\")(model_inputs)\n",
        "  \n",
        "  last_layer_model = None\n",
        "  if rnn_type == 'LSTM':\n",
        "    #adding everything except the last LSTM layer, because in last layer return state=True\n",
        "    for i in range(layers):\n",
        "      layered_model = keras.layers.LSTM(model_out_dim, return_sequences=True,return_state=True,dropout=dropout,name=prefix+\"cell_\"+str(i))\n",
        "      if i==0:\n",
        "        inp_layer = embed\n",
        "      else:\n",
        "        inp_layer = last_layer_model\n",
        "      \n",
        "      model_layer_out,state_h, state_c = layered_model(inp_layer,initial_state)\n",
        "      \n",
        "      last_layer_model = model_layer_out\n",
        "    \n",
        "    model_states = [state_h, state_c]\n",
        "    \n",
        "  elif rnn_type=='GRU':\n",
        "    #adding everything except the last GRU layer, because in last layer return state=True    \n",
        "    for i in range(layers):\n",
        "      layered_model = keras.layers.GRU(model_out_dim, return_sequences=True,return_state=True,dropout=dropout,name=prefix+\"cell_\"+str(i))\n",
        "      if i==0:\n",
        "        inp_layer = embed\n",
        "      else:\n",
        "        inp_layer = last_layer_model\n",
        "      \n",
        "      model_layer_out,state = layered_model(inp_layer,initial_state)\n",
        "      \n",
        "      last_layer_model = model_layer_out\n",
        "\n",
        "    model_states = [state]\n",
        "  elif rnn_type=='RNN':\n",
        "    #adding everything except the last RNN layer, because in last layer return state=True\n",
        "    for i in range(layers):      \n",
        "      layered_model = keras.layers.SimpleRNN(model_out_dim, return_sequences=True,return_state=True,dropout=dropout,name=prefix+\"cell_\"+str(i))\n",
        "      if i==0:\n",
        "        inp_layer = embed\n",
        "      else:\n",
        "        inp_layer = last_layer_model\n",
        "        \n",
        "      model_layer_out,state = layered_model(inp_layer,initial_state)\n",
        "      \n",
        "      last_layer_model = model_layer_out\n",
        "\n",
        "    model_states = [state]\n",
        "    \n",
        "  return model_states,last_layer_model,model_inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Br2JpV-7xET_"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Build the model\n",
        "def build_model(rnn_type,embedding_dim,encoder_layers,decoder_layers,dropout,latent_dim):\n",
        "  \n",
        "  encoder_states,encoder_outputs,encoder_inputs = build_layered_RNN_model(rnn_type=rnn_type,embedding_in_dim = num_encoder_tokens,embedding_out_dim = embedding_dim,layers = encoder_layers,dropout = dropout,inp_length = max_encoder_seq_length,model_out_dim = latent_dim,prefix=encoder_model_prefix)\n",
        "\n",
        "  _,decoder_outputs,decoder_inputs = build_layered_RNN_model(rnn_type=rnn_type,embedding_in_dim = num_decoder_tokens,embedding_out_dim = embedding_dim,layers = decoder_layers,dropout = dropout,inp_length = max_decoder_seq_length,model_out_dim = latent_dim,prefix=decoder_model_prefix,initial_state = encoder_states)\n",
        "  \n",
        "  decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\",name='final')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "  model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for constructing inference model"
      ],
      "metadata": {
        "id": "vTi5-tj7oye0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "iUuV-IQ8ltes"
      },
      "outputs": [],
      "source": [
        "def get_inference_encoder_model(model):\n",
        "  encoder_layers,_ = get_encoder_decoder_layers_from_model(model)\n",
        "  encoder_inputs = model.input[0]  \n",
        "  if isinstance(model.get_layer(encoder_model_prefix+\"cell_0\"), keras.layers.LSTM):\n",
        "    encoder_outputs, state_h_enc, state_c_enc = model.get_layer(encoder_model_prefix+\"cell_\"+str(encoder_layers - 1)).output  \n",
        "    encoder_states = [state_h_enc, state_c_enc]\n",
        "  elif (isinstance(model.get_layer(encoder_model_prefix+\"cell_0\"), keras.layers.GRU) or isinstance(model.get_layer(encoder_model_prefix+\"cell_0\"), keras.layers.RNN)):\n",
        "    encoder_outputs, state = model.get_layer(encoder_model_prefix+\"cell_\"+str(encoder_layers - 1)).output  \n",
        "    encoder_states = [state]\n",
        "\n",
        "  encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "  return encoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WhC8eYW4mPv-"
      },
      "outputs": [],
      "source": [
        "def get_inference_decoder_model(model):\n",
        "  latent_dim = get_latent_dim_from_model(model)\n",
        "  _,decoder_layers = get_encoder_decoder_layers_from_model(model)\n",
        "\n",
        "  # Decoder during inference takes just one character(i.e vector rep of a character). This is either from previous timestep or start of sequence(\"\\t\")\n",
        "  decoder_inputs =  keras.Input(shape=( 1))\n",
        "  # Contains input to each decoder layer\n",
        "  decoder_states_inputs=[]\n",
        "  # Contains state output from each decoder layer\n",
        "  decoder_states=[]\n",
        "  previous_decoder_output = None\n",
        "\n",
        "  emdedded_rep_of_decoder_input = model.get_layer(decoder_model_prefix+\"embed\")(decoder_inputs)\n",
        "  \n",
        "  if isinstance(model.get_layer(decoder_model_prefix+\"cell_0\"), keras.layers.LSTM):\n",
        "    for i in range(decoder_layers):\n",
        "      #every layer must have an input through which we can supply it's hidden state\n",
        "      decoder_state_input_h = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "      decoder_state_input_c = keras.Input(shape=(latent_dim,),name='inp4_'+str(i))\n",
        "      init_state = [decoder_state_input_h, decoder_state_input_c]\n",
        "      decoder_lstm = model.get_layer(decoder_model_prefix+\"cell_\"+str(i))\n",
        "      if i==0:\n",
        "        decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(emdedded_rep_of_decoder_input, initial_state=init_state)\n",
        "      else:\n",
        "        decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(previous_decoder_output, initial_state=init_state )\n",
        "      \n",
        "      previous_decoder_output = decoder_outputs\n",
        "      decoder_states_inputs.append (decoder_state_input_h)\n",
        "      decoder_states_inputs.append (decoder_state_input_c)\n",
        "      decoder_states.append (state_h_dec)\n",
        "      decoder_states.append (state_c_dec)\n",
        "  elif isinstance(model.get_layer(decoder_model_prefix+\"cell_0\"), keras.layers.GRU):\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_state_input = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "      init_state = [decoder_state_input]\n",
        "      decoder_gru = model.get_layer(decoder_model_prefix+\"cell_\"+str(i))\n",
        "      if i==0:\n",
        "        decoder_outputs, state = decoder_gru(emdedded_rep_of_decoder_input, initial_state=init_state)\n",
        "      else:\n",
        "        decoder_outputs, state = decoder_gru(previous_decoder_output, initial_state=init_state )\n",
        "      \n",
        "      previous_decoder_output = decoder_outputs\n",
        "      decoder_states_inputs.append (decoder_state_input)\n",
        "      decoder_states.append (state)\n",
        "  elif isinstance(model.get_layer(decoder_model_prefix+\"cell_0\"), keras.layers.RNN):\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_state_input = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "      init_state = [decoder_state_input]\n",
        "      decoder_rnn = model.get_layer(decoder_model_prefix+\"cell_\"+str(i))\n",
        "      if i==0:\n",
        "        decoder_outputs, state = decoder_rnn(emdedded_rep_of_decoder_input, initial_state=init_state)\n",
        "      else:\n",
        "        decoder_outputs, state = decoder_rnn(previous_decoder_output, initial_state=init_state )\n",
        "      \n",
        "      previous_decoder_output = decoder_outputs\n",
        "      decoder_states_inputs.append (decoder_state_input)\n",
        "      decoder_states.append (state)      \n",
        "  decoder_dense = model.get_layer('final')\n",
        "  decoder_outputs = decoder_dense(previous_decoder_output)\n",
        "  decoder_model = keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "  return decoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NDJJuJM8koKB"
      },
      "outputs": [],
      "source": [
        "def build_inference_model(model):\n",
        "    encoder_model = get_inference_encoder_model(model)\n",
        "    \n",
        "    decoder_model = get_inference_decoder_model(model)\n",
        "\n",
        "    return encoder_model,decoder_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for running samples on inference model(with and without beam search)"
      ],
      "metadata": {
        "id": "FpLlZooEo8U_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XshCgw_96g5y"
      },
      "outputs": [],
      "source": [
        "class BeamRecordKeeping:\n",
        "  def __init__(self,decoder_input_state_values,prev_char_index,joint_probability,accumulated_previous_chars):\n",
        "    self.decoder_input_state_values = decoder_input_state_values.copy()\n",
        "    self.prev_char_index = np.copy(prev_char_index)\n",
        "    self.joint_probability = joint_probability\n",
        "    self.accumulated_previous_chars = accumulated_previous_chars\n",
        "\n",
        "  def __str__(self):\n",
        "    return \"decoder_input_state_values: \"+str(self.decoder_input_state_values) + \"\\n prev_char_index: \"+str(self.prev_char_index)+\"\\n joint_probability: \"+str(self.joint_probability)+\" accumulated_previous_chars: \"+str(self.accumulated_previous_chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "510BPMuu6eS7"
      },
      "outputs": [],
      "source": [
        "def get_sampled_char(sampled_token_index):\n",
        "  if sampled_token_index == 0:\n",
        "    sampled_char='\\n'\n",
        "  else:\n",
        "    sampled_char = index_to_char_target[sampled_token_index]\n",
        "  return sampled_char\n",
        "\n",
        "def get_predicted_word_from_beam(list_of_beam_objs):\n",
        "  current_highest_score = list_of_beam_objs[0].joint_probability\n",
        "  predicted_word = list_of_beam_objs[0].accumulated_previous_chars\n",
        "  for each_obj in list_of_beam_objs:\n",
        "    if(each_obj.joint_probability > current_highest_score):\n",
        "      predicted_word = each_obj.accumulated_previous_chars\n",
        "  return predicted_word"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This function is for cases where beam_width=1"
      ],
      "metadata": {
        "id": "hOla8vEXpW2e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Qojx7JC35S5y"
      },
      "outputs": [],
      "source": [
        "def decode_batch_of_sequences(rnn_type,input_seq,encoder_model,decoder_model,batch_size,encoder_layers,decoder_layers):\n",
        "    # Get encoder output\n",
        "    encoder_output_state_values = encoder_model.predict(input_seq)\n",
        "    if rnn_type=='GRU' or 'RNN':\n",
        "      decoder_input_state_values=[encoder_output_state_values]\n",
        "    \n",
        "    # This is needed because encoder state is fed to all decoder layers\n",
        "    decoder_input_state_values = decoder_input_state_values * decoder_layers\n",
        "    \n",
        "    # This is contain previously predicted character's index for every words in batch.\n",
        "    prev_char_index = np.zeros((batch_size, 1))\n",
        "    # We start with \\t for every word in batch\n",
        "    prev_char_index[:, 0] = target_tokenizer.word_index['\\t']\n",
        "    \n",
        "    predicted_words = [ \"\" for i in range(batch_size)]\n",
        "    done=[False for i in range(batch_size)]\n",
        "    for i in range(max_decoder_seq_length):\n",
        "        decoder_out = decoder_model.predict(tuple([prev_char_index] + decoder_input_state_values))\n",
        "        # Decoder output has both output of all timesteps followed by hidden states\n",
        "        output_probability = decoder_out[0]\n",
        "        # Decoder state input is previous layer state output\n",
        "        decoder_input_state_values = decoder_out[1:]\n",
        "        for j in range(batch_size):\n",
        "          if done[j]:\n",
        "            continue          \n",
        "          sampled_token_index = np.argmax(output_probability[j, -1, :])\n",
        "          if sampled_token_index == 0:\n",
        "            sampled_char='\\n'\n",
        "          else:\n",
        "            sampled_char = index_to_char_target[sampled_token_index]\n",
        "          if sampled_char == '\\n':\n",
        "            done[j]=True\n",
        "            continue            \n",
        "          predicted_words[j] += sampled_char\n",
        "          #update the previously predicted characters        \n",
        "          prev_char_index[j,0]=target_tokenizer.word_index[sampled_char]\n",
        "    return predicted_words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This function is called when beam_width>1 during running inference model"
      ],
      "metadata": {
        "id": "uNkF8x_npNY4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Mwfd_Mx3WQmP"
      },
      "outputs": [],
      "source": [
        "def decode_batch_of_sequences_for_bigger_beam_width(rnn_type,input_seq,encoder_model,decoder_model,batch_size,encoder_layers,decoder_layers,beam_search_width):\n",
        "    print(\"batch_size\"+str(batch_size))\n",
        "    print(\"input_seq:\"+str(input_seq.shape))\n",
        "    next_list_of_beam_record_objects = []\n",
        "    predicted_words = [ \"\" for i in range(batch_size)]\n",
        "    list_of_beam_record_objects = []\n",
        "    for j in range(batch_size):\n",
        "      next_list_of_beam_record_objects = []\n",
        "      list_of_beam_record_objects = []\n",
        "      if(j % 100 == 0):\n",
        "        print(\"**********Batch number*************:\"+str(j))\n",
        "      current_seq = input_seq[j]\n",
        "      current_seq = tf.expand_dims(current_seq, 0)\n",
        "      # Get encoder output\n",
        "      decoder_input_state_values = encoder_model.predict(current_seq)\n",
        "      if rnn_type=='GRU' or 'RNN':\n",
        "        decoder_input_state_values=[decoder_input_state_values]\n",
        "        decoder_input_state_values = decoder_input_state_values * decoder_layers\n",
        "      else:\n",
        "        decoder_input_state_values = decoder_input_state_values[0] * decoder_layers\n",
        "      \n",
        "      prev_char_index = np.zeros((1, 1))\n",
        "      # We start with \\t for every word in batch\n",
        "      prev_char_index[:, 0] = target_tokenizer.word_index['\\t']\n",
        "      done  = False\n",
        "      for _ in range(beam_search_width):\n",
        "        current_beam_search_obj = BeamRecordKeeping(decoder_input_state_values,prev_char_index,0,\"\")\n",
        "        list_of_beam_record_objects.append(current_beam_search_obj)\n",
        "\n",
        "      for i in range(max_decoder_seq_length):\n",
        "        if(done):\n",
        "          break\n",
        "\n",
        "        if(i != 0):\n",
        "          list_of_beam_record_objects = next_list_of_beam_record_objects\n",
        "        next_list_of_beam_record_objects = []\n",
        "        for beam_index in range(beam_search_width):\n",
        "          # print(\"prev_char_index\"+str(list_of_beam_record_objects[beam_index].prev_char_index.shape))\n",
        "          # print(\"decoder_input_state_values\"+str(len(list_of_beam_record_objects[beam_index].decoder_input_state_values)))\n",
        "          # print(\"decoder_input_state_values\"+str(list_of_beam_record_objects[beam_index].decoder_input_state_values[0].shape))\n",
        "\n",
        "          decoder_out = decoder_model.predict(tuple([list_of_beam_record_objects[beam_index].prev_char_index] + list_of_beam_record_objects[beam_index].decoder_input_state_values))\n",
        "          # Decoder output has both output of all timesteps followed by hidden states\n",
        "          output_probability = decoder_out[0]\n",
        "          # Decoder state input is previous layer state output\n",
        "          decoder_input_state_values = decoder_out[1:]\n",
        "          sampled_token_index = np.argsort(output_probability[0][-1, :])[-beam_search_width:]\n",
        "          sampled_probability_values = output_probability[0][-1, :][sampled_token_index]\n",
        "\n",
        "          for each_candidate in range(1,len(sampled_probability_values)+1):\n",
        "            new_joint_probability = list_of_beam_record_objects[beam_index].joint_probability + math.log(sampled_probability_values[-each_candidate])\n",
        "            if(len(next_list_of_beam_record_objects) < beam_search_width):\n",
        "              sampled_char = get_sampled_char(sampled_token_index[-each_candidate])\n",
        "              if sampled_char == '\\n':\n",
        "                done = True\n",
        "                break\n",
        "              accumulated_previous_chars = list_of_beam_record_objects[beam_index].accumulated_previous_chars + sampled_char\n",
        "              prev_char_index[:, 0]=target_tokenizer.word_index[sampled_char]\n",
        "              next_beam_record_keeping_obj = BeamRecordKeeping(decoder_input_state_values,prev_char_index,new_joint_probability,accumulated_previous_chars)\n",
        "              next_list_of_beam_record_objects.append(next_beam_record_keeping_obj)\n",
        "            else:\n",
        "              replace_indx = -1\n",
        "              for (current_indx,each_obj) in enumerate(next_list_of_beam_record_objects):\n",
        "                if(each_obj.joint_probability < new_joint_probability):\n",
        "                  replace_indx = current_indx\n",
        "                  break\n",
        "              if(replace_indx != -1):\n",
        "                sampled_char = get_sampled_char(sampled_token_index[-each_candidate])\n",
        "                if sampled_char == '\\n':\n",
        "                  done = True\n",
        "                  break\n",
        "                accumulated_previous_chars = list_of_beam_record_objects[beam_index].accumulated_previous_chars + sampled_char\n",
        "                prev_char_index[:, 0]=target_tokenizer.word_index[sampled_char]\n",
        "                next_beam_record_keeping_obj = BeamRecordKeeping(decoder_input_state_values,prev_char_index,new_joint_probability,accumulated_previous_chars)\n",
        "                next_list_of_beam_record_objects[replace_indx] = next_beam_record_keeping_obj\n",
        "          \n",
        "          if(done or i == max_decoder_seq_length-1):\n",
        "            if( len(next_list_of_beam_record_objects) == 0):\n",
        "              predicted_words[j] = get_predicted_word_from_beam(list_of_beam_record_objects)\n",
        "            else:\n",
        "              predicted_words[j] = get_predicted_word_from_beam(next_list_of_beam_record_objects)\n",
        "            break\n",
        "        \n",
        "    return predicted_words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for obtaining accuracies on inference model"
      ],
      "metadata": {
        "id": "36MKoOcnpbs-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "kbTznqNEx4sR"
      },
      "outputs": [],
      "source": [
        "def test_accuracy(model,encoder_model,decoder_model,beam_search_width=1):\n",
        "  rnn_type = get_rnntype_from_model(model)\n",
        "  encoder_layers,decoder_layers = get_encoder_decoder_layers_from_model(model)\n",
        "  \n",
        "  success=0\n",
        "  success_char = 0\n",
        "  total_chars = 0\n",
        "  #Get all the predicted words\n",
        "  if(beam_search_width == 0 or beam_search_width == 1):\n",
        "    pred=decode_batch_of_sequences(rnn_type,test_input_tensor,encoder_model,decoder_model,test_input_tensor.shape[0],encoder_layers,decoder_layers)\n",
        "  else:\n",
        "    pred=decode_batch_of_sequences_for_bigger_beam_width(rnn_type,test_input_tensor,encoder_model,decoder_model,test_input_tensor.shape[0],encoder_layers,decoder_layers,beam_search_width)\n",
        "  for seq_index in range(test_input_tensor.shape[0]):\n",
        "      predicted_word = pred[seq_index]\n",
        "      target_word=test_target_texts[seq_index][1:-1]\n",
        "      for (indx,each_ele) in enumerate(target_word):\n",
        "        total_chars += 1\n",
        "        if(indx < len(predicted_word)):\n",
        "          if(target_word[indx] == predicted_word[indx]):\n",
        "            success_char += 1\n",
        "\n",
        "      #test the word one by one and write to files\n",
        "      if target_word == predicted_word:\n",
        "        success+=1\n",
        "        f = open(\"success.txt\", \"a\")\n",
        "        f.write(test_input_texts[seq_index]+' '+target_word+' '+predicted_word+'\\n')\n",
        "        f.close()\n",
        "      else:\n",
        "        f = open(\"failure.txt\", \"a\")\n",
        "        f.write(test_input_texts[seq_index]+' '+target_word+' '+predicted_word+'\\n')\n",
        "        f.close()\n",
        "  \n",
        "  print(\"success:\"+str(success))\n",
        "  print(\"success_char:\"+str(success_char))\n",
        "  return float(success)/float(test_input_tensor.shape[0]),float(success_char)/float(total_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "jcWesBWKm2hn"
      },
      "outputs": [],
      "source": [
        "def batch_validate(model,encoder_model,decoder_model,beam_search_width=1):\n",
        "  rnn_type = get_rnntype_from_model(model)\n",
        "  encoder_layers,decoder_layers = get_encoder_decoder_layers_from_model(model)\n",
        "\n",
        "  success = 0\n",
        "  success_char = 0\n",
        "  total_chars = 0\n",
        "  #get all the predicted words\n",
        "  if(beam_search_width == 0 or beam_search_width == 1):\n",
        "    pred=decode_batch_of_sequences(rnn_type,val_input_tensor,encoder_model,decoder_model,val_input_tensor.shape[0],encoder_layers,decoder_layers)\n",
        "  else:\n",
        "    pred=decode_batch_of_sequences_for_bigger_beam_width(rnn_type,val_input_tensor,encoder_model,decoder_model,val_input_tensor.shape[0],encoder_layers,decoder_layers,beam_search_width)\n",
        "\n",
        "  for seq_index in range(val_input_tensor.shape[0]):\n",
        "    predicted_word = pred[seq_index]\n",
        "    target_word = val_target_texts[seq_index][1:-1]\n",
        "    #test the words one by one\n",
        "    if predicted_word == target_word:\n",
        "      # print(\"pred:\"+str(pred[seq_index]))\n",
        "      # print(\"Target: \"+str(val_target_texts[seq_index][1:-1]))\n",
        "      success+=1\n",
        "      \n",
        "    for (indx,each_ele) in enumerate(target_word):\n",
        "      total_chars += 1\n",
        "      if(indx < len(predicted_word)):\n",
        "        if(target_word[indx] == predicted_word[indx]):\n",
        "          # print(\"pred:\"+str(pred[seq_index]))\n",
        "          # print(\"Target: \"+str(target_word))\n",
        "          success_char += 1\n",
        "  \n",
        "  print(\"success:\"+str(success))\n",
        "  print(\"success_char:\"+str(success_char))\n",
        "  # print(\"val_input_tensor.shape[0]:\"+str(val_input_tensor.shape[0]))\n",
        "  return float(success)/float(val_input_tensor.shape[0]),float(success_char)/float(total_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code to train model and return inference and root model "
      ],
      "metadata": {
        "id": "6zIFQx1pptUS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "P_psbbJxVblX"
      },
      "outputs": [],
      "source": [
        "def run_custom_model(encoder_layers,decoder_layers,epochs,lr,latent_dim,rnn_type,embedding_dim,dropout,bs,optimizer,model_save_path,encoder_save_path,decoder_save_path,save=False,use_wandb=False):\n",
        "\n",
        "      # Create a MirroredStrategy.\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        strategy = tf.distribute.MirroredStrategy()\n",
        "    else:  # use default strategy\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
        "        # Open a strategy scope and create the model\n",
        "    with strategy.scope():\n",
        "      model = build_model(rnn_type,embedding_dim,encoder_layers,decoder_layers,dropout,latent_dim)\n",
        "\n",
        "    plot_model(model, to_file=str(model_save_path)+'.png', show_shapes=True, show_dtype=True,show_layer_names=True)\n",
        "\n",
        "    model.summary()\n",
        "    \n",
        "    optimizerObj = get_optimizer(optimizer,lr)\n",
        "    \n",
        "    model.compile(optimizer=optimizerObj, loss=keras.losses.SparseCategoricalCrossentropy(reduction='none'), metrics=[\"accuracy\"])\n",
        "    if(use_wandb == False):\n",
        "      hist=model.fit([train_input_tensor, train_target_tensor],tf.concat([train_target_tensor[:,1:],tf.zeros((train_target_tensor[:,:].shape[0],1))], axis=1),batch_size=bs,epochs=epochs,shuffle=True)\n",
        "    else:\n",
        "      hist=model.fit([train_input_tensor, train_target_tensor],tf.concat([train_target_tensor[:,1:],tf.zeros((train_target_tensor[:,:].shape[0],1))], axis=1),batch_size=bs,epochs=epochs,shuffle=True,callbacks=[WandbCallback(), history])\n",
        "\n",
        "    encoder_inference_model,decoder_inference_model=build_inference_model(model)\n",
        "    plot_model(encoder_inference_model, to_file=str(encoder_save_path)+'.png', show_shapes=True)\n",
        "    plot_model(decoder_inference_model, to_file=str(decoder_save_path)+'.png', show_shapes=True)\n",
        "\n",
        "    if(save == True):\n",
        "      from google.colab import drive\n",
        "      drive.mount('/content/drive')\n",
        "      model.save('drive/MyDrive/Colab Notebooks/'+str(model_save_path)+'.h5')\n",
        "      encoder_inference_model.save('drive/MyDrive/Colab Notebooks/'+str(encoder_save_path)+\".h5\")\n",
        "      decoder_inference_model.save('drive/MyDrive/Colab Notebooks/'+str(decoder_save_path)+\".h5\")\n",
        "      plot_model(encoder_inference_model, to_file='drive/MyDrive/Colab Notebooks/'+str(encoder_save_path)+\".png\", show_shapes=True)\n",
        "      plot_model(decoder_inference_model, to_file='drive/MyDrive/Colab Notebooks/'+str(decoder_save_path)+'.png', show_shapes=True)\n",
        "      plot_model(model, to_file='drive/MyDrive/Colab Notebooks/'+str(model_save_path)+'.png', show_shapes=True, show_dtype=True,show_layer_names=True)\n",
        "\n",
        "\n",
        "    model.save(str(model_save_path)+'.h5')\n",
        "    encoder_inference_model.save(str(encoder_save_path)+'.h5')\n",
        "    decoder_inference_model.save(str(decoder_save_path)+'.h5')\n",
        "\n",
        "    return model,encoder_inference_model,decoder_inference_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code used to obtain accuracy either by loading models from path(in GDrive or local) or by passing model objects"
      ],
      "metadata": {
        "id": "XGAcBNw6qGNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_run_accuracy(is_test=False,model_path=None,encoder_inf_path=None,decoder_inf_path=None,from_gdrive=False,beam_width = 1,model=None,encoder_inference_model=None,decoder_inference_model=None):\n",
        "  if(from_gdrive == True):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "  \n",
        "  if(model is None):\n",
        "    print(\"Load model from path\")\n",
        "    model = keras.models.load_model(model_path)\n",
        "  \n",
        "  if(encoder_inf_path is None or decoder_inf_path is None):\n",
        "    if(decoder_inference_model is None or encoder_inference_model is None):\n",
        "      encoder_inference_model,decoder_inference_model=build_inference_model(model)\n",
        "  else:\n",
        "    encoder_inference_model= keras.models.load_model(encoder_inf_path)\n",
        "    decoder_inference_model= keras.models.load_model(decoder_inf_path)\n",
        "  \n",
        "  if(is_test == True):\n",
        "    word_val_acc,char_val_acc=test_accuracy(model,encoder_inference_model,decoder_inference_model,beam_width)\n",
        "  else:\n",
        "    word_val_acc,char_val_acc=batch_validate(model,encoder_inference_model,decoder_inference_model,beam_width)\n",
        "\n",
        "  return word_val_acc,char_val_acc"
      ],
      "metadata": {
        "id": "MYpd1tvBn59I"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code to run the best model observed and return inference and root models\n",
        "Later, load_model_run_accuracy needs to be called to obtain accuracies"
      ],
      "metadata": {
        "id": "0OAWGT3BqPyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_best_model(save=False):\n",
        "  encoder_layers = 3\n",
        "  decoder_layers = 3\n",
        "  epochs = 20\n",
        "  lr = 0.0001\n",
        "  latent_dim = 1024\n",
        "  rnn_type = 'GRU'\n",
        "  embedding_dim = 512\n",
        "  dropout = 0.4\n",
        "  bs = 64\n",
        "  optimizer = \"Adam\"\n",
        "  model,encoder_inference_model,decoder_inference_model = run_custom_model(encoder_layers,decoder_layers,epochs,lr,latent_dim,rnn_type,embedding_dim,dropout,bs,optimizer,\"best_model_assignment_3\",\"best_encoder_inference_model_assignment_3\",\"best_decoder_inference_model_assignment_3\",save)\n",
        "  return model,encoder_inference_model,decoder_inference_model"
      ],
      "metadata": {
        "id": "EpOpF5Rv1FFG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code related to hyper-parameter tuning"
      ],
      "metadata": {
        "id": "HaatzB6vqIaA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "VX5pjUqzDVr-"
      },
      "outputs": [],
      "source": [
        "default_config = {\n",
        "        \"rnn_type\": \"LSTM\",\n",
        "        \"dropout\": 0.5,\n",
        "        \"encoder_layers\":3,\n",
        "        \"decoder_layers\":3,\n",
        "        \"latent_dim\": 64,\n",
        "        \"epochs\": 1,\n",
        "        \"lr\": 0.0001,\n",
        "        \"embedding_out_dim\": 64,\n",
        "        \"beam_search\":1,\n",
        "        \"batch_size\":64,\n",
        "        \"optimizer\": \"Adam\"\n",
        "    }\n",
        "\n",
        "#Keras callback    \n",
        "history = History()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "PvOMovPZyGqj"
      },
      "outputs": [],
      "source": [
        "def HP_tuning_run():\n",
        "    # Create a MirroredStrategy.\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        strategy = tf.distribute.MirroredStrategy()\n",
        "    else:  # use default strategy\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
        "\n",
        "    # wandb.init(config=default_config, magic=True,project=\"DeepLearningAssignment-3\", entity='cs21s002-ee21s113-dlassignment-1')\n",
        "    wandb.init(config=default_config, magic=True,project=\"DL-Assignment3\", entity='cs21s002-ee21s113-dlassignment-1')\n",
        "    # wandb.init(config=default_config, magic=True,project=\"DeepLearningAssignment-3\", entity='akshaygrao')\n",
        "    config = wandb.config\n",
        "    print(\"Config: \"+str(config))\n",
        "    run_name = str(config).replace(\"{\", \"\").replace(\"}\",\"\").replace(\":\",\"-\")\n",
        "    wandb.run.name = run_name\n",
        "    \n",
        "    model,encoder_inference_model,decoder_inference_model = run_custom_model(encoder_layers=config.encoder_layers,decoder_layers=config.decoder_layers,epochs=config.epochs,lr=config.lr,latent_dim=config.latent_dim,rnn_type=config.rnn_type,embedding_dim=config.embedding_out_dim,dropout=config.dropout,bs=config.batch_size,optimizer=config.optimizer,model_save_path=f'{run_name.replace(\",\",\"-\")}_model',encoder_save_path=f'{run_name.replace(\",\",\"-\")}_encoder',decoder_save_path=f'{run_name.replace(\",\",\"-\")}_decoder',save=False,use_wandb=True)\n",
        "\n",
        "    word_val_acc,char_val_acc = load_model_run_accuracy(model=model,encoder_inference_model=encoder_inference_model,decoder_inference_model=decoder_inference_model,beam_width=config.beam_search)\n",
        "    print(\"word_val_acc\"+str(word_val_acc))\n",
        "    print(\"char_val_acc\"+str(char_val_acc))\n",
        "    wandb.log({\"word_val_acc\":round(word_val_acc,5)})\n",
        "    wandb.log({\"char_val_acc\":round(char_val_acc,5)})\n",
        "    wandb.log({\"language\":transliteration_target_language})\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "w5KOeeV13mvj"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    \"name\": \"Assignment-3-final-batch-optimizer\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\":{\n",
        "      \"goal\": \"maximize\",\n",
        "      \"name\": \"word_val_acc\"\n",
        "    },\n",
        "    \"project\": 'DL-Assignment3',\n",
        "    \"parameters\": {\n",
        "        \"rnn_type\": {\n",
        "            \"values\": [\"LSTM\",\"GRU\",\"RNN\"]\n",
        "        },\n",
        "        \"dropout\": {\n",
        "            \"values\": [0.2,0.4]\n",
        "        },\n",
        "        \"encoder_layers\": {\n",
        "            \"values\": [3]\n",
        "        },\n",
        "        \"decoder_layers\": {\n",
        "            \"values\": [3]\n",
        "        },\n",
        "        \"latent_dim\": {\n",
        "            \"values\": [512,1024,2048]\n",
        "        },\n",
        "        \"epochs\": {\n",
        "            \"values\": [20]\n",
        "        },\n",
        "        \"lr\": {\n",
        "            \"values\": [0.0001]\n",
        "        },\n",
        "        \"embedding_out_dim\": {\n",
        "            \"values\":[128,256]\n",
        "        },\n",
        "        \"beam_search\":{\n",
        "            \"values\":[1]\n",
        "        },\n",
        "        \"batch_size\":{\n",
        "            \"values\":[64,128,256]\n",
        "        },\n",
        "        \"optimizer\":{\n",
        "            \"values\":[\"Nadam\",\"SGD\",\"RMSprop\"]\n",
        "        }\n",
        "        \n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fag3IotW5Qu2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34,
          "referenced_widgets": [
            "3e4a6c8bdb65476ea8769eb2f769bf9b",
            "6b628f825dac4dbfa7a43b3685657468",
            "fcc9e0adc3124bacb35d79a4b7da7b33",
            "fe78bde25b8b47f5a7bf934cb0f5e5f6",
            "02f12aa98f3c406bb00d7a96977d9228",
            "251dca0c577247dcbf63cd196ba9cf33",
            "f4021cf367d14bd5bee610c851ad12e2",
            "9c24fb54642645629cc66732458d1591"
          ]
        },
        "outputId": "77f4382c-b9a6-4b2b-958e-92cbeed3f9e1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e4a6c8bdb65476ea8769eb2f769bf9b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# sweep_id = wandb.sweep(sweep_config,  project='DeepLearningAssignment-3', entity='akshaygrao')\n",
        "# sweep_id = wandb.sweep(sweep_config,  project='DeepLearningAssignment-3', entity='cs21s002-ee21s113-dlassignment-1')\n",
        "# sweep_id = wandb.sweep(sweep_config,  project='DL-Assignment3', entity='cs21s002-ee21s113-dlassignment-1')\n",
        "sweep_id=\"1fwq5qge\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itgfsYWC5oCn"
      },
      "outputs": [],
      "source": [
        "# wandb.agent(sweep_id, function=HP_tuning_run, project='DeepLearningAssignment-3', entity='akshaygrao')\n",
        "# wandb.agent(sweep_id, function=HP_tuning_run, project='DeepLearningAssignment-3', entity='cs21s002-ee21s113-dlassignment-1')\n",
        "wandb.agent(sweep_id, function=HP_tuning_run, project='DL-Assignment3', entity='cs21s002-ee21s113-dlassignment-1')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run this code in colab to get test accuracy for best model"
      ],
      "metadata": {
        "id": "LgC5dYaPqdgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model,encoder_inference_model,decoder_inference_model = run_best_model()\n",
        "word_val_acc,char_val_acc = load_model_run_accuracy(model=model,encoder_inference_model=encoder_inference_model,decoder_inference_model=decoder_inference_model,is_test=True)\n",
        "print(\"Test word_val_acc\"+str(word_val_acc))\n",
        "print(\"Test char_val_acc\"+str(char_val_acc))"
      ],
      "metadata": {
        "id": "EyVtENGJ76B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Command line argument handlers"
      ],
      "metadata": {
        "id": "mpsSeuYIqkCQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "-hBu8EDUmm-f"
      },
      "outputs": [],
      "source": [
        "def init_argparse() -> argparse.ArgumentParser:\n",
        "    parser = argparse.ArgumentParser()\n",
        "    \n",
        "    subparsers = parser.add_subparsers(dest='command')\n",
        "    run_model_parser = subparsers.add_parser('run_model')\n",
        "    run_model_parser.add_argument(\"--encoder_layers\", action=\"store\",dest='encoder_layers', help=\"Specify number of layers in encoder(default:3)\",default=3,required=False)\n",
        "    run_model_parser.add_argument(\"--decoder_layers\", action=\"store\",dest='decoder_layers', help=\"Specify number of layers in decoder(default:3)\",default=3,required=False)\n",
        "    run_model_parser.add_argument(\"--epochs\", action=\"store\",dest='epochs', help=\"Specify number of epochs(Default:15)\",default=15,required=False)\n",
        "    run_model_parser.add_argument(\"--lr\", action=\"store\",dest='lr', help=\"Specify learning rate(Default:0.0001)\",default=0.0001,required=False)\n",
        "    run_model_parser.add_argument(\"--latent_dim\", action=\"store\",dest='latent_dim', help=\"Specify latent dimensions(Default:1024)\",default=1024,required=False)\n",
        "    run_model_parser.add_argument(\"--rnn_type\", action=\"store\",dest='rnn_type', help=\"Specify cell type of RNN('RNN','GRU','LSTM'))(Default:GRU)\",default='GRU',required=False)\n",
        "    run_model_parser.add_argument(\"--embedding_dim\", action=\"store\",dest='embedding_dim', help=\"Specify dimension of embedding layer output(Default:256)\",default=256,required=False)\n",
        "    run_model_parser.add_argument(\"--dropout\", action=\"store\",dest='dropout', help=\"Specify dropout in input (applies at all layers of encoder and decoder)(Default:0.4)\",default=0.4,required=False)\n",
        "    run_model_parser.add_argument(\"--bs\", action=\"store\",dest='bs', help=\"Specify batch size(Default:64)\",default=64,required=False)\n",
        "    run_model_parser.add_argument(\"--optimizer\", action=\"store\",dest='optimizer', help=\"Specify optimizer algorithm('Adam','Nadam','SGD','RMSprop')(Default:'Adam')\",default='Adam',required=False)\n",
        "    run_model_parser.add_argument(\"--model_save_path\", action=\"store\",dest='model_save_path', help=\"Specify path to save model into(Default:'model')\",default='model',required=False)\n",
        "    run_model_parser.add_argument(\"--encoder_save_path\", action=\"store\",dest='encoder_save_path', help=\"Specify path to save encoder model into(Default:'encoder_inference_model')\",default='encoder_inference_model',required=False)\n",
        "    run_model_parser.add_argument(\"--decoder_save_path\", action=\"store\",dest='decoder_save_path', help=\"Specify path to save decoder model into(Default:'decoder_inference_model')\",default='decoder_inference_model',required=False)\n",
        "    run_model_parser.add_argument(\"--save\", action=\"store_true\",dest='save', help=\"Save model to google drive(default false)\",required=False)\n",
        "\n",
        "    run_best_model_parser = subparsers.add_parser('run_best_model')\n",
        "    run_best_model_parser.add_argument(\"--save\", action=\"store_true\",dest='save', help=\"Save model to google drive(default false)\",required=False)\n",
        "\n",
        "    last_accuracy_parser = subparsers.add_parser('last_model_run_accuracy')\n",
        "    last_accuracy_parser.add_argument(\"--is_test\", action=\"store_true\",dest='is_test', help=\"Specify if you need test accuracy(or validation accuracy). If passed assumes True otherwise returns validation accuracy\",required=False)\n",
        "    last_accuracy_parser.add_argument(\"--beam_width\", action=\"store\",dest='beam_width', help=\"Specify beam width(Default:1)\",default=1,required=False)\n",
        "\n",
        "    accuracy_parser = subparsers.add_parser('load_model_run_accuracy')\n",
        "    accuracy_parser.add_argument(\"--model_path\", action=\"store\",dest='model_path', help=\"Specify model path\",required=True)\n",
        "    accuracy_parser.add_argument(\"--encoder_inf_path\", action=\"store\",dest='encoder_inf_path', help=\"Specify encoder model path\",required=True)\n",
        "    accuracy_parser.add_argument(\"--decoder_inf_path\", action=\"store\",dest='decoder_inf_path', help=\"Specify decoder model path\",required=True)\n",
        "    accuracy_parser.add_argument(\"--from_gdrive\", action=\"store_true\",dest='from_gdrive', help=\"Retrieve path from google drive(If passed assumes True)\",required=False)\n",
        "    accuracy_parser.add_argument(\"--beam_width\", action=\"store\",dest='beam_width', help=\"Specify beam width(Default:1)\",default=1,required=False)\n",
        "    accuracy_parser.add_argument(\"--is_test\", action=\"store_true\",dest='is_test', help=\"Specify if you need test accuracy(or validation accuracy). If passed assumes True otherwise returns validation accuracy\",required=False)\n",
        "\n",
        "    return parser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  parser = init_argparse()\n",
        "  args = parser.parse_args(['--help'])\n",
        "\n",
        "  model = None\n",
        "  encoder_inference_model = None\n",
        "  decoder_inference_model = None\n",
        "  \n",
        "  if(args.command == 'run_model'):\n",
        "    encoder_layers = args.encoder_layers \n",
        "    decoder_layers = args.decoder_layers \n",
        "    epochs = args.epochs \n",
        "    lr = args.lr \n",
        "    latent_dim = args.latent_dim \n",
        "    rnn_type = args.rnn_type \n",
        "    embedding_dim = args.embedding_dim \n",
        "    dropout = args.dropout \n",
        "    bs = args.bs \n",
        "    optimizer = args.optimizer \n",
        "    model_save_path = args.model_save_path \n",
        "    encoder_save_path = args.encoder_save_path \n",
        "    decoder_save_path = args.decoder_save_path \n",
        "    save = args.save\n",
        "\n",
        "    model,encoder_inference_model,decoder_inference_model = run_custom_model(encoder_layers,decoder_layers,epochs,lr,latent_dim,rnn_type,embedding_dim,dropout,bs,beam_width,optimizer,model_save_path,encoder_save_path,decoder_save_path,save=save)\n",
        "  \n",
        "  elif(args.command == 'run_best_model'):\n",
        "    print(\"Running best model----\")\n",
        "    model,encoder_inference_model,decoder_inference_model = run_best_model(save=args.save)\n",
        "  elif(args.command == 'last_model_run_accuracy'):\n",
        "    if(model is None or encoder_inference_model is None or decoder_inference_model is None):\n",
        "      print(\"Producing accuracy of last run model\")\n",
        "      is_test = args.is_test\n",
        "      beam_width = args.beam_width\n",
        "      if(is_test):\n",
        "        word_val_acc,char_val_acc = load_model_run_accuracy(model=model,encoder_inference_model=encoder_inference_model,decoder_inference_model=decoder_inference_model,beam_width=beam_width,is_test=is_test)\n",
        "        print(\"Test word_val_acc\"+str(word_val_acc))\n",
        "        print(\"Test char_val_acc\"+str(char_val_acc))\n",
        "      else:\n",
        "        word_val_acc,char_val_acc = load_model_run_accuracy(model=model,encoder_inference_model=encoder_inference_model,decoder_inference_model=decoder_inference_model,beam_width=beam_width,is_test=is_test)\n",
        "        print(\"Validation word_val_acc\"+str(word_val_acc))\n",
        "        print(\"Validation char_val_acc\"+str(char_val_acc))\n",
        "    else:\n",
        "      print(\"Invalid command!!! Run a model before running accuracy\")\n",
        "  elif(args.command == 'load_model_run_accuracy'):\n",
        "    print(\"Producing accuracy by running model loaded from path\")\n",
        "    is_test = args.is_test\n",
        "    beam_width = args.beam_width\n",
        "    model_path = args.model_path\n",
        "    encoder_inf_path = args.encoder_inf_path\n",
        "    decoder_inf_path = args.decoder_inf_path\n",
        "    from_gdrive = args.from_gdrive\n",
        "    \n",
        "    if(is_test):\n",
        "      word_val_acc,char_val_acc = load_model_run_accuracy(model_path=model_path,encoder_inf_path=encoder_inf_path,decoder_inf_path=decoder_inf_path,beam_width=beam_width,is_test=is_test,from_gdrive=from_gdrive)\n",
        "      print(\"Test word_val_acc\"+str(word_val_acc))\n",
        "      print(\"Test char_val_acc\"+str(char_val_acc))\n",
        "    else:\n",
        "      word_val_acc,char_val_acc = load_model_run_accuracy(model_path=model_path,encoder_inf_path=encoder_inf_path,decoder_inf_path=decoder_inf_path,beam_width=beam_width,is_test=is_test,from_gdrive=from_gdrive)\n",
        "      print(\"Validation word_val_acc\"+str(word_val_acc))\n",
        "      print(\"Validation char_val_acc\"+str(char_val_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "QkmYTo7hZ17s",
        "outputId": "d973a595-dc02-4083-b797-787f931478a1"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: ipykernel_launcher.py [-h]\n",
            "                             {run_model,run_best_model,last_model_run_accuracy,load_model_run_accuracy}\n",
            "                             ...\n",
            "\n",
            "positional arguments:\n",
            "  {run_model,run_best_model,last_model_run_accuracy,load_model_run_accuracy}\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xxycHaDOlpgp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DL_Assignment_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3e4a6c8bdb65476ea8769eb2f769bf9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b628f825dac4dbfa7a43b3685657468",
              "IPY_MODEL_fcc9e0adc3124bacb35d79a4b7da7b33"
            ],
            "layout": "IPY_MODEL_fe78bde25b8b47f5a7bf934cb0f5e5f6"
          }
        },
        "6b628f825dac4dbfa7a43b3685657468": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02f12aa98f3c406bb00d7a96977d9228",
            "placeholder": "",
            "style": "IPY_MODEL_251dca0c577247dcbf63cd196ba9cf33",
            "value": "0.019 MB of 0.019 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "fcc9e0adc3124bacb35d79a4b7da7b33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4021cf367d14bd5bee610c851ad12e2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c24fb54642645629cc66732458d1591",
            "value": 1
          }
        },
        "fe78bde25b8b47f5a7bf934cb0f5e5f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02f12aa98f3c406bb00d7a96977d9228": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "251dca0c577247dcbf63cd196ba9cf33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4021cf367d14bd5bee610c851ad12e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c24fb54642645629cc66732458d1591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}